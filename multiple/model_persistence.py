#!/usr/bin/env python3
"""
model_persistence.py
ML ëª¨ë¸ ì €ì¥/ë¡œë“œ ë° ì¦ë¶„ í•™ìŠµ ì‹œìŠ¤í…œ
- LSTM/Transformer: .h5 (Keras HDF5)
- XGBoost/LightGBM/RF: .pkl (Pickle)
- ë©”íƒ€ë°ì´í„°: .json
- ë²„ì „ ê´€ë¦¬ ë° ì„±ëŠ¥ ì¶”ì 
"""

import os
import json
import pickle
import joblib
from datetime import datetime
from pathlib import Path
import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, List
import hashlib

from logger_config import get_logger
logger = get_logger(__name__)


class ModelPersistence:
    """ëª¨ë¸ ì €ì¥/ë¡œë“œ ë° ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self, base_dir: str = "models"):
        """
        Args:
            base_dir: ëª¨ë¸ ì €ì¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬
        """
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)

    def _get_model_dir(self, ticker: str) -> Path:
        """í‹°ì»¤ë³„ ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±/ë°˜í™˜"""
        model_dir = self.base_dir / ticker
        model_dir.mkdir(exist_ok=True)
        return model_dir

    def _generate_version(self) -> str:
        """ë²„ì „ ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)"""
        return datetime.now().strftime("%Y%m%d_%H%M%S")

    def _get_model_path(self, ticker: str, model_type: str, version: Optional[str] = None) -> Path:
        """ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        model_dir = self._get_model_dir(ticker)

        if version is None:
            # ìµœì‹  ë²„ì „ ì°¾ê¸°
            pattern = f"{model_type}_*.pkl" if model_type in ['xgboost', 'lightgbm', 'random_forest'] else f"{model_type}_*.h5"
            existing = sorted(model_dir.glob(pattern))
            if existing:
                return existing[-1]
            else:
                version = self._generate_version()

        # íŒŒì¼ í™•ì¥ì ê²°ì •
        if model_type in ['lstm', 'transformer']:
            ext = 'h5'
        else:
            ext = 'pkl'

        return model_dir / f"{model_type}_{version}.{ext}"

    def _get_metadata_path(self, model_path: Path) -> Path:
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ"""
        return model_path.with_suffix('.json')

    # ========== Keras ëª¨ë¸ (LSTM/Transformer) ==========

    def save_keras_model(self, model, ticker: str, model_type: str,
                        metadata: Optional[Dict[str, Any]] = None,
                        scaler=None) -> str:
        """
        Keras ëª¨ë¸ ì €ì¥ (.h5)

        Args:
            model: Keras ëª¨ë¸
            ticker: ì£¼ì‹ í‹°ì»¤
            model_type: 'lstm' ë˜ëŠ” 'transformer'
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì„±ëŠ¥, íŒŒë¼ë¯¸í„° ë“±)
            scaler: MinMaxScaler ê°ì²´

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        version = self._generate_version()
        model_path = self._get_model_path(ticker, model_type, version)

        try:
            # ëª¨ë¸ ì €ì¥
            model.save(str(model_path))
            logger.info(f"âœ… {model_type.upper()} ëª¨ë¸ ì €ì¥: {model_path}")

            # Scaler ì €ì¥
            if scaler is not None:
                scaler_path = model_path.with_suffix('.scaler.pkl')
                joblib.dump(scaler, scaler_path)
                logger.info(f"âœ… Scaler ì €ì¥: {scaler_path}")

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            meta = {
                'ticker': ticker,
                'model_type': model_type,
                'version': version,
                'saved_at': datetime.now().isoformat(),
                'framework': 'tensorflow/keras',
                'file_path': str(model_path)
            }

            if metadata:
                meta.update(metadata)

            meta_path = self._get_metadata_path(model_path)
            with open(meta_path, 'w') as f:
                json.dump(meta, f, indent=2)

            logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {meta_path}")

            # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì €ì¥
            self._save_training_history(ticker, model_type, version, metadata)

            return str(model_path)

        except Exception as e:
            logger.error(f"âŒ Keras ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def load_keras_model(self, ticker: str, model_type: str,
                        version: Optional[str] = None) -> tuple:
        """
        Keras ëª¨ë¸ ë¡œë“œ (.h5)

        Returns:
            (model, metadata, scaler)
        """
        try:
            # TensorFlow Lazy Import
            import tensorflow as tf
            from tensorflow import keras

            model_path = self._get_model_path(ticker, model_type, version)

            if not model_path.exists():
                logger.warning(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                return None, None, None

            # ëª¨ë¸ ë¡œë“œ
            model = keras.models.load_model(str(model_path))
            logger.info(f"âœ… {model_type.upper()} ëª¨ë¸ ë¡œë“œ: {model_path}")

            # Scaler ë¡œë“œ
            scaler = None
            scaler_path = model_path.with_suffix('.scaler.pkl')
            if scaler_path.exists():
                scaler = joblib.load(scaler_path)
                logger.info(f"âœ… Scaler ë¡œë“œ: {scaler_path}")

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            meta_path = self._get_metadata_path(model_path)
            metadata = None
            if meta_path.exists():
                with open(meta_path, 'r') as f:
                    metadata = json.load(f)
                logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ: {meta_path}")

            return model, metadata, scaler

        except Exception as e:
            logger.error(f"âŒ Keras ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None, None

    # ========== Scikit-learn/XGBoost/LightGBM ëª¨ë¸ ==========

    def save_sklearn_model(self, model, ticker: str, model_type: str,
                          metadata: Optional[Dict[str, Any]] = None,
                          scaler=None) -> str:
        """
        Scikit-learn/XGBoost/LightGBM ëª¨ë¸ ì €ì¥ (.pkl)

        Args:
            model: ëª¨ë¸ ê°ì²´
            ticker: ì£¼ì‹ í‹°ì»¤
            model_type: 'xgboost', 'lightgbm', 'random_forest'
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            scaler: Scaler ê°ì²´

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        version = self._generate_version()
        model_path = self._get_model_path(ticker, model_type, version)

        try:
            # ëª¨ë¸ ì €ì¥ (joblibì´ pickleë³´ë‹¤ ë¹ ë¦„)
            joblib.dump(model, model_path)
            logger.info(f"âœ… {model_type.upper()} ëª¨ë¸ ì €ì¥: {model_path}")

            # Scaler ì €ì¥
            if scaler is not None:
                scaler_path = model_path.with_suffix('.scaler.pkl')
                joblib.dump(scaler, scaler_path)
                logger.info(f"âœ… Scaler ì €ì¥: {scaler_path}")

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            meta = {
                'ticker': ticker,
                'model_type': model_type,
                'version': version,
                'saved_at': datetime.now().isoformat(),
                'framework': self._get_framework_name(model),
                'file_path': str(model_path)
            }

            if metadata:
                meta.update(metadata)

            # Feature importance ì €ì¥ (ê°€ëŠ¥í•œ ê²½ìš°)
            if hasattr(model, 'feature_importances_'):
                meta['has_feature_importance'] = True

            meta_path = self._get_metadata_path(model_path)
            with open(meta_path, 'w') as f:
                json.dump(meta, f, indent=2)

            logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {meta_path}")

            # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì €ì¥
            self._save_training_history(ticker, model_type, version, metadata)

            return str(model_path)

        except Exception as e:
            logger.error(f"âŒ {model_type} ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def load_sklearn_model(self, ticker: str, model_type: str,
                          version: Optional[str] = None) -> tuple:
        """
        Scikit-learn/XGBoost/LightGBM ëª¨ë¸ ë¡œë“œ (.pkl)

        Returns:
            (model, metadata, scaler)
        """
        try:
            model_path = self._get_model_path(ticker, model_type, version)

            if not model_path.exists():
                logger.warning(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                return None, None, None

            # ëª¨ë¸ ë¡œë“œ
            model = joblib.load(model_path)
            logger.info(f"âœ… {model_type.upper()} ëª¨ë¸ ë¡œë“œ: {model_path}")

            # Scaler ë¡œë“œ
            scaler = None
            scaler_path = model_path.with_suffix('.scaler.pkl')
            if scaler_path.exists():
                scaler = joblib.load(scaler_path)
                logger.info(f"âœ… Scaler ë¡œë“œ: {scaler_path}")

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            meta_path = self._get_metadata_path(model_path)
            metadata = None
            if meta_path.exists():
                with open(meta_path, 'r') as f:
                    metadata = json.load(f)
                logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ: {meta_path}")

            return model, metadata, scaler

        except Exception as e:
            logger.error(f"âŒ {model_type} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None, None

    # ========== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ==========

    def _get_framework_name(self, model) -> str:
        """ëª¨ë¸ì˜ í”„ë ˆì„ì›Œí¬ ì´ë¦„ ë°˜í™˜"""
        model_class = type(model).__name__
        module = type(model).__module__

        if 'xgboost' in module:
            return 'xgboost'
        elif 'lightgbm' in module:
            return 'lightgbm'
        elif 'sklearn' in module:
            return 'scikit-learn'
        else:
            return 'unknown'

    def _save_training_history(self, ticker: str, model_type: str,
                               version: str, metadata: Optional[Dict] = None):
        """í›ˆë ¨ íˆìŠ¤í† ë¦¬ CSV íŒŒì¼ì— ì¶”ê°€"""
        model_dir = self._get_model_dir(ticker)
        history_path = model_dir / "training_history.csv"

        # íˆìŠ¤í† ë¦¬ ë ˆì½”ë“œ ìƒì„±
        record = {
            'timestamp': datetime.now().isoformat(),
            'model_type': model_type,
            'version': version,
        }

        if metadata:
            # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
            for key in ['train_loss', 'val_loss', 'rmse', 'mae', 'confidence_score']:
                if key in metadata:
                    record[key] = metadata[key]

        # CSV íŒŒì¼ì— ì¶”ê°€
        df = pd.DataFrame([record])

        if history_path.exists():
            df.to_csv(history_path, mode='a', header=False, index=False)
        else:
            df.to_csv(history_path, index=False)

        logger.debug(f"í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸: {history_path}")

    def get_training_history(self, ticker: str) -> Optional[pd.DataFrame]:
        """í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        model_dir = self._get_model_dir(ticker)
        history_path = model_dir / "training_history.csv"

        if history_path.exists():
            return pd.read_csv(history_path)
        else:
            return None

    def list_models(self, ticker: str) -> List[Dict[str, Any]]:
        """ì €ì¥ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        model_dir = self._get_model_dir(ticker)

        if not model_dir.exists():
            return []

        models = []
        for meta_file in model_dir.glob("*.json"):
            try:
                with open(meta_file, 'r') as f:
                    meta = json.load(f)
                    models.append(meta)
            except:
                continue

        # ìµœì‹ ìˆœ ì •ë ¬
        models.sort(key=lambda x: x.get('saved_at', ''), reverse=True)
        return models

    def delete_old_models(self, ticker: str, keep_latest: int = 5):
        """ì˜¤ë˜ëœ ëª¨ë¸ ì‚­ì œ (ìµœì‹  Nê°œë§Œ ìœ ì§€)"""
        models = self.list_models(ticker)

        # ëª¨ë¸ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        by_type = {}
        for model in models:
            mtype = model['model_type']
            if mtype not in by_type:
                by_type[mtype] = []
            by_type[mtype].append(model)

        # ê° íƒ€ì…ë³„ë¡œ ì˜¤ë˜ëœ ê²ƒ ì‚­ì œ
        deleted_count = 0
        for mtype, mlist in by_type.items():
            if len(mlist) > keep_latest:
                to_delete = mlist[keep_latest:]
                for model_meta in to_delete:
                    try:
                        # ëª¨ë¸ íŒŒì¼ ì‚­ì œ
                        model_path = Path(model_meta['file_path'])
                        if model_path.exists():
                            model_path.unlink()

                        # ë©”íƒ€ë°ì´í„° ì‚­ì œ
                        meta_path = self._get_metadata_path(model_path)
                        if meta_path.exists():
                            meta_path.unlink()

                        # Scaler ì‚­ì œ
                        scaler_path = model_path.with_suffix('.scaler.pkl')
                        if scaler_path.exists():
                            scaler_path.unlink()

                        deleted_count += 1
                        logger.info(f"ğŸ—‘ï¸  ì˜¤ë˜ëœ ëª¨ë¸ ì‚­ì œ: {model_path.name}")

                    except Exception as e:
                        logger.error(f"ëª¨ë¸ ì‚­ì œ ì‹¤íŒ¨: {e}")

        logger.info(f"âœ… {deleted_count}ê°œ ì˜¤ë˜ëœ ëª¨ë¸ ì‚­ì œ ì™„ë£Œ")
        return deleted_count

    # ========== ì¦ë¶„ í•™ìŠµ (Incremental Learning) ==========

    def supports_incremental_learning(self, model_type: str) -> bool:
        """ëª¨ë¸ì´ ì¦ë¶„ í•™ìŠµì„ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸"""
        # XGBoost, LightGBMì€ ê¸°ì¡´ ëª¨ë¸ì—ì„œ ê³„ì† í•™ìŠµ ê°€ëŠ¥
        incremental_models = ['xgboost', 'lightgbm']
        return model_type in incremental_models

    def incremental_train_xgboost(self, ticker: str, X_new, y_new,
                                  n_estimators_add: int = 50) -> Any:
        """
        XGBoost ì¦ë¶„ í•™ìŠµ

        Args:
            ticker: ì£¼ì‹ í‹°ì»¤
            X_new: ìƒˆë¡œìš´ í•™ìŠµ ë°ì´í„°
            y_new: ìƒˆë¡œìš´ íƒ€ê²Ÿ
            n_estimators_add: ì¶”ê°€í•  íŠ¸ë¦¬ ê°œìˆ˜

        Returns:
            ì—…ë°ì´íŠ¸ëœ ëª¨ë¸
        """
        try:
            import xgboost as xgb

            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            model, metadata, scaler = self.load_sklearn_model(ticker, 'xgboost')

            if model is None:
                logger.warning("ê¸°ì¡´ XGBoost ëª¨ë¸ ì—†ìŒ, ìƒˆë¡œ í›ˆë ¨í•´ì•¼ í•©ë‹ˆë‹¤")
                return None

            logger.info(f"XGBoost ì¦ë¶„ í•™ìŠµ ì‹œì‘... (ê¸°ì¡´ íŠ¸ë¦¬: {model.n_estimators})")

            # ìƒˆ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ
            # XGBoostëŠ” xgb_model íŒŒë¼ë¯¸í„°ë¡œ ê¸°ì¡´ ëª¨ë¸ ì „ë‹¬
            new_model = xgb.XGBRegressor(
                n_estimators=model.n_estimators + n_estimators_add,
                max_depth=model.max_depth,
                learning_rate=model.learning_rate,
                random_state=42,
                verbosity=0
            )

            # Scaler ì ìš© (ìˆìœ¼ë©´)
            if scaler is not None:
                X_new_scaled = scaler.transform(X_new)
            else:
                X_new_scaled = X_new

            # ê¸°ì¡´ ëª¨ë¸ì—ì„œ ê³„ì† í•™ìŠµ
            new_model.fit(X_new_scaled, y_new, xgb_model=model.get_booster())

            logger.info(f"âœ… XGBoost ì¦ë¶„ í•™ìŠµ ì™„ë£Œ (ì´ íŠ¸ë¦¬: {new_model.n_estimators})")

            # ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ ì €ì¥
            new_metadata = metadata.copy() if metadata else {}
            new_metadata['incremental_learning'] = True
            new_metadata['previous_version'] = metadata.get('version', 'unknown')
            new_metadata['trees_added'] = n_estimators_add

            self.save_sklearn_model(new_model, ticker, 'xgboost', new_metadata, scaler)

            return new_model

        except Exception as e:
            logger.error(f"âŒ XGBoost ì¦ë¶„ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return None

    def incremental_train_lightgbm(self, ticker: str, X_new, y_new,
                                   n_estimators_add: int = 50) -> Any:
        """
        LightGBM ì¦ë¶„ í•™ìŠµ
        """
        try:
            import lightgbm as lgb

            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            model, metadata, scaler = self.load_sklearn_model(ticker, 'lightgbm')

            if model is None:
                logger.warning("ê¸°ì¡´ LightGBM ëª¨ë¸ ì—†ìŒ, ìƒˆë¡œ í›ˆë ¨í•´ì•¼ í•©ë‹ˆë‹¤")
                return None

            logger.info(f"LightGBM ì¦ë¶„ í•™ìŠµ ì‹œì‘... (ê¸°ì¡´ íŠ¸ë¦¬: {model.n_estimators})")

            # ìƒˆ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ
            new_model = lgb.LGBMRegressor(
                n_estimators=model.n_estimators + n_estimators_add,
                max_depth=model.max_depth,
                learning_rate=model.learning_rate,
                random_state=42,
                verbosity=-1
            )

            # Scaler ì ìš©
            if scaler is not None:
                X_new_scaled = scaler.transform(X_new)
            else:
                X_new_scaled = X_new

            # ê¸°ì¡´ ëª¨ë¸ì—ì„œ ê³„ì† í•™ìŠµ
            new_model.fit(
                X_new_scaled, y_new,
                init_model=model.booster_
            )

            logger.info(f"âœ… LightGBM ì¦ë¶„ í•™ìŠµ ì™„ë£Œ (ì´ íŠ¸ë¦¬: {new_model.n_estimators})")

            # ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ ì €ì¥
            new_metadata = metadata.copy() if metadata else {}
            new_metadata['incremental_learning'] = True
            new_metadata['previous_version'] = metadata.get('version', 'unknown')
            new_metadata['trees_added'] = n_estimators_add

            self.save_sklearn_model(new_model, ticker, 'lightgbm', new_metadata, scaler)

            return new_model

        except Exception as e:
            logger.error(f"âŒ LightGBM ì¦ë¶„ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return None


# ========== ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ==========
_model_persistence = None

def get_model_persistence() -> ModelPersistence:
    """ì „ì—­ ModelPersistence ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _model_persistence
    if _model_persistence is None:
        _model_persistence = ModelPersistence()
    return _model_persistence


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    persistence = ModelPersistence()

    print("=== Model Persistence í…ŒìŠ¤íŠ¸ ===")
    print(f"Base directory: {persistence.base_dir}")

    # ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì˜ˆì œ
    models = persistence.list_models("AAPL")
    print(f"\nì €ì¥ëœ AAPL ëª¨ë¸ ê°œìˆ˜: {len(models)}")

    if models:
        print("\nìµœê·¼ ëª¨ë¸:")
        for model in models[:3]:
            print(f"  - {model['model_type']}: {model['version']} ({model['saved_at']})")
