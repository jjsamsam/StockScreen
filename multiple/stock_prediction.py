#!/usr/bin/env python3
"""
stock_prediction.py
TensorFlow ì—†ì´ scikit-learn, XGBoost, LightGBM, statsmodelsë¥¼ ì‚¬ìš©í•œ ì£¼ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- Kalman Filter (ìˆœìˆ˜ NumPy êµ¬í˜„)
- XGBoost Regressor
- LightGBM Regressor  
- Random Forest
- ARIMA
- Ensemble Method
"""

import numpy as np
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

# ë¡œê¹… ì„¤ì •
from logger_config import get_logger
logger = get_logger(__name__)

# ìµœì í™” ëª¨ë“ˆ
from cache_manager import get_stock_data

# ğŸš€ Enhanced Trading System - ìƒˆë¡œìš´ ëª¨ë“ˆë“¤ (ì¡°ìš©íˆ í†µí•©)
try:
    from regime_detector_enhanced import EnhancedRegimeDetector, fetch_market_data
    from ensemble_weight_optimizer import EnsembleWeightOptimizer, BrierScoreCalculator
    ENHANCED_REGIME_AVAILABLE = True
    logger.info("âœ… Enhanced Regime Detection í™œì„±í™”")
except ImportError as e:
    logger.debug(f"Enhanced modules not available: {e}")
    ENHANCED_REGIME_AVAILABLE = False

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from sklearn.linear_model import LinearRegression
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
    from sklearn.metrics import mean_squared_error, mean_absolute_error
    from sklearn.model_selection import train_test_split, TimeSeriesSplit
    SKLEARN_AVAILABLE = True
    logger.info("scikit-learn ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.error("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install scikit-learn")
    SKLEARN_AVAILABLE = False

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
    logger.info(f"XGBoost ì‚¬ìš© ê°€ëŠ¥ (v{xgb.__version__})")
except ImportError:
    logger.error("XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install xgboost")
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
    logger.info(f"LightGBM ì‚¬ìš© ê°€ëŠ¥ (v{lgb.__version__})")
except ImportError:
    logger.error("LightGBMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install lightgbm")
    LIGHTGBM_AVAILABLE = False

try:
    from statsmodels.tsa.arima.model import ARIMA
    import statsmodels.api as sm
    STATSMODELS_AVAILABLE = True
    logger.info("statsmodels ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.error("statsmodelsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install statsmodels")
    STATSMODELS_AVAILABLE = False

# TensorFlow Lazy Import (ì‹¤ì œ ì‚¬ìš© ì‹œì ì— import)
TENSORFLOW_AVAILABLE = None  # None = ì•„ì§ í™•ì¸ ì•ˆ í•¨, True/False = í™•ì¸ ì™„ë£Œ
_tensorflow_modules = {}  # ìºì‹±ìš©

def _lazy_import_tensorflow():
    """TensorFlowë¥¼ í•„ìš”í•  ë•Œë§Œ import (Lazy Loading)"""
    global TENSORFLOW_AVAILABLE, _tensorflow_modules

    if TENSORFLOW_AVAILABLE is not None:
        return TENSORFLOW_AVAILABLE

    try:
        logger.info("TensorFlow ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒ, ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        # Helpful environment info for debugging mismatched environments
        import sys, platform
        logger.info(f"Python: {sys.version.split()[0]} | Exec: {sys.executable} | OS: {platform.platform()}")
        import os, re
        # On Windows + PyQt, DLL search path may be polluted by Qt bin via AddDllDirectory.
        # 1) Prepend safe DLL directories explicitly (System32, Python's DLLs) using add_dll_directory
        # 2) Temporarily remove Qt bin paths from PATH during TF import
        try:
            import sys
            if os.name == 'nt' and hasattr(os, 'add_dll_directory'):
                _system32 = os.path.join(os.environ.get('SystemRoot', r'C:\\Windows'), 'System32')
                _py_dlls = os.path.join(sys.prefix, 'DLLs')
                _py_dir = os.path.dirname(sys.executable)
                for _dir in (_system32, _py_dlls, _py_dir):
                    if os.path.isdir(_dir):
                        try:
                            os.add_dll_directory(_dir)
                            logger.info(f"Added DLL search dir: {_dir}")
                        except Exception:
                            pass
        except Exception:
            pass

        # Temporarily remove Qt bin paths from PATH to avoid DLL conflicts when importing TensorFlow under PyQt GUI
        _orig_path = os.environ.get('PATH', '')
        _path_parts = _orig_path.split(os.pathsep) if _orig_path else []
        # Detect Qt bin paths robustly (both slash styles, case-insensitive)
        def _norm(p: str) -> str:
            return p.replace('/', '\\').lower()
        _qt_markers = ("\\pyqt5\\qt5\\bin", "\\pyside6\\qt\\bin", "\\qt\\bin")
        _removed_qt_paths = [p for p in _path_parts if any(m in _norm(p) for m in _qt_markers)]
        if _removed_qt_paths:
            os.environ['PATH'] = os.pathsep.join([p for p in _path_parts if p not in _removed_qt_paths])
            sample = "; ".join(_removed_qt_paths[:2])
            logger.info(f"Temporarily sanitized PATH for TF import (removed {len(_removed_qt_paths)} Qt bin entries): {sample}...")
        try:
            import tensorflow as tf
        finally:
            # Restore original PATH regardless of import outcome
            if _removed_qt_paths:
                os.environ['PATH'] = _orig_path
                logger.info("Restored original PATH after TF import attempt")
        from tensorflow import keras
        from tensorflow.keras import layers, Model
        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

        _tensorflow_modules['tf'] = tf
        _tensorflow_modules['keras'] = keras
        _tensorflow_modules['layers'] = layers
        _tensorflow_modules['Model'] = Model
        _tensorflow_modules['EarlyStopping'] = EarlyStopping
        _tensorflow_modules['ReduceLROnPlateau'] = ReduceLROnPlateau

        TENSORFLOW_AVAILABLE = True
        logger.info(f"âœ… TensorFlow ë¡œë”© ì™„ë£Œ (v{tf.__version__})")
        return True
    except ImportError as e:
        # Log full traceback to diagnose DLL/runtime import errors
        logger.warning("TensorFlow ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ImportError)", exc_info=True)
        try:
            import os, sys, platform, ctypes, ctypes.util
            logger.info(f"CWD: {os.getcwd()}")
            logger.info(f"Arch: {platform.machine()} | 64bit: {ctypes.sizeof(ctypes.c_void_p) == 8}")
            # Environment snapshot (trim overly long PATH in logs, but include start and end)
            path = os.environ.get('PATH', '')
            if len(path) > 400:
                logger.info(f"PATH(start): {path[:200]}")
                logger.info(f"PATH(end):   {path[-200:]}")
            else:
                logger.info(f"PATH: {path}")
            # Check VC++ runtime presence
            for dll in ("vcruntime140_1.dll", "vcruntime140.dll", "msvcp140_1.dll", "msvcp140.dll"):
                try:
                    ctypes.CDLL(dll)
                    logger.info(f"DLL available: {dll}")
                except OSError as de:
                    logger.info(f"DLL missing: {dll} -> {de}")
        except Exception:
            pass
        logger.info("LSTM/Transformer ëª¨ë¸ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤ (XGBoost/LightGBM ë“± ë‹¤ë¥¸ ëª¨ë¸ì€ ì •ìƒ ì‘ë™)")
        TENSORFLOW_AVAILABLE = False
        return False

def reset_tensorflow_import():
    """Reset TensorFlow lazy-import state so it can be retried.

    Call this after fixing an environment issue so the next use can
    attempt a fresh import via _lazy_import_tensorflow().
    """
    global TENSORFLOW_AVAILABLE, _tensorflow_modules
    TENSORFLOW_AVAILABLE = None
    _tensorflow_modules.clear()

try:
    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
    from skopt import BayesSearchCV
    from skopt.space import Real, Integer, Categorical
    HYPEROPT_AVAILABLE = True
    logger.info("Hyperparameter ìµœì í™” ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.warning("scikit-optimize ì„¤ì¹˜ ê¶Œì¥: pip install scikit-optimize")
    HYPEROPT_AVAILABLE = False


class MarketDataFetcher:
    """ì™¸ë¶€ ì‹œì¥ ë°ì´í„°(ê±°ì‹œì§€í‘œ, ì„¹í„° ETF ë“±)ë¥¼ ê°€ì ¸ì˜¤ëŠ” í´ë˜ìŠ¤"""

    # ì£¼ìš” ì„¹í„° ETF ì‹¬ë³¼
    SECTOR_ETFS = {
        'technology': 'XLK',      # Technology Select Sector
        'financial': 'XLF',       # Financial Select Sector
        'healthcare': 'XLV',      # Healthcare Select Sector
        'energy': 'XLE',          # Energy Select Sector
        'consumer_disc': 'XLY',   # Consumer Discretionary
        'consumer_staples': 'XLP', # Consumer Staples
        'industrial': 'XLI',      # Industrial Select Sector
        'materials': 'XLB',       # Materials Select Sector
        'utilities': 'XLU',       # Utilities Select Sector
        'real_estate': 'XLRE',    # Real Estate Select Sector
    }

    # ì£¼ìš” ê±°ì‹œì§€í‘œ ì‹¬ë³¼ (yfinanceë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œ ê²ƒë“¤)
    MACRO_INDICATORS = {
        'vix': '^VIX',           # Volatility Index
        'sp500': '^GSPC',        # S&P 500
        'nasdaq': '^IXIC',       # NASDAQ
        'dxy': 'DX-Y.NYB',       # US Dollar Index
        'treasury_10y': '^TNX',  # 10-Year Treasury Yield
        'treasury_2y': '^IRX',   # 2-Year Treasury Yield (ì¶”ê°€)
        'oil': 'CL=F',           # Crude Oil
        'gold': 'GC=F',          # Gold
    }

    @staticmethod
    def fetch_sector_etf(sector, period='1y', interval='1d'):
        """
        ì„¹í„° ETF ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

        Args:
            sector: ì„¹í„° ì´ë¦„ (ì˜ˆ: 'technology', 'financial')
            period: ê¸°ê°„
            interval: ê°„ê²©

        Returns:
            DataFrame with ETF data or None
        """
        if sector not in MarketDataFetcher.SECTOR_ETFS:
            logger.warning(f"Unknown sector: {sector}")
            return None

        symbol = MarketDataFetcher.SECTOR_ETFS[sector]
        try:
            data = get_stock_data(symbol, period=period, interval=interval, validate_cache=False)
            if data is not None and not data.empty:
                logger.debug(f"Fetched {sector} sector ETF ({symbol}): {len(data)} data points")
                return data
        except Exception as e:
            logger.error(f"Error fetching sector ETF {sector} ({symbol}): {e}")

        return None

    @staticmethod
    def fetch_macro_indicator(indicator, period='1y', interval='1d'):
        """
        ê±°ì‹œì§€í‘œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

        Args:
            indicator: ì§€í‘œ ì´ë¦„ (ì˜ˆ: 'vix', 'sp500', 'treasury_10y')
            period: ê¸°ê°„
            interval: ê°„ê²©

        Returns:
            DataFrame with indicator data or None
        """
        if indicator not in MarketDataFetcher.MACRO_INDICATORS:
            logger.warning(f"Unknown indicator: {indicator}")
            return None

        symbol = MarketDataFetcher.MACRO_INDICATORS[indicator]
        try:
            data = get_stock_data(symbol, period=period, interval=interval, validate_cache=False)
            if data is not None and not data.empty:
                logger.debug(f"Fetched {indicator} ({symbol}): {len(data)} data points")
                return data
        except Exception as e:
            logger.error(f"Error fetching macro indicator {indicator} ({symbol}): {e}")

        return None

    @staticmethod
    def align_and_merge(primary_data, auxiliary_data, prefix='aux'):
        """
        ì£¼ìš” ë°ì´í„°ì™€ ë³´ì¡° ë°ì´í„°ë¥¼ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ ë° ë³‘í•©

        Args:
            primary_data: ì£¼ìš” ì£¼ê°€ ë°ì´í„° (DataFrame)
            auxiliary_data: ë³´ì¡° ë°ì´í„° (DataFrame)
            prefix: ë³´ì¡° ë°ì´í„° ì»¬ëŸ¼ ì ‘ë‘ì‚¬

        Returns:
            Merged DataFrame
        """
        if auxiliary_data is None or auxiliary_data.empty:
            return primary_data

        try:
            # ë³´ì¡° ë°ì´í„°ì˜ Close ê°€ê²©ë§Œ ì‚¬ìš© (ìƒëŒ€ ë³€í™”ìœ¨ ê³„ì‚°ìš©)
            aux_close = auxiliary_data[['Close']].copy()
            aux_close.columns = [f'{prefix}_close']

            # ë‚ ì§œ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© (forward fillë¡œ ëˆ„ë½ ë°ì´í„° ì±„ìš°ê¸°)
            merged = primary_data.join(aux_close, how='left')
            merged[f'{prefix}_close'] = merged[f'{prefix}_close'].fillna(method='ffill')

            # ë³´ì¡° ë°ì´í„°ì˜ ë³€í™”ìœ¨ ê³„ì‚°
            merged[f'{prefix}_return'] = merged[f'{prefix}_close'].pct_change()
            merged[f'{prefix}_ma5'] = merged[f'{prefix}_close'].rolling(5).mean()
            merged[f'{prefix}_ma20'] = merged[f'{prefix}_close'].rolling(20).mean()

            logger.debug(f"Merged auxiliary data with prefix '{prefix}': {len(merged)} data points")
            return merged

        except Exception as e:
            logger.error(f"Error merging auxiliary data: {e}")
            return primary_data


class KalmanFilterPredictor:
    """
    ìˆœìˆ˜ NumPyë¡œ êµ¬í˜„í•œ Kalman Filterë¥¼ ì‚¬ìš©í•œ ì£¼ê°€ ì˜ˆì¸¡
    
    ì›ë¦¬:
    - ìƒíƒœ ê³µê°„ ëª¨ë¸ë¡œ ì£¼ê°€ì˜ ìˆ¨ê²¨ì§„ ìƒíƒœ(ì¶”ì„¸, ë…¸ì´ì¦ˆ)ë¥¼ ì¶”ì 
    - ì´ì „ ìƒíƒœ + ê´€ì¸¡ê°’ì„ ê²°í•©í•˜ì—¬ ìµœì  ì¶”ì •
    - ë‹¨ê¸° ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ì§„ì§œ ì¶”ì„¸ íŒŒì•…
    """
    
    def __init__(self, process_variance=1e-5, measurement_variance=1e-1):
        """
        Args:
            process_variance: í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ (ì‘ì„ìˆ˜ë¡ ì•ˆì •ì )
            measurement_variance: ì¸¡ì • ë…¸ì´ì¦ˆ (í´ìˆ˜ë¡ ê´€ì¸¡ê°’ ëœ ì‹ ë¢°)
        """
        self.process_variance = process_variance
        self.measurement_variance = measurement_variance
        self.reset()
    
    def reset(self):
        """ì¹¼ë§Œ í•„í„° ìƒíƒœ ì´ˆê¸°í™”"""
        self.x = 0  # ìƒíƒœ ì¶”ì •ê°’ (ê°€ê²©)
        self.P = 1  # ì˜¤ì°¨ ê³µë¶„ì‚°
        self.Q = self.process_variance  # í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ
        self.R = self.measurement_variance  # ì¸¡ì • ë…¸ì´ì¦ˆ
        
        self.predictions = []
        self.states = []
    
    def predict_and_update(self, measurement):
        """ì¹¼ë§Œ í•„í„°ì˜ ì˜ˆì¸¡-ì—…ë°ì´íŠ¸ ë‹¨ê³„"""
        # 1. ì˜ˆì¸¡ ë‹¨ê³„
        x_pred = self.x
        P_pred = self.P + self.Q
        
        # 2. ì—…ë°ì´íŠ¸ ë‹¨ê³„
        K = P_pred / (P_pred + self.R)
        self.x = x_pred + K * (measurement - x_pred)
        self.P = (1 - K) * P_pred
        
        # ê²°ê³¼ ì €ì¥
        self.predictions.append(self.x)
        self.states.append({'x': self.x, 'P': self.P, 'K': K})
        
        return self.x
    
    def fit_predict(self, prices, forecast_days=5):
        """ê°€ê²© ì‹œê³„ì—´ì— ì¹¼ë§Œ í•„í„° ì ìš© ë° ë¯¸ë˜ ì˜ˆì¸¡"""
        self.reset()
        
        # ëª¨ë“  ê´€ì¸¡ê°’ì— ëŒ€í•´ ì¹¼ë§Œ í•„í„° ì ìš©
        filtered_prices = []
        for price in prices:
            filtered_price = self.predict_and_update(price)
            filtered_prices.append(filtered_price)
        
        # âœ… Kalman Filter ë¯¸ë˜ ì˜ˆì¸¡
        # í˜„ì¬ê°€ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ (í•„í„°ë§ ê°’ê³¼ì˜ ì°¨ì´ ë³´ì •)
        current_price = prices[-1]
        last_filtered = filtered_prices[-1]

        # í•„í„°ë§ëœ ì¶”ì„¸ ê³„ì‚°
        if len(filtered_prices) >= 10:
            trend = (filtered_prices[-1] - filtered_prices[-10]) / 10
        else:
            trend = filtered_prices[-1] - filtered_prices[-2] if len(filtered_prices) >= 2 else 0

        # í˜„ì¬ê°€ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ (í•„í„°ë§ ì˜¤ì°¨ ë³´ì •)
        future_predictions = []
        for i in range(forecast_days):
            # í˜„ì¬ê°€ + ì¶”ì„¸
            future_pred = current_price + trend * (i + 1)
            future_predictions.append(future_pred)
        
        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
        confidence_interval = 2 * np.sqrt(self.P)
        
        return {
            'filtered_prices': np.array(filtered_prices),
            'future_predictions': np.array(future_predictions),
            'confidence_interval': confidence_interval,
            'last_state': self.x,
            'uncertainty': self.P
        }

class HyperparameterOptimizer:
    """Bayesian Optimizationì„ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""

    # ìµœì í™” ê²°ê³¼ ìºì‹œ (tickerë³„ë¡œ ì €ì¥)
    _optimization_cache = {}

    @staticmethod
    def get_cached_params(ticker, model_type):
        """ìºì‹œëœ ìµœì  íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°"""
        cache_key = f"{ticker}_{model_type}"
        if cache_key in HyperparameterOptimizer._optimization_cache:
            params, timestamp = HyperparameterOptimizer._optimization_cache[cache_key]
            # ìºì‹œê°€ 7ì¼ ì´ë‚´ë©´ ì¬ì‚¬ìš©
            if (datetime.now() - timestamp).days < 7:
                logger.info(f"Using cached hyperparameters for {ticker} {model_type}")
                return params
        return None

    @staticmethod
    def cache_params(ticker, model_type, params):
        """ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_key = f"{ticker}_{model_type}"
        HyperparameterOptimizer._optimization_cache[cache_key] = (params, datetime.now())
        logger.debug(f"Cached hyperparameters for {ticker} {model_type}")

    @staticmethod
    def optimize_xgboost(X_train, y_train, n_iter=20, ticker=None):
        """XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        if not HYPEROPT_AVAILABLE or not XGBOOST_AVAILABLE:
            logger.warning("Bayesian Optimization ë¶ˆê°€ - ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
            return None

        # ìºì‹œëœ íŒŒë¼ë¯¸í„° í™•ì¸
        if ticker:
            cached_params = HyperparameterOptimizer.get_cached_params(ticker, 'xgboost')
            if cached_params:
                return xgb.XGBRegressor(**cached_params, random_state=42, verbosity=0)

        search_spaces = {
            'n_estimators': Integer(100, 500),
            'max_depth': Integer(3, 15),
            'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
            'min_child_weight': Integer(1, 10),
            'subsample': Real(0.6, 1.0),
            'colsample_bytree': Real(0.6, 1.0),
            'gamma': Real(0.0, 0.5),
            'reg_alpha': Real(0.0, 1.0),
            'reg_lambda': Real(0.0, 2.0)
        }

        bayes_cv = BayesSearchCV(
            xgb.XGBRegressor(random_state=42, verbosity=0),
            search_spaces,
            n_iter=n_iter,
            cv=3,
            n_jobs=-1,
            scoring='neg_mean_squared_error',
            verbose=0
        )

        bayes_cv.fit(X_train, y_train)
        logger.info(f"XGBoost ìµœì  íŒŒë¼ë¯¸í„°: {bayes_cv.best_params_}")

        # ìµœì  íŒŒë¼ë¯¸í„° ìºì‹±
        if ticker:
            HyperparameterOptimizer.cache_params(ticker, 'xgboost', bayes_cv.best_params_)

        return bayes_cv.best_estimator_

    @staticmethod
    def optimize_lightgbm(X_train, y_train, n_iter=20, ticker=None):
        """LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        if not HYPEROPT_AVAILABLE or not LIGHTGBM_AVAILABLE:
            logger.warning("Bayesian Optimization ë¶ˆê°€ - ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
            return None

        # ìºì‹œëœ íŒŒë¼ë¯¸í„° í™•ì¸
        if ticker:
            cached_params = HyperparameterOptimizer.get_cached_params(ticker, 'lightgbm')
            if cached_params:
                return lgb.LGBMRegressor(**cached_params, random_state=42, verbosity=-1)

        search_spaces = {
            'n_estimators': Integer(100, 500),
            'max_depth': Integer(3, 15),
            'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
            'num_leaves': Integer(20, 100),
            'min_child_samples': Integer(10, 50),
            'subsample': Real(0.6, 1.0),
            'colsample_bytree': Real(0.6, 1.0),
            'reg_alpha': Real(0.0, 1.0),
            'reg_lambda': Real(0.0, 2.0)
        }

        bayes_cv = BayesSearchCV(
            lgb.LGBMRegressor(random_state=42, verbosity=-1),
            search_spaces,
            n_iter=n_iter,
            cv=3,
            n_jobs=-1,
            scoring='neg_mean_squared_error',
            verbose=0
        )

        bayes_cv.fit(X_train, y_train)
        logger.info(f"LightGBM ìµœì  íŒŒë¼ë¯¸í„°: {bayes_cv.best_params_}")

        # ìµœì  íŒŒë¼ë¯¸í„° ìºì‹±
        if ticker:
            HyperparameterOptimizer.cache_params(ticker, 'lightgbm', bayes_cv.best_params_)

        return bayes_cv.best_estimator_

    @staticmethod
    def optimize_random_forest(X_train, y_train, n_iter=20, ticker=None):
        """Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        if not HYPEROPT_AVAILABLE or not SKLEARN_AVAILABLE:
            logger.warning("Bayesian Optimization ë¶ˆê°€ - ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
            return None

        # ìºì‹œëœ íŒŒë¼ë¯¸í„° í™•ì¸
        if ticker:
            cached_params = HyperparameterOptimizer.get_cached_params(ticker, 'random_forest')
            if cached_params:
                return RandomForestRegressor(**cached_params, random_state=42, n_jobs=-1)

        search_spaces = {
            'n_estimators': Integer(100, 500),
            'max_depth': Integer(5, 30),
            'min_samples_split': Integer(2, 20),
            'min_samples_leaf': Integer(1, 10),
            'max_features': Categorical(['sqrt', 'log2', None])
        }

        bayes_cv = BayesSearchCV(
            RandomForestRegressor(random_state=42, n_jobs=-1),
            search_spaces,
            n_iter=n_iter,
            cv=3,
            n_jobs=1,  # RF ìì²´ê°€ ë³‘ë ¬ì²˜ë¦¬
            scoring='neg_mean_squared_error',
            verbose=0
        )

        bayes_cv.fit(X_train, y_train)
        logger.info(f"Random Forest ìµœì  íŒŒë¼ë¯¸í„°: {bayes_cv.best_params_}")

        # ìµœì  íŒŒë¼ë¯¸í„° ìºì‹±
        if ticker:
            HyperparameterOptimizer.cache_params(ticker, 'random_forest', bayes_cv.best_params_)

        return bayes_cv.best_estimator_


class AdvancedMLPredictor:
    """
    XGBoost, LightGBM, Random Forestë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ê¸°
    """

    def __init__(self, sequence_length=30, use_optimization=True, ticker=None, auto_load=True):
        self.sequence_length = sequence_length
        self.scaler = RobustScaler()  # StandardScaler -> RobustScaler (ì´ìƒì¹˜ì— ê°•í•¨)
        self.models = {}
        self.use_optimization = use_optimization
        self.progress_callback = None  # ì§„í–‰ ì½œë°± (ì™¸ë¶€ì—ì„œ ì„¤ì •)
        self.ticker = ticker
        self.persistence = None
        self.loaded_model_meta = None
        self.loaded_model_meta = None
        self.loaded_model_meta = None

        # ëª¨ë¸ ì €ì¥/ë¡œë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            from model_persistence import get_model_persistence
            self.persistence = get_model_persistence()

            # ìë™ ë¡œë“œ
            if auto_load and ticker:
                self._try_load_models()
        except ImportError:
            logger.debug("model_persistence ëª¨ë“ˆ ì—†ìŒ, ì €ì¥/ë¡œë“œ ë¹„í™œì„±í™”")

    def _try_load_models(self):
        """ì €ì¥ëœ ML ëª¨ë¸ë“¤ ìë™ ë¡œë“œ ì‹œë„"""
        if not self.persistence or not self.ticker:
            return

        # TensorFlowê°€ ì—†ìœ¼ë©´ Keras ëª¨ë¸ ìë™ ë¡œë“œë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŒ
        if TENSORFLOW_AVAILABLE is None:
            _lazy_import_tensorflow()
        if not TENSORFLOW_AVAILABLE:
            logger.debug("TensorFlow ì—†ìŒ - LSTM ìë™ ë¡œë“œ ê±´ë„ˆëœ€")
            return False

        loaded_count = 0
        for model_type in ['random_forest', 'xgboost', 'lightgbm']:
            try:
                model, metadata, scaler = self.persistence.load_sklearn_model(self.ticker, model_type)
                if model is not None:
                    self.models[model_type] = model
                    if scaler is not None and loaded_count == 0:  # ì²« ë²ˆì§¸ ëª¨ë¸ì˜ scaler ì‚¬ìš©
                        self.scaler = scaler
                    loaded_count += 1
                    logger.info(f"âœ… ì €ì¥ëœ {model_type} ëª¨ë¸ ë¡œë“œ: {self.ticker}")
            except Exception as e:
                logger.debug(f"{model_type} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        if loaded_count > 0:
            logger.info(f"âœ… ì´ {loaded_count}ê°œ ML ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True

        return False
        
    def create_features(self, data):
        """ê¸°ìˆ ì  ì§€í‘œë¥¼ í¬í•¨í•œ í”¼ì²˜ ìƒì„± - ê³ ê¸‰ ì§€í‘œ ì¶”ê°€"""
        df = pd.DataFrame()

        # ê¸°ë³¸ ê°€ê²© ì •ë³´
        df['close'] = data
        df['high'] = data  # ê°„ë‹¨í™”ë¥¼ ìœ„í•´ closeì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        df['low'] = data
        df['volume'] = 1000000  # ë”ë¯¸ ê±°ë˜ëŸ‰

        # === ê¸°ì¡´ ì§€í‘œ ===
        # ì´ë™í‰ê· 
        df['ma5'] = df['close'].rolling(5).mean()
        df['ma10'] = df['close'].rolling(10).mean()
        df['ma20'] = df['close'].rolling(20).mean()
        df['ma50'] = df['close'].rolling(50).mean()
        df['ma200'] = df['close'].rolling(200).mean()

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # MACD
        exp1 = df['close'].ewm(span=12).mean()
        exp2 = df['close'].ewm(span=26).mean()
        df['macd'] = exp1 - exp2
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']

        # ë³¼ë¦°ì € ë°´ë“œ
        df['bb_middle'] = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']

        # ë³€í™”ìœ¨
        df['pct_change_1'] = df['close'].pct_change()
        df['pct_change_5'] = df['close'].pct_change(5)
        df['pct_change_10'] = df['close'].pct_change(10)

        # ë³€ë™ì„±
        df['volatility'] = df['close'].rolling(20).std()

        # === ìƒˆë¡œìš´ ê³ ê¸‰ ì§€í‘œ ===
        # ATR (Average True Range) - ë³€ë™ì„±
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        df['atr'] = true_range.rolling(14).mean()

        # ADX (Average Directional Index) - ì¶”ì„¸ ê°•ë„
        plus_dm = df['high'].diff()
        minus_dm = -df['low'].diff()
        plus_dm[plus_dm < 0] = 0
        minus_dm[minus_dm < 0] = 0

        tr14 = true_range.rolling(14).sum()
        plus_di = 100 * (plus_dm.rolling(14).sum() / tr14)
        minus_di = 100 * (minus_dm.rolling(14).sum() / tr14)

        dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di)
        df['adx'] = dx.rolling(14).mean()
        df['plus_di'] = plus_di
        df['minus_di'] = minus_di

        # Stochastic Oscillator - ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„
        low_14 = df['low'].rolling(14).min()
        high_14 = df['high'].rolling(14).max()
        df['stoch_k'] = 100 * (df['close'] - low_14) / (high_14 - low_14)
        df['stoch_d'] = df['stoch_k'].rolling(3).mean()

        # OBV (On-Balance Volume) - ê±°ë˜ëŸ‰ ê¸°ë°˜
        obv = [0]
        for i in range(1, len(df)):
            if df['close'].iloc[i] > df['close'].iloc[i-1]:
                obv.append(obv[-1] + df['volume'].iloc[i])
            elif df['close'].iloc[i] < df['close'].iloc[i-1]:
                obv.append(obv[-1] - df['volume'].iloc[i])
            else:
                obv.append(obv[-1])
        df['obv'] = obv
        df['obv_ma'] = df['obv'].rolling(20).mean()

        # Ichimoku Cloud
        nine_period_high = df['high'].rolling(9).max()
        nine_period_low = df['low'].rolling(9).min()
        df['tenkan_sen'] = (nine_period_high + nine_period_low) / 2

        period26_high = df['high'].rolling(26).max()
        period26_low = df['low'].rolling(26).min()
        df['kijun_sen'] = (period26_high + period26_low) / 2

        df['senkou_span_a'] = ((df['tenkan_sen'] + df['kijun_sen']) / 2).shift(26)

        period52_high = df['high'].rolling(52).max()
        period52_low = df['low'].rolling(52).min()
        df['senkou_span_b'] = ((period52_high + period52_low) / 2).shift(26)

        # Williams %R
        df['williams_r'] = -100 * (high_14 - df['close']) / (high_14 - low_14)

        # CCI (Commodity Channel Index)
        tp = (df['high'] + df['low'] + df['close']) / 3
        df['cci'] = (tp - tp.rolling(20).mean()) / (0.015 * tp.rolling(20).std())

        # ROC (Rate of Change)
        df['roc'] = ((df['close'] - df['close'].shift(10)) / df['close'].shift(10)) * 100

        # MFI (Money Flow Index)
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        money_flow = typical_price * df['volume']

        positive_flow = []
        negative_flow = []
        for i in range(1, len(df)):
            if typical_price.iloc[i] > typical_price.iloc[i-1]:
                positive_flow.append(money_flow.iloc[i])
                negative_flow.append(0)
            elif typical_price.iloc[i] < typical_price.iloc[i-1]:
                positive_flow.append(0)
                negative_flow.append(money_flow.iloc[i])
            else:
                positive_flow.append(0)
                negative_flow.append(0)

        positive_flow = [0] + positive_flow
        negative_flow = [0] + negative_flow

        positive_mf = pd.Series(positive_flow).rolling(14).sum()
        negative_mf = pd.Series(negative_flow).rolling(14).sum()
        mfi = 100 - (100 / (1 + positive_mf / negative_mf))
        df['mfi'] = mfi

        # === ë¹„ì„ í˜• ì¡°í•© ì§€í‘œ (ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§) ===
        # ADX Ã— RSI: ì¶”ì„¸ ê°•ë„ì™€ ëª¨ë©˜í…€ì˜ ê²°í•© (ê°•í•œ ì¶”ì„¸ì—ì„œ ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ê°ì§€)
        df['adx_rsi'] = df['adx'] * df['rsi'] / 100

        # Bollinger Width Ã— Volatility: ë³€ë™ì„±ì˜ ë³€í™”ìœ¨ (ì¥ì„¸ ì „í™˜ ê°ì§€)
        df['bb_vol_product'] = df['bb_width'] * df['volatility']

        # RSI Ã— Stochastic: ì´ì¤‘ ì˜¤ì‹¤ë ˆì´í„° ê²°í•© (ë” ê°•ë ¥í•œ ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ì‹ í˜¸)
        df['rsi_stoch'] = df['rsi'] * df['stoch_k'] / 100

        # MACD Histogram Ã— ADX: ì¶”ì„¸ ê°•ë„ë¥¼ ê³ ë ¤í•œ MACD ì‹ í˜¸
        df['macd_adx'] = df['macd_hist'] * df['adx'] / 100

        # Bollinger Band Position: ê°€ê²©ì´ ë°´ë“œ ë‚´ ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ”ì§€ (0~1)
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-10)

        # Trend Strength: ë‹¨ê¸°/ì¥ê¸° ì´í‰ì„  ë¹„ìœ¨ (ì¶”ì„¸ ë°©í–¥ì„±)
        df['ma_ratio_short'] = df['ma5'] / (df['ma20'] + 1e-10)
        df['ma_ratio_long'] = df['ma20'] / (df['ma50'] + 1e-10)

        # Volatility Ratio: í˜„ì¬ ë³€ë™ì„± / ê³¼ê±° í‰ê·  ë³€ë™ì„±
        df['volatility_ratio'] = df['volatility'] / (df['volatility'].rolling(50).mean() + 1e-10)

        # Price Distance from MA: ê°€ê²©ì´ ì´í‰ì„ ì—ì„œ ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€
        df['price_vs_ma20'] = (df['close'] - df['ma20']) / (df['ma20'] + 1e-10)
        df['price_vs_ma50'] = (df['close'] - df['ma50']) / (df['ma50'] + 1e-10)

        # Momentum Acceleration: RSIì˜ ë³€í™”ìœ¨ (ëª¨ë©˜í…€ì˜ ê°€ì†/ê°ì†)
        df['rsi_momentum'] = df['rsi'].diff()

        # Volume-Price Trend: ê±°ë˜ëŸ‰ê³¼ ê°€ê²© ë³€í™”ì˜ ê´€ê³„
        df['vpt'] = df['volume'] * df['pct_change_1']
        df['vpt_ma'] = df['vpt'].rolling(20).mean()

        # === ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ í†µê³„ (ì£¼ê°„/ì›”ê°„ ìš”ì•½) ===
        # ì£¼ê°„ í†µê³„ (5ì¼ ê¸°ì¤€)
        df['weekly_return'] = df['close'].pct_change(5)  # ì£¼ê°„ ìˆ˜ìµë¥ 
        df['weekly_high'] = df['high'].rolling(5).max()   # ì£¼ê°„ ìµœê³ ê°€
        df['weekly_low'] = df['low'].rolling(5).min()     # ì£¼ê°„ ìµœì €ê°€
        df['weekly_volatility'] = df['close'].rolling(5).std()  # ì£¼ê°„ ë³€ë™ì„±
        df['weekly_volume_avg'] = df['volume'].rolling(5).mean()  # ì£¼ê°„ í‰ê·  ê±°ë˜ëŸ‰

        # ì›”ê°„ í†µê³„ (20ì¼ ê¸°ì¤€)
        df['monthly_return'] = df['close'].pct_change(20)  # ì›”ê°„ ìˆ˜ìµë¥ 
        df['monthly_high'] = df['high'].rolling(20).max()   # ì›”ê°„ ìµœê³ ê°€
        df['monthly_low'] = df['low'].rolling(20).min()     # ì›”ê°„ ìµœì €ê°€
        df['monthly_volatility'] = df['close'].rolling(20).std()  # ì›”ê°„ ë³€ë™ì„±
        df['monthly_volume_avg'] = df['volume'].rolling(20).mean()  # ì›”ê°„ í‰ê·  ê±°ë˜ëŸ‰

        # 3ê°œì›” í†µê³„ (60ì¼ ê¸°ì¤€)
        df['quarterly_return'] = df['close'].pct_change(60)  # ë¶„ê¸° ìˆ˜ìµë¥ 
        df['quarterly_volatility'] = df['close'].rolling(60).std()  # ë¶„ê¸° ë³€ë™ì„±

        # í˜„ì¬ ê°€ê²©ì˜ ì£¼ê°„/ì›”ê°„ ë ˆì¸ì§€ ë‚´ ìœ„ì¹˜ (0~1)
        df['weekly_position'] = (df['close'] - df['weekly_low']) / (df['weekly_high'] - df['weekly_low'] + 1e-10)
        df['monthly_position'] = (df['close'] - df['monthly_low']) / (df['monthly_high'] - df['monthly_low'] + 1e-10)

        # ê±°ë˜ëŸ‰ ë¹„ìœ¨ (í˜„ì¬ ê±°ë˜ëŸ‰ / í‰ê·  ê±°ë˜ëŸ‰)
        df['volume_ratio_weekly'] = df['volume'] / (df['weekly_volume_avg'] + 1e-10)
        df['volume_ratio_monthly'] = df['volume'] / (df['monthly_volume_avg'] + 1e-10)

        # ë³€ë™ì„± ì¶”ì„¸ (ë‹¨ê¸° ë³€ë™ì„± / ì¥ê¸° ë³€ë™ì„±)
        df['volatility_trend'] = df['weekly_volatility'] / (df['monthly_volatility'] + 1e-10)

        return df
    
    def prepare_data(self, prices):
        """ML ëª¨ë¸ìš© ë°ì´í„° ì „ì²˜ë¦¬ - ì´ìƒì¹˜ ì œê±° ì¶”ê°€"""
        # í”¼ì²˜ ìƒì„±
        df = self.create_features(prices)

        # NaN ì œê±°
        df = df.dropna()

        # ì´ìƒì¹˜ ì œê±° (Z-score > 3)
        from scipy import stats
        z_scores = np.abs(stats.zscore(df['close']))
        df = df[(z_scores < 3)]

        if len(df) == 0:
            raise ValueError("ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

        logger.debug(f"ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„° í¬ì¸íŠ¸: {len(df)}ê°œ")
        
        if len(df) < self.sequence_length + 1:
            raise ValueError(f"ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ {self.sequence_length + 1}ê°œ í•„ìš”")
        
        # í”¼ì²˜ ì„ íƒ - ê³ ê¸‰ ì§€í‘œ í¬í•¨
        feature_columns = [
            # ê¸°ì¡´ ì§€í‘œ
            'ma5', 'ma10', 'ma20', 'ma50', 'ma200',
            'rsi', 'macd', 'macd_signal', 'macd_hist',
            'bb_upper', 'bb_lower', 'bb_width',
            'pct_change_1', 'pct_change_5', 'pct_change_10',
            'volatility',
            # ìƒˆë¡œìš´ ê³ ê¸‰ ì§€í‘œ
            'atr', 'adx', 'plus_di', 'minus_di',
            'stoch_k', 'stoch_d',
            'obv_ma', 'williams_r', 'cci', 'roc', 'mfi',
            'tenkan_sen', 'kijun_sen',
            # ë¹„ì„ í˜• ì¡°í•© ì§€í‘œ
            'adx_rsi', 'bb_vol_product', 'rsi_stoch', 'macd_adx',
            'bb_position', 'ma_ratio_short', 'ma_ratio_long',
            'volatility_ratio', 'price_vs_ma20', 'price_vs_ma50',
            'rsi_momentum', 'vpt_ma',
            # ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ í†µê³„
            'weekly_return', 'weekly_volatility', 'weekly_position', 'volume_ratio_weekly',
            'monthly_return', 'monthly_volatility', 'monthly_position', 'volume_ratio_monthly',
            'quarterly_return', 'quarterly_volatility', 'volatility_trend'
        ]
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        # âœ… ìˆ˜ì •: ë‹¤ìŒë‚  ë³€í™”ìœ¨(%)ì„ ì˜ˆì¸¡í•˜ë„ë¡ ë³€ê²½
        X, y = [], []
        for i in range(self.sequence_length, len(df) - 1):  # -1: ë‹¤ìŒë‚  ë°ì´í„°ê°€ ìˆì–´ì•¼ í•¨
            # ê³¼ê±° sequence_lengthê°œì˜ í”¼ì²˜ë“¤
            sequence_features = []
            for j in range(i - self.sequence_length, i):
                row_features = df[feature_columns].iloc[j].values
                sequence_features.extend(row_features)

            X.append(sequence_features)

            # âœ… íƒ€ê²Ÿ: ë‹¤ìŒë‚  ë³€í™”ìœ¨ (%)
            current_price = df['close'].iloc[i]
            next_price = df['close'].iloc[i + 1]
            price_change_pct = (next_price - current_price) / current_price * 100
            y.append(price_change_pct)

        return np.array(X), np.array(y)
    
    def _expanding_window_cv(self, X, y, n_splits=5):
        """
        Expanding Window êµì°¨ê²€ì¦ ìƒì„±ê¸°
        ìµœê·¼ êµ¬ê°„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë‘ëŠ” ë°©ì‹

        Args:
            X: ì…ë ¥ ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            n_splits: ë¶„í•  ê°œìˆ˜

        Yields:
            (train_idx, val_idx) íŠœí”Œ
        """
        n_samples = len(X)
        # ìµœì†Œ í›ˆë ¨ ë°ì´í„° í¬ê¸° (ì „ì²´ì˜ 40%)
        min_train_size = int(n_samples * 0.4)

        # ê²€ì¦ ì„¸íŠ¸ í¬ê¸° (ì „ì²´ì˜ 10%)
        test_size = max(int(n_samples * 0.1), 1)

        for i in range(n_splits):
            # Expanding window: í›ˆë ¨ ì„¸íŠ¸ê°€ ì ì  ì»¤ì§
            split_point = min_train_size + int((n_samples - min_train_size - test_size) * i / (n_splits - 1))
            train_end = split_point
            val_start = split_point
            val_end = min(split_point + test_size, n_samples)

            train_idx = np.arange(0, train_end)
            val_idx = np.arange(val_start, val_end)

            if len(val_idx) > 0:
                yield train_idx, val_idx

    def train_models_with_cv(self, X, y):
        """Expanding Window Cross-Validationì„ ì‚¬ìš©í•œ ëª¨ë¸ í›ˆë ¨ (ìµœê·¼ êµ¬ê°„ ê°€ì¤‘)"""
        if not SKLEARN_AVAILABLE:
            return

        # Expanding Window CV ì‚¬ìš© (ìµœê·¼ ë°ì´í„°ì— ë” ë§ì€ ê°€ì¤‘ì¹˜)
        cv_splits = list(self._expanding_window_cv(X, y, n_splits=5))

        # 1. Random Forest with CV
        if self.progress_callback:
            self.progress_callback('ml', 'Random Forest í•™ìŠµ ì¤‘ (1/3)...')

        rf_scores = []
        for train_idx, val_idx in cv_splits:
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            rf_model = RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1
            )
            rf_model.fit(X_train, y_train)
            rf_pred = rf_model.predict(X_val)
            rf_scores.append(np.sqrt(mean_squared_error(y_val, rf_pred)))

        # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í›ˆë ¨
        rf_final = RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1
        )
        rf_final.fit(X, y)
        self.models['random_forest'] = rf_final
        logger.info(f"Random Forest CV RMSE: {np.mean(rf_scores):.2f} (Â±{np.std(rf_scores):.2f})")

        # ëª¨ë¸ ì €ì¥
        if self.persistence and self.ticker:
            try:
                metadata = {'cv_rmse_mean': np.mean(rf_scores), 'cv_rmse_std': np.std(rf_scores)}
                self.persistence.save_sklearn_model(rf_final, self.ticker, 'random_forest', metadata, self.scaler)
            except Exception as e:
                logger.warning(f"Random Forest ì €ì¥ ì‹¤íŒ¨: {e}")

        # 2. XGBoost with CV
        if XGBOOST_AVAILABLE:
            if self.progress_callback:
                self.progress_callback('ml', 'XGBoost í•™ìŠµ ì¤‘ (2/3)...')

            xgb_scores = []
            for train_idx, val_idx in cv_splits:
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                xgb_model = xgb.XGBRegressor(
                    n_estimators=150,  # 300 â†’ 150 (ì†ë„ 2ë°° í–¥ìƒ)
                    max_depth=8,
                    learning_rate=0.05,  # 0.03 â†’ 0.05 (learning rate ë†’ì—¬ì„œ ë³´ì™„)
                    min_child_weight=3,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    gamma=0.1,
                    reg_alpha=0.1,
                    reg_lambda=1.0,
                    random_state=42,
                    verbosity=0
                )
                xgb_model.fit(X_train, y_train)
                xgb_pred = xgb_model.predict(X_val)
                xgb_scores.append(np.sqrt(mean_squared_error(y_val, xgb_pred)))

            xgb_final = xgb.XGBRegressor(
                n_estimators=150,  # 300 â†’ 150
                max_depth=8,
                learning_rate=0.05,  # 0.03 â†’ 0.05
                min_child_weight=3,
                subsample=0.8,
                colsample_bytree=0.8,
                gamma=0.1,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=42,
                verbosity=0
            )
            xgb_final.fit(X, y)
            self.models['xgboost'] = xgb_final
            logger.info(f"XGBoost CV RMSE: {np.mean(xgb_scores):.2f} (Â±{np.std(xgb_scores):.2f})")

            # ëª¨ë¸ ì €ì¥
            if self.persistence and self.ticker:
                try:
                    metadata = {'cv_rmse_mean': np.mean(xgb_scores), 'cv_rmse_std': np.std(xgb_scores)}
                    self.persistence.save_sklearn_model(xgb_final, self.ticker, 'xgboost', metadata, self.scaler)
                except Exception as e:
                    logger.warning(f"XGBoost ì €ì¥ ì‹¤íŒ¨: {e}")

        # 3. LightGBM with CV
        if LIGHTGBM_AVAILABLE:
            if self.progress_callback:
                self.progress_callback('ml', 'LightGBM í•™ìŠµ ì¤‘ (3/3)...')

            lgb_scores = []
            for train_idx, val_idx in cv_splits:
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                lgb_model = lgb.LGBMRegressor(
                    n_estimators=300,
                    max_depth=8,
                    learning_rate=0.03,
                    num_leaves=31,
                    min_child_samples=20,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=0.1,
                    reg_lambda=1.0,
                    random_state=42,
                    verbosity=-1
                )
                lgb_model.fit(X_train, y_train)
                lgb_pred = lgb_model.predict(X_val)
                lgb_scores.append(np.sqrt(mean_squared_error(y_val, lgb_pred)))

            lgb_final = lgb.LGBMRegressor(
                n_estimators=300,
                max_depth=8,
                learning_rate=0.03,
                num_leaves=31,
                min_child_samples=20,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=42,
                verbosity=-1
            )
            lgb_final.fit(X, y)
            self.models['lightgbm'] = lgb_final
            logger.info(f"LightGBM CV RMSE: {np.mean(lgb_scores):.2f} (Â±{np.std(lgb_scores):.2f})")

            # ëª¨ë¸ ì €ì¥
            if self.persistence and self.ticker:
                try:
                    metadata = {'cv_rmse_mean': np.mean(lgb_scores), 'cv_rmse_std': np.std(lgb_scores)}
                    self.persistence.save_sklearn_model(lgb_final, self.ticker, 'lightgbm', metadata, self.scaler)
                except Exception as e:
                    logger.warning(f"LightGBM ì €ì¥ ì‹¤íŒ¨: {e}")

    def train_models(self, X_train, y_train, X_val, y_val):
        """ì—¬ëŸ¬ ML ëª¨ë¸ í›ˆë ¨ - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜µì…˜"""

        # 1. Random Forest
        if SKLEARN_AVAILABLE:
            if self.use_optimization:
                logger.info("Random Forest Bayesian Optimization ì‹¤í–‰...")
                rf_model = HyperparameterOptimizer.optimize_random_forest(X_train, y_train, n_iter=15)

            if not self.use_optimization or rf_model is None:
                rf_model = RandomForestRegressor(
                    n_estimators=200,
                    max_depth=15,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    max_features='sqrt',
                    random_state=42,
                    n_jobs=-1
                )
                rf_model.fit(X_train, y_train)

            self.models['random_forest'] = rf_model
            rf_pred = rf_model.predict(X_val)
            rf_score = np.sqrt(mean_squared_error(y_val, rf_pred))
            logger.info(f"Random Forest RMSE: {rf_score:.2f}")

        # 2. XGBoost
        if XGBOOST_AVAILABLE:
            if self.use_optimization:
                logger.info("XGBoost Bayesian Optimization ì‹¤í–‰...")
                xgb_model = HyperparameterOptimizer.optimize_xgboost(X_train, y_train, n_iter=15)

            if not self.use_optimization or xgb_model is None:
                xgb_model = xgb.XGBRegressor(
                    n_estimators=300,
                    max_depth=8,
                    learning_rate=0.03,
                    min_child_weight=3,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    gamma=0.1,
                    reg_alpha=0.1,
                    reg_lambda=1.0,
                    random_state=42,
                    verbosity=0
                )
                xgb_model.fit(X_train, y_train)

            self.models['xgboost'] = xgb_model
            xgb_pred = xgb_model.predict(X_val)
            xgb_score = np.sqrt(mean_squared_error(y_val, xgb_pred))
            logger.info(f"XGBoost RMSE: {xgb_score:.2f}")

        # 3. LightGBM
        if LIGHTGBM_AVAILABLE:
            if self.use_optimization:
                logger.info("LightGBM Bayesian Optimization ì‹¤í–‰...")
                lgb_model = HyperparameterOptimizer.optimize_lightgbm(X_train, y_train, n_iter=15)

            if not self.use_optimization or lgb_model is None:
                lgb_model = lgb.LGBMRegressor(
                    n_estimators=300,
                    max_depth=8,
                    learning_rate=0.03,
                    num_leaves=31,
                    min_child_samples=20,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=0.1,
                    reg_lambda=1.0,
                    random_state=42,
                    verbosity=-1
                )
                lgb_model.fit(X_train, y_train)

            self.models['lightgbm'] = lgb_model
            lgb_pred = lgb_model.predict(X_val)
            lgb_score = np.sqrt(mean_squared_error(y_val, lgb_pred))
            logger.info(f"LightGBM RMSE: {lgb_score:.2f}")
    
    def fit_predict(self, prices, forecast_days=5, use_cv=True):
        """ML ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡ - CV ì˜µì…˜ ì¶”ê°€"""
        if len(prices) < self.sequence_length + 20:
            raise ValueError(f"ìµœì†Œ {self.sequence_length + 20}ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤")

        # ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_data(prices)

        # Time Series Cross-Validation ì‚¬ìš© ì—¬ë¶€
        if use_cv and len(X) > 100:
            logger.info("Time Series Cross-Validation ì‚¬ìš©")
            self.train_models_with_cv(X, y)
        else:
            # í›ˆë ¨/ê²€ì¦ ë¶„í• 
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42, shuffle=False
            )

            # ëª¨ë¸ í›ˆë ¨
            self.train_models(X_train, y_train, X_val, y_val)
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        if not self.models:
            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ML ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ìµœê·¼ ì‹œí€€ìŠ¤ë¡œ ë¯¸ë˜ ì˜ˆì¸¡
        last_sequence = X[-1].reshape(1, -1)
        
        # âœ… ëª¨ë¸ ì˜ˆì¸¡: ë³€í™”ìœ¨(%)ì„ ì˜ˆì¸¡
        predictions_pct = {}
        for model_name, model in self.models.items():
            pred_pct = model.predict(last_sequence)[0]  # ë³€í™”ìœ¨(%)
            predictions_pct[model_name] = pred_pct
            logger.info(f"{model_name} ì˜ˆì¸¡ ë³€í™”ìœ¨: {pred_pct:+.2f}%")

        # ì•™ìƒë¸” (í‰ê·  ë³€í™”ìœ¨)
        ensemble_pct = np.mean(list(predictions_pct.values()))
        current_price = prices[-1]

        logger.info(f"í˜„ì¬ê°€: {current_price:.2f}, ì•™ìƒë¸” ì˜ˆì¸¡ ë³€í™”ìœ¨: {ensemble_pct:+.2f}%")

        # âœ… ë³€í™”ìœ¨ì„ ì ˆëŒ€ ê°€ê²©ìœ¼ë¡œ ë³€í™˜
        day1_pred = current_price * (1 + ensemble_pct / 100)

        # âœ… ì¬ê·€ì  ì˜ˆì¸¡: ë§¤ì¼ ê°™ì€ ë³€í™”ìœ¨ ì ìš© (ë‹¨ìˆœí™”)
        future_predictions = []
        future_predictions.append(day1_pred)

        # 2ì¼ì°¨ ì´í›„: ë³€í™”ìœ¨ì„ ë°˜ë³µ ì ìš© (ê°ì‡  ì ìš©)
        for i in range(1, forecast_days):
            # ë³€í™”ìœ¨ ê°ì‡ : ë©€ì–´ì§ˆìˆ˜ë¡ 0%ì— ìˆ˜ë ´
            decay = 0.9 ** i
            adjusted_pct = ensemble_pct * decay
            next_pred = current_price * (1 + adjusted_pct * (i + 1) / 100)
            future_predictions.append(next_pred)

        logger.info(f"ML ì˜ˆì¸¡ ì™„ë£Œ: 1ì¼ì°¨={future_predictions[0]:.2f} ({ensemble_pct:+.2f}%), "
                   f"ìµœì¢…={future_predictions[-1]:.2f} ({(future_predictions[-1]-current_price)/current_price*100:+.2f}%)")
        
        return {
            'future_predictions': np.array(future_predictions),
            'individual_predictions': predictions_pct,  # ë³€í™”ìœ¨ë¡œ ì €ì¥
            'ensemble_prediction_pct': ensemble_pct,  # ë³€í™”ìœ¨(%)
            'model_performance': {
                'models_used': list(self.models.keys()),
                'validation_score': 'calculated_above'
            }
        }

class LSTMPredictor:
    """LSTM ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì£¼ê°€ ì˜ˆì¸¡"""

    def __init__(self, sequence_length=60, units=128, ticker=None, auto_load=True):
        self.sequence_length = sequence_length
        self.units = units
        self.model = None
        self.scaler = MinMaxScaler()
        self.ticker = ticker
        self.persistence = None

        # ëª¨ë¸ ì €ì¥/ë¡œë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            from model_persistence import get_model_persistence
            self.persistence = get_model_persistence()

            # ìë™ ë¡œë“œ (í‹°ì»¤ê°€ ìˆê³ , auto_load=Trueì¸ ê²½ìš°)
            if auto_load and ticker:
                self._try_load_model()
        except ImportError:
            logger.debug("model_persistence ëª¨ë“ˆ ì—†ìŒ, ì €ì¥/ë¡œë“œ ë¹„í™œì„±í™”")

    def _try_load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œë„"""
        if not self.persistence or not self.ticker:
            return

        # TensorFlowê°€ ì—†ìœ¼ë©´ Keras ëª¨ë¸ ìë™ ë¡œë“œë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŒ
        if TENSORFLOW_AVAILABLE is None:
            _lazy_import_tensorflow()
        if not TENSORFLOW_AVAILABLE:
            logger.debug("TensorFlow ì—†ìŒ - LSTM ìë™ ë¡œë“œ ê±´ë„ˆëœ€")
            return False

        # TensorFlowê°€ ì—†ìœ¼ë©´ Keras ëª¨ë¸ ìë™ ë¡œë“œë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŒ
        if TENSORFLOW_AVAILABLE is None:
            _lazy_import_tensorflow()
        if not TENSORFLOW_AVAILABLE:
            logger.debug("TensorFlow ì—†ìŒ - Transformer ìë™ ë¡œë“œ ê±´ë„ˆëœ€")
            return False

        try:
            model, metadata, scaler = self.persistence.load_keras_model(self.ticker, 'lstm')
            if model is not None:
                self.model = model
                if scaler is not None:
                    self.scaler = scaler
                self.loaded_model_meta = metadata if isinstance(metadata, dict) else None
                self.loaded_model_meta = metadata if isinstance(metadata, dict) else None
                version = metadata.get('version', 'unknown') if isinstance(metadata, dict) else 'unknown'
                logger.info(f"âœ… ì €ì¥ëœ LSTM ëª¨ë¸ ë¡œë“œ: {self.ticker} (ë²„ì „: {version})")
                return True
        except Exception as e:
            logger.info(f"LSTM ì €ì¥ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë¶ˆê°€ (ì¬í•™ìŠµ ì˜ˆì •): {e}")

        return False

    def build_model(self, input_shape):
        """LSTM ëª¨ë¸ êµ¬ì¶•"""
        # Lazy Import TensorFlow
        if not _lazy_import_tensorflow():
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤")

        keras = _tensorflow_modules['keras']
        layers = _tensorflow_modules['layers']

        model = keras.Sequential([
            layers.LSTM(self.units, return_sequences=True, input_shape=input_shape),
            layers.Dropout(0.2),
            layers.LSTM(self.units // 2, return_sequences=True),
            layers.Dropout(0.2),
            layers.LSTM(self.units // 4),
            layers.Dropout(0.2),
            layers.Dense(32, activation='relu'),
            layers.Dense(1)
        ])

        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )

        return model

    def prepare_sequences(self, data):
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ì¤€ë¹„"""
        scaled_data = self.scaler.fit_transform(data.reshape(-1, 1))

        X, y = [], []
        for i in range(self.sequence_length, len(scaled_data)):
            X.append(scaled_data[i-self.sequence_length:i, 0])
            y.append(scaled_data[i, 0])

        return np.array(X), np.array(y)

    def fit_predict(self, prices, forecast_days=5, force_retrain=False):
        """LSTM ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"""
        if TENSORFLOW_AVAILABLE is None:
            _lazy_import_tensorflow()
        if not TENSORFLOW_AVAILABLE:
            logger.warning("TensorFlow ì—†ìŒ - LSTM ê±´ë„ˆëœ€")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback'}

        try:
            # ì €ì¥ëœ ëª¨ë¸ì´ ìˆê³  ì¬í›ˆë ¨ ê°•ì œê°€ ì•„ë‹ˆë©´ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰
            if self.model is not None and not force_retrain:
                logger.info("âœ… ê¸°ì¡´ LSTM ëª¨ë¸ ì‚¬ìš© (ì¬í›ˆë ¨ ì—†ìŒ)")
                return self._predict_only(prices, forecast_days)

            logger.info("ì €ì¥ëœ LSTM ëª¨ë¸ ì—†ìŒ ë˜ëŠ” ì¬í›ˆë ¨ ê°•ì œ - í•™ìŠµ ì‹œì‘")
            X, y = self.prepare_sequences(prices)

            if len(X) < 50:
                raise ValueError("LSTM í•™ìŠµì— ì¶©ë¶„í•œ ë°ì´í„° ì—†ìŒ")

            # í•™ìŠµ/ê²€ì¦ ë¶„í• 
            split = int(len(X) * 0.8)
            X_train, X_val = X[:split], X[split:]
            y_train, y_val = y[:split], y[split:]

            # 3D í˜•íƒœë¡œ reshape
            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))

            # ëª¨ë¸ êµ¬ì¶•
            self.model = self.build_model((X_train.shape[1], 1))

            # ì½œë°± ì„¤ì •
            EarlyStopping = _tensorflow_modules['EarlyStopping']
            ReduceLROnPlateau = _tensorflow_modules['ReduceLROnPlateau']
            early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  # patience 10 â†’ 15
            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)

            # í•™ìŠµ
            logger.info("ğŸ”„ LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=150,  # 100 â†’ 150
                batch_size=32,
                callbacks=[early_stop, reduce_lr],
                verbose=0
            )

            # ë¯¸ë˜ ì˜ˆì¸¡
            last_sequence = X[-1].reshape((1, self.sequence_length, 1))
            predictions = []

            for _ in range(forecast_days):
                pred = self.model.predict(last_sequence, verbose=0)[0, 0]
                predictions.append(pred)

                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                last_sequence = np.roll(last_sequence, -1, axis=1)
                last_sequence[0, -1, 0] = pred

            # ì—­ìŠ¤ì¼€ì¼ë§
            predictions = self.scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()

            # ëª¨ë¸ ì €ì¥
            if self.persistence and self.ticker:
                try:
                    metadata = {
                        'train_loss': history.history['loss'][-1],
                        'val_loss': history.history['val_loss'][-1],
                        'epochs_trained': len(history.history['loss']),
                        'sequence_length': self.sequence_length,
                        'units': self.units,
                        'data_size': len(prices)
                    }
                    self.persistence.save_keras_model(self.model, self.ticker, 'lstm', metadata, self.scaler)
                except Exception as e:
                    logger.warning(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

            return {
                'future_predictions': predictions,
                'model_type': 'LSTM',
                'train_loss': history.history['loss'][-1],
                'val_loss': history.history['val_loss'][-1]
            }

        except Exception as e:
            logger.error(f"LSTM ì‹¤íŒ¨: {e}")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback', 'error': str(e)}

    def _predict_only(self, prices, forecast_days):
        """ì €ì¥ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰ (ì¬í›ˆë ¨ ì—†ìŒ)"""
        try:
            X, y = self.prepare_sequences(prices)
            X = X.reshape((X.shape[0], X.shape[1], 1))

            # ë¯¸ë˜ ì˜ˆì¸¡
            last_sequence = X[-1].reshape((1, self.sequence_length, 1))
            predictions = []

            for _ in range(forecast_days):
                pred = self.model.predict(last_sequence, verbose=0)[0, 0]
                predictions.append(pred)

                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                last_sequence = np.roll(last_sequence, -1, axis=1)
                last_sequence[0, -1, 0] = pred

            # ì—­ìŠ¤ì¼€ì¼ë§
            predictions = self.scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()

            return {
                'future_predictions': predictions,
                'model_type': 'LSTM',
                'using_cached_model': True
            }

        except Exception as e:
            logger.error(f"LSTM ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback', 'error': str(e)}


class TransformerPredictor:
    """Transformer ëª¨ë¸ì„ ì‚¬ìš©í•œ ì£¼ê°€ ì˜ˆì¸¡"""

    def __init__(self, sequence_length=60, d_model=64, num_heads=4, num_layers=2, ticker=None, auto_load=True):
        self.sequence_length = sequence_length
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.model = None
        self.scaler = MinMaxScaler()
        self.ticker = ticker
        self.persistence = None

        # ëª¨ë¸ ì €ì¥/ë¡œë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            from model_persistence import get_model_persistence
            self.persistence = get_model_persistence()

            # ìë™ ë¡œë“œ
            if auto_load and ticker:
                self._try_load_model()
        except ImportError:
            logger.debug("model_persistence ëª¨ë“ˆ ì—†ìŒ, ì €ì¥/ë¡œë“œ ë¹„í™œì„±í™”")

    def _try_load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œë„"""
        if not self.persistence or not self.ticker:
            return

        # TensorFlowê°€ ì—†ìœ¼ë©´ Keras ëª¨ë¸ ìë™ ë¡œë“œë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŒ
        if TENSORFLOW_AVAILABLE is None:
            _lazy_import_tensorflow()
        if not TENSORFLOW_AVAILABLE:
            logger.debug("TensorFlow ì—†ìŒ - Transformer ìë™ ë¡œë“œ ê±´ë„ˆëœ€")
            return False

        try:
            model, metadata, scaler = self.persistence.load_keras_model(self.ticker, 'transformer')
            if model is not None:
                self.model = model
                if scaler is not None:
                    self.scaler = scaler
                version = metadata.get('version', 'unknown') if isinstance(metadata, dict) else 'unknown'
                logger.info(f"âœ… ì €ì¥ëœ Transformer ëª¨ë¸ ë¡œë“œ: {self.ticker} (ë²„ì „: {version})")
                return True
        except Exception as e:
            logger.info(f"Transformer ì €ì¥ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë¶ˆê°€ (ì¬í•™ìŠµ ì˜ˆì •): {e}")

        return False

    def transformer_encoder(self, inputs, head_size, num_heads, ff_dim, dropout=0.1):
        """Transformer Encoder Block"""
        layers = _tensorflow_modules['layers']

        # Multi-Head Attention
        x = layers.MultiHeadAttention(
            key_dim=head_size, num_heads=num_heads, dropout=dropout
        )(inputs, inputs)
        x = layers.Dropout(dropout)(x)
        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)

        # Feed Forward
        ff = layers.Dense(ff_dim, activation="relu")(x)
        ff = layers.Dropout(dropout)(ff)
        ff = layers.Dense(inputs.shape[-1])(ff)
        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)

        return x

    def build_model(self, input_shape):
        """Transformer ëª¨ë¸ êµ¬ì¶•"""
        # Lazy Import TensorFlow
        if not _lazy_import_tensorflow():
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤")

        keras = _tensorflow_modules['keras']
        layers = _tensorflow_modules['layers']

        inputs = keras.Input(shape=input_shape)

        # Positional Encoding
        x = layers.Dense(self.d_model)(inputs)

        # Transformer Encoder Blocks
        for _ in range(self.num_layers):
            x = self.transformer_encoder(
                x,
                head_size=self.d_model // self.num_heads,
                num_heads=self.num_heads,
                ff_dim=self.d_model * 4,
                dropout=0.1
            )

        # Output layers
        x = layers.GlobalAveragePooling1D()(x)
        x = layers.Dropout(0.1)(x)
        x = layers.Dense(64, activation="relu")(x)
        x = layers.Dropout(0.1)(x)
        outputs = layers.Dense(1)(x)

        model = keras.Model(inputs=inputs, outputs=outputs)
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )

        return model

    def prepare_sequences(self, data):
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ì¤€ë¹„"""
        scaled_data = self.scaler.fit_transform(data.reshape(-1, 1))

        X, y = [], []
        for i in range(self.sequence_length, len(scaled_data)):
            X.append(scaled_data[i-self.sequence_length:i, 0])
            y.append(scaled_data[i, 0])

        return np.array(X), np.array(y)

    def fit_predict(self, prices, forecast_days=5, force_retrain=False):
        """Transformer ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"""
        if not TENSORFLOW_AVAILABLE:
            logger.warning("TensorFlow ì—†ìŒ - Transformer ê±´ë„ˆëœ€")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback'}

        try:
            # ì €ì¥ëœ ëª¨ë¸ì´ ìˆê³  ì¬í›ˆë ¨ ê°•ì œê°€ ì•„ë‹ˆë©´ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰
            if self.model is not None and not force_retrain:
                logger.info("âœ… ê¸°ì¡´ Transformer ëª¨ë¸ ì‚¬ìš© (ì¬í›ˆë ¨ ì—†ìŒ)")
                return self._predict_only(prices, forecast_days)

            logger.info("ì €ì¥ëœ Transformer ëª¨ë¸ ì—†ìŒ ë˜ëŠ” ì¬í›ˆë ¨ ê°•ì œ - í•™ìŠµ ì‹œì‘")
            X, y = self.prepare_sequences(prices)

            if len(X) < 50:
                raise ValueError("Transformer í•™ìŠµì— ì¶©ë¶„í•œ ë°ì´í„° ì—†ìŒ")

            # í•™ìŠµ/ê²€ì¦ ë¶„í• 
            split = int(len(X) * 0.8)
            X_train, X_val = X[:split], X[split:]
            y_train, y_val = y[:split], y[split:]

            # 3D í˜•íƒœë¡œ reshape
            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))

            # ëª¨ë¸ êµ¬ì¶•
            self.model = self.build_model((X_train.shape[1], 1))

            # ì½œë°± ì„¤ì •
            EarlyStopping = _tensorflow_modules['EarlyStopping']
            ReduceLROnPlateau = _tensorflow_modules['ReduceLROnPlateau']
            early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)  # patience 15 â†’ 20
            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)

            # í•™ìŠµ
            logger.info("ğŸ”„ Transformer ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=150,  # 100 â†’ 150
                batch_size=32,
                callbacks=[early_stop, reduce_lr],
                verbose=0
            )

            # ë¯¸ë˜ ì˜ˆì¸¡
            last_sequence = X[-1].reshape((1, self.sequence_length, 1))
            predictions = []

            for _ in range(forecast_days):
                pred = self.model.predict(last_sequence, verbose=0)[0, 0]
                predictions.append(pred)

                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                last_sequence = np.roll(last_sequence, -1, axis=1)
                last_sequence[0, -1, 0] = pred

            # ì—­ìŠ¤ì¼€ì¼ë§
            predictions = self.scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()

            # ëª¨ë¸ ì €ì¥
            if self.persistence and self.ticker:
                try:
                    metadata = {
                        'train_loss': history.history['loss'][-1],
                        'val_loss': history.history['val_loss'][-1],
                        'epochs_trained': len(history.history['loss']),
                        'sequence_length': self.sequence_length,
                        'd_model': self.d_model,
                        'num_heads': self.num_heads,
                        'num_layers': self.num_layers,
                        'data_size': len(prices)
                    }
                    self.persistence.save_keras_model(self.model, self.ticker, 'transformer', metadata, self.scaler)
                except Exception as e:
                    logger.warning(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

            return {
                'future_predictions': predictions,
                'model_type': 'Transformer',
                'train_loss': history.history['loss'][-1],
                'val_loss': history.history['val_loss'][-1]
            }

        except Exception as e:
            logger.error(f"Transformer ì‹¤íŒ¨: {e}")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback', 'error': str(e)}

    def _predict_only(self, prices, forecast_days):
        """ì €ì¥ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰ (ì¬í›ˆë ¨ ì—†ìŒ)"""
        try:
            X, y = self.prepare_sequences(prices)
            X = X.reshape((X.shape[0], X.shape[1], 1))

            # ë¯¸ë˜ ì˜ˆì¸¡
            last_sequence = X[-1].reshape((1, self.sequence_length, 1))
            predictions = []

            for _ in range(forecast_days):
                pred = self.model.predict(last_sequence, verbose=0)[0, 0]
                predictions.append(pred)

                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                last_sequence = np.roll(last_sequence, -1, axis=1)
                last_sequence[0, -1, 0] = pred

            # ì—­ìŠ¤ì¼€ì¼ë§
            predictions = self.scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()

            return {
                'future_predictions': predictions,
                'model_type': 'Transformer',
                'using_cached_model': True
            }

        except Exception as e:
            logger.error(f"Transformer ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback', 'error': str(e)}


class ARIMAPredictor:
    """ARIMA ëª¨ë¸ì„ ì‚¬ìš©í•œ ì£¼ê°€ ì˜ˆì¸¡"""
    
    def __init__(self, order=(1,1,1)):
        self.order = order
        self.model = None
    
    def fit_predict(self, prices, forecast_days=5):
        """ARIMA ëª¨ë¸ í”¼íŒ… ë° ì˜ˆì¸¡"""
        if not STATSMODELS_AVAILABLE:
            # âœ… ARIMA ëŒ€ì²´: ì´ë™í‰ê·  ê¸°ë°˜ ì¶”ì„¸
            ma_window = min(10, len(prices) // 2)
            if ma_window < 2:
                ma_window = 2
            trend = np.mean(np.diff(prices[-ma_window:]))
            last_price = prices[-1]

            future_predictions = []
            for i in range(forecast_days):
                future_pred = last_price + trend * (i + 1)
                future_predictions.append(future_pred)

            return {
                'future_predictions': np.array(future_predictions),
                'method': 'moving_average_fallback',
                'trend': trend
            }
        
        try:
            # ARIMA ëª¨ë¸ í”¼íŒ…
            model = ARIMA(prices, order=self.order)
            self.model = model.fit()
            
            # ì˜ˆì¸¡
            forecast_result = self.model.forecast(steps=forecast_days)
            confidence_intervals = self.model.get_forecast(steps=forecast_days).conf_int()
            
            return {
                'future_predictions': forecast_result.values if hasattr(forecast_result, 'values') else forecast_result,
                'confidence_intervals': confidence_intervals.values if hasattr(confidence_intervals, 'values') else confidence_intervals,
                'aic': self.model.aic,
                'bic': self.model.bic
            }
        
        except Exception as e:
            logger.error(f"ARIMA ëª¨ë¸ ì˜¤ë¥˜: {e}")
            # âœ… ARIMA ì‹¤íŒ¨ì‹œ ì„ í˜• ì¶”ì„¸ ì™¸ì‚½
            trend = np.mean(np.diff(prices[-10:]))
            last_price = prices[-1]

            future_predictions = []
            for i in range(forecast_days):
                future_pred = last_price + trend * (i + 1)
                future_predictions.append(future_pred)

            return {
                'future_predictions': np.array(future_predictions),
                'method': 'linear_trend_fallback',
                'error': str(e)
            }

class MarketCorrelationAnalyzer:
    """ì‹œì¥ ì§€ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„"""

    @staticmethod
    def get_market_indices(ticker):
        """ì¢…ëª©ì— ë§ëŠ” ì‹œì¥ ì§€ìˆ˜ ì„ íƒ"""
        if ticker.endswith('.KS') or ticker.endswith('.KQ'):
            # í•œêµ­ ì¢…ëª©
            return {
                'KOSPI': '^KS11',
                'KOSDAQ': '^KQ11'
            }
        else:
            # ë¯¸êµ­ ì¢…ëª©
            return {
                'S&P500': '^GSPC',
                'Nasdaq': '^IXIC',
                'Dow': '^DJI'
            }

    @staticmethod
    def calculate_correlation(stock_symbol, period='1y'):
        """ì‹œì¥ ì§€ìˆ˜ì™€ ìƒê´€ê³„ìˆ˜ ê³„ì‚°"""
        try:
            # ì¢…ëª© ë°ì´í„°
            stock_data = get_stock_data(stock_symbol, period=period)
            if stock_data is None or len(stock_data) < 30:
                return {}

            # ì§€ìˆ˜ ë°ì´í„°
            indices = MarketCorrelationAnalyzer.get_market_indices(stock_symbol)
            correlations = {}

            for index_name, index_symbol in indices.items():
                try:
                    index_data = get_stock_data(index_symbol, period=period)
                    if index_data is not None and len(index_data) >= 30:
                        # ë‚ ì§œ ë§ì¶”ê¸°
                        merged = stock_data['Close'].to_frame().join(
                            index_data['Close'].to_frame(),
                            how='inner',
                            rsuffix='_index'
                        )

                        if len(merged) >= 30:
                            corr = merged.iloc[:, 0].corr(merged.iloc[:, 1])
                            correlations[index_name] = corr
                except Exception as e:
                    logger.warning(f"{index_name} ìƒê´€ê´€ê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")

            return correlations

        except Exception as e:
            logger.error(f"ì‹œì¥ ìƒê´€ê´€ê³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}


class SectorAnalyzer:
    """ì„¹í„°/ì‚°ì—… ë™í–¥ ë¶„ì„"""

    SECTOR_ETFS = {
        'Technology': 'XLK',
        'Healthcare': 'XLV',
        'Financial': 'XLF',
        'Energy': 'XLE',
        'Consumer Discretionary': 'XLY',
        'Consumer Staples': 'XLP',
        'Industrials': 'XLI',
        'Materials': 'XLB',
        'Real Estate': 'XLRE',
        'Utilities': 'XLU',
        'Communication': 'XLC'
    }

    @staticmethod
    def get_sector_performance(period='1mo'):
        """ì„¹í„°ë³„ ì„±ê³¼ ë¶„ì„"""
        sector_performance = {}

        for sector, etf in SectorAnalyzer.SECTOR_ETFS.items():
            try:
                data = get_stock_data(etf, period=period)
                if data is not None and len(data) >= 2:
                    start_price = data['Close'].iloc[0]
                    end_price = data['Close'].iloc[-1]
                    performance = ((end_price - start_price) / start_price) * 100
                    sector_performance[sector] = performance
            except Exception as e:
                logger.warning(f"{sector} ì„±ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")

        return sector_performance

    @staticmethod
    def compare_with_sector(stock_symbol, sector_etf, period='1y'):
        """ì¢…ëª©ê³¼ ì„¹í„° ETF ë¹„êµ"""
        try:
            stock_data = get_stock_data(stock_symbol, period=period)
            sector_data = get_stock_data(sector_etf, period=period)

            if stock_data is None or sector_data is None:
                return None

            # ìƒëŒ€ ì„±ê³¼
            stock_return = ((stock_data['Close'].iloc[-1] - stock_data['Close'].iloc[0])
                           / stock_data['Close'].iloc[0]) * 100
            sector_return = ((sector_data['Close'].iloc[-1] - sector_data['Close'].iloc[0])
                            / sector_data['Close'].iloc[0]) * 100

            return {
                'stock_return': stock_return,
                'sector_return': sector_return,
                'outperformance': stock_return - sector_return
            }
        except Exception as e:
            logger.error(f"ì„¹í„° ë¹„êµ ì˜¤ë¥˜: {e}")
            return None


class InstitutionalFlowAnalyzer:
    """ì™¸êµ­ì¸/ê¸°ê´€ ë§¤ë§¤ ë™í–¥ ë¶„ì„ (í•œêµ­ ì¢…ëª©)"""

    @staticmethod
    def is_korean_stock(ticker):
        """í•œêµ­ ì¢…ëª© ì—¬ë¶€ í™•ì¸"""
        return ticker.endswith('.KS') or ticker.endswith('.KQ')

    @staticmethod
    def fetch_institutional_data(ticker):
        """ì™¸êµ­ì¸/ê¸°ê´€ ë§¤ë§¤ ë°ì´í„° ìˆ˜ì§‘"""
        if not InstitutionalFlowAnalyzer.is_korean_stock(ticker):
            return None

        try:
            # FinanceDataReader ì‚¬ìš© (ì„¤ì¹˜ í•„ìš”: pip install finance-datareader)
            import FinanceDataReader as fdr
            from datetime import datetime, timedelta

            # ìµœê·¼ 60ì¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì—¬ìœ ìˆê²Œ 30ì¼ ì´ìƒ í™•ë³´)
            end_date = datetime.now()
            start_date = end_date - timedelta(days=60)

            df = fdr.DataReader(ticker, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))

            if df is not None and len(df) > 0 and 'ForeignBuy' in df.columns and 'InstitutionBuy' in df.columns:
                # ìµœê·¼ 30ì˜ì—…ì¼ ë°ì´í„° ì‚¬ìš©
                recent_30d = df.tail(30)

                return {
                    'foreign_net_buy': recent_30d['ForeignBuy'].sum() - recent_30d['ForeignSell'].sum(),
                    'institution_net_buy': recent_30d['InstitutionBuy'].sum() - recent_30d['InstitutionSell'].sum(),
                    'foreign_ownership': recent_30d['ForeignOwnership'].iloc[-1] if 'ForeignOwnership' in df.columns else None
                }
            else:
                return None

        except ImportError:
            logger.warning("FinanceDataReader ì„¤ì¹˜ í•„ìš”: pip install finance-datareader")
            return None
        except Exception as e:
            logger.error(f"ì™¸êµ­ì¸/ê¸°ê´€ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return None


class MarketRegimeDetector:
    """ì‹œì¥ ìƒí™©(ìƒìŠ¹ì¥/í•˜ë½ì¥/íš¡ë³´ì¥) ê°ì§€ ì‹œìŠ¤í…œ - VIX, ê¸ˆë¦¬ ë“± ì™¸ë¶€ ì§€í‘œ í¬í•¨"""

    @staticmethod
    def detect_regime(prices, window=50, use_external_indicators=True):
        """
        ì‹œì¥ ìƒí™© ê°ì§€ (ê¸°ë³¸ ë²„ì „ + ì™¸ë¶€ ì§€í‘œ)
        Returns: 'bull' (ìƒìŠ¹ì¥), 'bear' (í•˜ë½ì¥), 'sideways' (íš¡ë³´ì¥), 'high_volatility' (ê³ ë³€ë™ì„±)
        """
        if len(prices) < window:
            return 'sideways'

        recent_prices = prices[-window:]

        # ì¶”ì„¸ ë¶„ì„
        trend = np.polyfit(range(len(recent_prices)), recent_prices, 1)[0]
        avg_price = np.mean(recent_prices)
        trend_pct = (trend / avg_price) * 100

        # ë³€ë™ì„± ë¶„ì„
        volatility = np.std(recent_prices) / np.mean(recent_prices)

        # ìƒìŠ¹/í•˜ë½ ì¼ìˆ˜ ë¹„ìœ¨
        price_changes = np.diff(recent_prices)
        up_days = np.sum(price_changes > 0)
        down_days = np.sum(price_changes < 0)
        up_ratio = up_days / len(price_changes) if len(price_changes) > 0 else 0.5

        # ê¸°ë³¸ ì‹œì¥ ìƒí™© íŒë‹¨
        regime_score = 0  # -1: bear, 0: sideways, 1: bull

        # ì¶”ì„¸ ì ìˆ˜
        if trend_pct > 0.5 and up_ratio > 0.55:
            regime_score += 1
        elif trend_pct < -0.5 and up_ratio < 0.45:
            regime_score -= 1

        # ì™¸ë¶€ ì§€í‘œ ì¶”ê°€ (VIX, SP500, Treasury ë“±)
        if use_external_indicators:
            try:
                external_score = MarketRegimeDetector._get_external_regime_score()
                regime_score += external_score
                logger.debug(f"External regime score: {external_score}")
            except Exception as e:
                logger.debug(f"External indicators not available: {e}")

        # ìµœì¢… ë ˆì§ ê²°ì •
        if volatility > 0.05:  # 5% ì´ìƒ ë³€ë™ì„±
            regime = 'high_volatility'
        elif regime_score >= 1:
            regime = 'bull'
        elif regime_score <= -1:
            regime = 'bear'
        else:
            regime = 'sideways'

        logger.info(f"ì‹œì¥ ìƒí™©: {regime} (ì¶”ì„¸: {trend_pct:.2f}%, ë³€ë™ì„±: {volatility:.2%}, ìƒìŠ¹ë¹„ìœ¨: {up_ratio:.1%})")

        return regime

    @staticmethod
    def _get_external_regime_score():
        """
        ì™¸ë¶€ ì‹œì¥ ì§€í‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë ˆì§ ì ìˆ˜ ê³„ì‚°
        Returns: -1 (bearish), 0 (neutral), 1 (bullish)
        """
        score = 0

        try:
            # VIX ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë³€ë™ì„± ì§€ìˆ˜)
            vix_data = MarketDataFetcher.fetch_macro_indicator('vix', period='3mo', interval='1d')
            if vix_data is not None and not vix_data.empty:
                recent_vix = vix_data['Close'].iloc[-1]
                avg_vix = vix_data['Close'].mean()

                # VIXê°€ í‰ê· ë³´ë‹¤ ë‚®ìœ¼ë©´ ì•ˆì •ì  (bullish), ë†’ìœ¼ë©´ ë¶ˆì•ˆì • (bearish)
                if recent_vix < avg_vix * 0.9:
                    score += 0.5  # VIX ë‚®ìŒ -> ê¸ì •ì 
                elif recent_vix > avg_vix * 1.2:
                    score -= 0.5  # VIX ë†’ìŒ -> ë¶€ì •ì 

                logger.debug(f"VIX: {recent_vix:.2f} (avg: {avg_vix:.2f})")

            # S&P 500 ì¶”ì„¸ í™•ì¸
            sp500_data = MarketDataFetcher.fetch_macro_indicator('sp500', period='3mo', interval='1d')
            if sp500_data is not None and not sp500_data.empty:
                sp500_prices = sp500_data['Close'].values
                sp500_return = (sp500_prices[-1] - sp500_prices[0]) / sp500_prices[0]

                # S&P 500 3ê°œì›” ìˆ˜ìµë¥  ê¸°ì¤€
                if sp500_return > 0.05:  # 5% ì´ìƒ ìƒìŠ¹
                    score += 0.5
                elif sp500_return < -0.05:  # 5% ì´ìƒ í•˜ë½
                    score -= 0.5

                logger.debug(f"S&P 500 3-month return: {sp500_return*100:.2f}%")

            # 10ë…„ êµ­ì±„ ìˆ˜ìµë¥  ì¶”ì„¸
            treasury_data = MarketDataFetcher.fetch_macro_indicator('treasury_10y', period='3mo', interval='1d')
            if treasury_data is not None and not treasury_data.empty:
                recent_yield = treasury_data['Close'].iloc[-1]
                past_yield = treasury_data['Close'].iloc[0] if len(treasury_data) > 0 else recent_yield
                yield_change = recent_yield - past_yield

                # ê¸ˆë¦¬ ìƒìŠ¹ì€ ì£¼ì‹ì— ë¶€ì •ì , í•˜ë½ì€ ê¸ì •ì  (ë‹¨ìˆœí™”)
                if yield_change < -0.2:  # 0.2%p ì´ìƒ í•˜ë½
                    score += 0.3
                elif yield_change > 0.3:  # 0.3%p ì´ìƒ ìƒìŠ¹
                    score -= 0.3

                logger.debug(f"10Y Treasury yield change: {yield_change:.2f}%")

            # ê¸ˆë¦¬ ìŠ¤í”„ë ˆë“œ (10Y-2Y): ê²½ê¸° ì¹¨ì²´ ì˜ˆì¸¡ ì§€í‘œ
            treasury_10y = MarketDataFetcher.fetch_macro_indicator('treasury_10y', period='1mo', interval='1d')
            treasury_2y = MarketDataFetcher.fetch_macro_indicator('treasury_2y', period='1mo', interval='1d')

            if treasury_10y is not None and treasury_2y is not None and not treasury_10y.empty and not treasury_2y.empty:
                yield_10y = treasury_10y['Close'].iloc[-1]
                yield_2y = treasury_2y['Close'].iloc[-1]
                yield_spread = yield_10y - yield_2y

                # ìˆ˜ìµë¥  ê³¡ì„  ì—­ì „ (10Y < 2Y): ê²½ê¸° ì¹¨ì²´ ì‹ í˜¸ -> ë§¤ìš° ë¶€ì •ì 
                if yield_spread < -0.1:  # ì—­ì „ (inversion)
                    score -= 0.7  # ê°•í•œ bearish ì‹ í˜¸
                    logger.debug(f"âš ï¸ Yield curve inverted: {yield_spread:.2f}%")
                elif yield_spread < 0.3:  # í‰íƒ„í™” (flattening)
                    score -= 0.3  # ì•½í•œ bearish ì‹ í˜¸
                    logger.debug(f"Yield curve flattening: {yield_spread:.2f}%")
                elif yield_spread > 1.5:  # ê°€íŒŒë¥¸ ê³¡ì„  (steepening) - ê²½ê¸° íšŒë³µ
                    score += 0.4  # bullish ì‹ í˜¸
                    logger.debug(f"Yield curve steepening: {yield_spread:.2f}%")
                else:
                    logger.debug(f"Yield spread normal: {yield_spread:.2f}%")

        except Exception as e:
            logger.debug(f"Error fetching external indicators: {e}")
            return 0

        # ì ìˆ˜ë¥¼ -1, 0, 1ë¡œ ì •ê·œí™”
        if score >= 0.5:
            return 1
        elif score <= -0.5:
            return -1
        else:
            return 0

    @staticmethod
    def get_regime_weights(regime):
        """ì‹œì¥ ìƒí™©ë³„ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë°˜í™˜"""
        weights = {
            'bull': {  # ìƒìŠ¹ì¥: íŠ¸ë Œë“œ ì¶”ì¢… ëª¨ë¸ ê°•í™”
                'kalman': 0.20,
                'ml_models': 0.40,
                'arima': 0.15,
                'lstm': 0.15,
                'transformer': 0.10
            },
            'bear': {  # í•˜ë½ì¥: ì•ˆì •ì ì¸ ëª¨ë¸ ê°•í™”
                'kalman': 0.30,
                'ml_models': 0.25,
                'arima': 0.25,
                'lstm': 0.10,
                'transformer': 0.10
            },
            'sideways': {  # íš¡ë³´ì¥: ê· í˜•ì¡íŒ ê°€ì¤‘ì¹˜
                'kalman': 0.20,
                'ml_models': 0.25,
                'arima': 0.25,
                'lstm': 0.15,
                'transformer': 0.10
            },
            'high_volatility': {  # ê³ ë³€ë™ì„±: ë³´ìˆ˜ì  ëª¨ë¸ ê°•í™”
                'kalman': 0.35,
                'ml_models': 0.20,
                'arima': 0.30,
                'lstm': 0.10,
                'transformer': 0.05
            }
        }

        return weights.get(regime, weights['sideways'])


class PredictionErrorAnalyzer:
    """ì˜ˆì¸¡ ì˜¤ë¥˜ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„± í´ë˜ìŠ¤"""

    def __init__(self):
        self.error_history = []
        self.feature_importance_cache = {}

    def log_prediction_error(self, ticker, model_name, predicted_value, actual_value,
                            market_regime, features=None, feature_names=None):
        """
        ì˜ˆì¸¡ ì˜¤ë¥˜ ë¡œê¹…

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            model_name: ëª¨ë¸ ì´ë¦„
            predicted_value: ì˜ˆì¸¡ê°’
            actual_value: ì‹¤ì œê°’
            market_regime: ì‹œì¥ ìƒí™©
            features: ì‚¬ìš©ëœ í”¼ì²˜ ê°’ë“¤
            feature_names: í”¼ì²˜ ì´ë¦„ë“¤
        """
        error_entry = {
            'timestamp': datetime.now(),
            'ticker': ticker,
            'model': model_name,
            'predicted': predicted_value,
            'actual': actual_value,
            'error': abs(predicted_value - actual_value),
            'error_pct': abs((predicted_value - actual_value) / actual_value * 100) if actual_value != 0 else 0,
            'direction_correct': np.sign(predicted_value) == np.sign(actual_value),
            'market_regime': market_regime,
            'features': features,
            'feature_names': feature_names
        }

        self.error_history.append(error_entry)

        # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
        if len(self.error_history) > 1000:
            self.error_history = self.error_history[-1000:]

    def generate_error_report(self, ticker=None, lookback_days=30):
        """
        ì—ëŸ¬ ì–´íŠ¸ë¦¬ë·°ì…˜ ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            ticker: íŠ¹ì • ì¢…ëª© (Noneì´ë©´ ì „ì²´)
            lookback_days: ë¶„ì„ ê¸°ê°„ (ì¼)

        Returns:
            Dict with error analysis
        """
        if len(self.error_history) == 0:
            return {"error": "No error history available"}

        # ê¸°ê°„ í•„í„°ë§
        cutoff_time = datetime.now() - timedelta(days=lookback_days)
        filtered_errors = [e for e in self.error_history
                          if e['timestamp'] > cutoff_time]

        if ticker:
            filtered_errors = [e for e in filtered_errors if e['ticker'] == ticker]

        if len(filtered_errors) == 0:
            return {"error": f"No errors found for ticker {ticker} in last {lookback_days} days"}

        # ëª¨ë¸ë³„ ì˜¤ë¥˜ ë¶„ì„
        model_stats = {}
        for error in filtered_errors:
            model = error['model']
            if model not in model_stats:
                model_stats[model] = {
                    'count': 0,
                    'total_error': 0,
                    'total_error_pct': 0,
                    'direction_correct_count': 0,
                    'regime_errors': {}
                }

            stats = model_stats[model]
            stats['count'] += 1
            stats['total_error'] += error['error']
            stats['total_error_pct'] += error['error_pct']
            if error['direction_correct']:
                stats['direction_correct_count'] += 1

            # ë ˆì§ë³„ ì˜¤ë¥˜
            regime = error['market_regime']
            if regime not in stats['regime_errors']:
                stats['regime_errors'][regime] = {'count': 0, 'total_error': 0}
            stats['regime_errors'][regime]['count'] += 1
            stats['regime_errors'][regime]['total_error'] += error['error']

        # í†µê³„ ê³„ì‚°
        for model, stats in model_stats.items():
            stats['avg_error'] = stats['total_error'] / stats['count']
            stats['avg_error_pct'] = stats['total_error_pct'] / stats['count']
            stats['direction_accuracy'] = stats['direction_correct_count'] / stats['count'] * 100

            # ë ˆì§ë³„ í‰ê·  ì˜¤ë¥˜
            for regime, regime_stats in stats['regime_errors'].items():
                regime_stats['avg_error'] = regime_stats['total_error'] / regime_stats['count']

        report = {
            'period': f'Last {lookback_days} days',
            'ticker': ticker or 'All',
            'total_predictions': len(filtered_errors),
            'model_statistics': model_stats,
            'worst_performing_model': max(model_stats.items(),
                                         key=lambda x: x[1]['avg_error'])[0] if model_stats else None,
            'best_performing_model': min(model_stats.items(),
                                        key=lambda x: x[1]['avg_error'])[0] if model_stats else None
        }

        return report

    def save_error_report(self, ticker=None, lookback_days=30, filepath=None):
        """ì—ëŸ¬ ë¦¬í¬íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
        import json
        report = self.generate_error_report(ticker, lookback_days)

        if 'error' in report:
            logger.warning(f"Error Report: {report['error']}")
            return None

        if filepath is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            ticker_str = ticker or 'all'
            filepath = f"error_reports/error_report_{ticker_str}_{timestamp}.json"

        # ë””ë ‰í† ë¦¬ ìƒì„±
        import os
        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else 'error_reports', exist_ok=True)

        # JSONìœ¼ë¡œ ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, default=str)

        logger.info(f"ğŸ“Š ì—ëŸ¬ ì–´íŠ¸ë¦¬ë·°ì…˜ ë¦¬í¬íŠ¸ ì €ì¥: {filepath}")
        return filepath

    def print_error_report(self, ticker=None, lookback_days=30):
        """ì—ëŸ¬ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        report = self.generate_error_report(ticker, lookback_days)

        if 'error' in report:
            logger.info(f"Error Report: {report['error']}")
            return

        logger.info(f"\n{'='*60}")
        logger.info(f"PREDICTION ERROR ANALYSIS REPORT")
        logger.info(f"{'='*60}")
        logger.info(f"Period: {report['period']}")
        logger.info(f"Ticker: {report['ticker']}")
        logger.info(f"Total Predictions: {report['total_predictions']}")
        logger.info(f"\nBest Model: {report['best_performing_model']}")
        logger.info(f"Worst Model: {report['worst_performing_model']}")

        logger.info(f"\n{'Model Performance Details':-^60}")
        for model, stats in report['model_statistics'].items():
            logger.info(f"\n{model.upper()}:")
            logger.info(f"  Predictions: {stats['count']}")
            logger.info(f"  Avg Error: {stats['avg_error']:.2f}")
            logger.info(f"  Avg Error %: {stats['avg_error_pct']:.2f}%")
            logger.info(f"  Direction Accuracy: {stats['direction_accuracy']:.1f}%")

            if stats['regime_errors']:
                logger.info(f"  Regime Performance:")
                for regime, regime_stats in stats['regime_errors'].items():
                    logger.info(f"    {regime}: {regime_stats['avg_error']:.2f} (n={regime_stats['count']})")

        logger.info(f"\n{'='*60}\n")


class EnsemblePredictor:
    """ì—¬ëŸ¬ ëª¨ë¸ì„ ê²°í•©í•œ ì•™ìƒë¸” ì˜ˆì¸¡ê¸° - ë™ì  ê°€ì¤‘ì¹˜ + ì‹œì¥ ìƒí™© ì¸ì‹"""

    def __init__(self, use_deep_learning=False, use_optimization=False, ticker=None):
        self.ticker = ticker
        self.kalman = KalmanFilterPredictor()
        self.ml_predictor = AdvancedMLPredictor(use_optimization=use_optimization, ticker=ticker) if (SKLEARN_AVAILABLE or XGBOOST_AVAILABLE or LIGHTGBM_AVAILABLE) else None
        self.arima = ARIMAPredictor()

        # ë”¥ëŸ¬ë‹ ëª¨ë¸ (ì˜µì…˜)
        # Initialize without hard-gating on TensorFlow; defer check to runtime
        self.use_deep_learning = bool(use_deep_learning)
        if self.use_deep_learning:
            self.lstm = LSTMPredictor(ticker=ticker)
            self.transformer = TransformerPredictor(ticker=ticker)
        else:
            self.lstm = None
            self.transformer = None

        # ì´ˆê¸° ê°€ì¤‘ì¹˜ (ë™ì ìœ¼ë¡œ ì¡°ì •ë¨)
        self.weights = {
            'kalman': 0.25,
            'ml_models': 0.40 if self.ml_predictor else 0,
            'arima': 0.25 if self.ml_predictor else 0.75,
            'lstm': 0.05 if self.use_deep_learning else 0,
            'transformer': 0.05 if self.use_deep_learning else 0
        }

        # ëª¨ë¸ ì„±ëŠ¥ ì¶”ì 
        self.performance_history = {
            'kalman': [],
            'ml_models': [],
            'arima': [],
            'lstm': [],
            'transformer': []
        }

        # âœ… ì§„í–‰ ìƒíƒœ ì½œë°±
        self.progress_callback = None

        # ì‹œì¥ ìƒí™©
        self.current_regime = 'sideways'

        # ì—ëŸ¬ ë¶„ì„ê¸°
        self.error_analyzer = PredictionErrorAnalyzer()

        # ğŸš€ Enhanced Trading System - ìƒˆë¡œìš´ ê¸°ëŠ¥ (ì¡°ìš©íˆ ì¶”ê°€)
        if ENHANCED_REGIME_AVAILABLE:
            self.enhanced_regime_detector = EnhancedRegimeDetector(use_ml=False)
            self.weight_optimizer = EnsembleWeightOptimizer(method='adaptive')
            self.use_enhanced_regime = True
            logger.debug("Enhanced regime detection & weight optimization í™œì„±í™”")
        else:
            self.enhanced_regime_detector = None
            self.weight_optimizer = None
            self.use_enhanced_regime = False

        # Brier Score ì¶”ì  (ë”¥ëŸ¬ë‹ ëª¨ë¸ìš©)
        self.lstm_brier_history = []
        self.transformer_brier_history = []

    def update_weights_dynamically(self, validation_errors, validation_predictions=None, validation_targets=None):
        """
        ê²€ì¦ ì˜¤ë¥˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì • (ê³ ë„í™”: Sharpe Ratio ì¶”ê°€)

        Args:
            validation_errors: ê° ëª¨ë¸ì˜ MAE ì˜¤ë¥˜
            validation_predictions: ê° ëª¨ë¸ì˜ ê²€ì¦ ì˜ˆì¸¡ê°’ (ë°©í–¥ì„± ë° Sharpe Ratio í‰ê°€ìš©)
            validation_targets: ê²€ì¦ íƒ€ê²Ÿ ê°’ (ë°©í–¥ì„± ë° Sharpe Ratio í‰ê°€ìš©)
        """
        # 1. ì—­ì˜¤ë¥˜ ì ìˆ˜ (MAE ê¸°ë°˜)
        inverse_error_scores = {}
        for model_name, error in validation_errors.items():
            if error > 0:
                inverse_error_scores[model_name] = 1.0 / error

        # 2. ë°©í–¥ì„± ì ìˆ˜ (ì˜ˆì¸¡ ë°©í–¥ì´ ì‹¤ì œì™€ ì¼ì¹˜í•˜ëŠ”ì§€)
        direction_scores = {}
        if validation_predictions and validation_targets is not None:
            for model_name, predictions in validation_predictions.items():
                if len(predictions) > 0 and len(validation_targets) > 0:
                    # ë°©í–¥ ì¼ì¹˜ìœ¨ ê³„ì‚°
                    pred_directions = np.sign(np.diff(predictions))
                    target_directions = np.sign(np.diff(validation_targets))

                    if len(pred_directions) > 0:
                        direction_match = np.sum(pred_directions == target_directions) / len(pred_directions)
                        direction_scores[model_name] = direction_match
                    else:
                        direction_scores[model_name] = 0.5  # ì¤‘ë¦½
                else:
                    direction_scores[model_name] = 0.5  # ì¤‘ë¦½

        # 2-1. Sharpe Ratio ì ìˆ˜ (ìœ„í—˜ ëŒ€ë¹„ ìˆ˜ìµë¥ )
        sharpe_scores = {}
        if validation_predictions and validation_targets is not None:
            try:
                # validation_targets ê¸¸ì´ ì²´í¬ (NumPy ë°°ì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
                targets_len = len(validation_targets)
            except:
                targets_len = 0

            if targets_len > 0:
                for model_name, predictions in validation_predictions.items():
                    if len(predictions) > 0:
                        try:
                            # ì˜ˆì¸¡ ê¸°ë°˜ ìˆ˜ìµë¥  ê³„ì‚°
                            pred_returns = np.diff(predictions) / predictions[:-1]

                            # Sharpe Ratio = (í‰ê·  ìˆ˜ìµë¥  - ë¬´ìœ„í—˜ ìˆ˜ìµë¥ ) / ìˆ˜ìµë¥  í‘œì¤€í¸ì°¨
                            # ê°„ë‹¨íˆ ë¬´ìœ„í—˜ ìˆ˜ìµë¥ ì€ 0ìœ¼ë¡œ ê°€ì •
                            mean_return = np.mean(pred_returns)
                            std_return = np.std(pred_returns)

                            if std_return > 0:
                                sharpe = mean_return / std_return
                                # Sharpeë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™” (sigmoid ì ìš©)
                                sharpe_scores[model_name] = 1 / (1 + np.exp(-sharpe))
                            else:
                                sharpe_scores[model_name] = 0.5  # ë³€ë™ì„± ì—†ìŒ
                        except Exception as e:
                            logger.debug(f"Sharpe ratio ê³„ì‚° ì‹¤íŒ¨ ({model_name}): {e}")
                            sharpe_scores[model_name] = 0.5
                    else:
                        sharpe_scores[model_name] = 0.5

        # 3. ì„±ëŠ¥ ì´ë ¥ ì¶”ì  (ìµœê·¼ ì„±ëŠ¥ì— ë” ë§ì€ ê°€ì¤‘ì¹˜)
        for model_name, error in validation_errors.items():
            self.performance_history[model_name].append(error)
            # ìµœê·¼ 10ê°œë§Œ ìœ ì§€
            if len(self.performance_history[model_name]) > 10:
                self.performance_history[model_name] = self.performance_history[model_name][-10:]

        # 4. ìµœê·¼ ì„±ëŠ¥ ì¶”ì„¸ ì ìˆ˜
        trend_scores = {}
        for model_name, history in self.performance_history.items():
            if len(history) >= 2:
                # ìµœê·¼ ì˜¤ë¥˜ê°€ ê°ì†Œ ì¶”ì„¸ë©´ ë†’ì€ ì ìˆ˜
                recent_avg = np.mean(history[-3:]) if len(history) >= 3 else history[-1]
                older_avg = np.mean(history[-6:-3]) if len(history) >= 6 else np.mean(history[:-3])

                # ì˜¤ë¥˜ê°€ ê°ì†Œí•˜ë©´ ì ìˆ˜ í–¥ìƒ
                if older_avg > 0:
                    trend_scores[model_name] = max(0.5, 1.0 - (recent_avg / older_avg))
                else:
                    trend_scores[model_name] = 0.5
            else:
                trend_scores[model_name] = 0.5  # ì¤‘ë¦½

        # 5. ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        # - MAE ì—­ìˆ˜: 40%
        # - ë°©í–¥ì„±: 25%
        # - Sharpe Ratio: 20%
        # - ì¶”ì„¸: 15%
        combined_scores = {}
        for model_name in self.weights.keys():
            score = 0.0

            # MAE ì—­ìˆ˜ ì ìˆ˜ (ì •ê·œí™”)
            if model_name in inverse_error_scores:
                total_inverse = sum(inverse_error_scores.values())
                if total_inverse > 0:
                    score += 0.40 * (inverse_error_scores[model_name] / total_inverse)

            # ë°©í–¥ì„± ì ìˆ˜
            if model_name in direction_scores:
                score += 0.25 * direction_scores[model_name]

            # Sharpe Ratio ì ìˆ˜
            if model_name in sharpe_scores:
                score += 0.20 * sharpe_scores[model_name]

            # ì¶”ì„¸ ì ìˆ˜
            if model_name in trend_scores:
                score += 0.15 * trend_scores[model_name]

            combined_scores[model_name] = score

        # 6. ì •ê·œí™”í•˜ì—¬ ìµœì¢… ê°€ì¤‘ì¹˜ ê³„ì‚°
        total_score = sum(combined_scores.values())
        if total_score > 0:
            for model_name in self.weights.keys():
                self.weights[model_name] = combined_scores[model_name] / total_score

        logger.info(f"ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸: {self.weights}")
        if direction_scores:
            logger.debug(f"ë°©í–¥ì„± ì¼ì¹˜ìœ¨: {direction_scores}")
        if sharpe_scores:
            logger.debug(f"Sharpe Ratio ì ìˆ˜: {sharpe_scores}")
        if trend_scores:
            logger.debug(f"ì„±ëŠ¥ ì¶”ì„¸ ì ìˆ˜: {trend_scores}")
    
    def fit_predict(self, prices, forecast_days=5):
        """ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰ - ë™ì  ê°€ì¤‘ì¹˜ + ì‹œì¥ ìƒí™© ì¸ì‹"""
        results = {}
        predictions = []
        validation_errors = {}
        validation_predictions = {}  # âœ… ê²€ì¦ ì˜ˆì¸¡ê°’ ì €ì¥ (Sharpe Ratioìš©)

        logger.info("ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘...")

        # ğŸš€ ì‹œì¥ ìƒí™© ê°ì§€ (Enhanced ë²„ì „ ì‚¬ìš© ê°€ëŠ¥ ì‹œ)
        regime_features = {}  # í”¼ì²˜ ì €ì¥ìš©
        if self.use_enhanced_regime and self.enhanced_regime_detector:
            try:
                # Enhanced regime detection with more features
                market_data = fetch_market_data()
                regime, regime_probs, regime_features = self.enhanced_regime_detector.detect_regime(
                    prices, volumes=None, market_data=market_data, window=50
                )
                self.current_regime = regime
                logger.info(f"Enhanced ë ˆì§ ê°ì§€: {regime} (í™•ë¥ : {regime_probs})")
                logger.debug(f"ì£¼ìš” í”¼ì²˜: volatility={regime_features.get('volatility', 0):.3f}, "
                           f"trend={regime_features.get('trend_pct', 0):.2f}%")
            except Exception as e:
                logger.warning(f"Enhanced regime detection ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
                self.current_regime = MarketRegimeDetector.detect_regime(prices)
        else:
            # ê¸°ì¡´ ë°©ì‹
            self.current_regime = MarketRegimeDetector.detect_regime(prices)

        regime_weights = MarketRegimeDetector.get_regime_weights(self.current_regime)

        # ê²€ì¦ ì„¸íŠ¸ ë¶„ë¦¬ (ë§ˆì§€ë§‰ 10% ì‚¬ìš©)
        split_point = int(len(prices) * 0.9)
        train_prices = prices[:split_point]
        val_prices = prices[split_point:]

        # 1. Kalman Filter ì˜ˆì¸¡
        if self.progress_callback:
            self.progress_callback('kalman', 'Kalman Filter ì˜ˆì¸¡ ì¤‘...')
        logger.debug("Kalman Filter ì‹¤í–‰ ì¤‘...")
        try:
            kalman_result = self.kalman.fit_predict(train_prices, len(val_prices))
            results['kalman'] = self.kalman.fit_predict(prices, forecast_days)
            predictions.append(results['kalman']['future_predictions'])

            # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
            kalman_preds = results['kalman']['future_predictions']
            logger.info(f"Kalman ì˜ˆì¸¡: 1ì¼ì°¨={kalman_preds[0]:.2f} ({(kalman_preds[0]-prices[-1])/prices[-1]*100:+.2f}%), "
                       f"ìµœì¢…={kalman_preds[-1]:.2f} ({(kalman_preds[-1]-prices[-1])/prices[-1]*100:+.2f}%)")

            # ê²€ì¦ ì˜¤ë¥˜ ë° ì˜ˆì¸¡ê°’ ì €ì¥
            kalman_val_preds = kalman_result['future_predictions'][:len(val_prices)]
            kalman_val_error = np.mean(np.abs(kalman_val_preds - val_prices))
            validation_errors['kalman'] = kalman_val_error
            validation_predictions['kalman'] = kalman_val_preds
            logger.debug(f"Kalman ê²€ì¦ MAE: {kalman_val_error:.2f}")
        except Exception as e:
            logger.warning(f"Kalman ì‹¤íŒ¨: {e}")
            validation_errors['kalman'] = float('inf')

        # 2. ML ëª¨ë¸ ì˜ˆì¸¡ (XGBoost, LightGBM, Random Forest)
        if self.ml_predictor and len(prices) >= 50:
            if self.progress_callback:
                self.progress_callback('ml', 'ML ëª¨ë¸ ê²€ì¦ ë°ì´í„° í•™ìŠµ ì¤‘...')

            # ML ì˜ˆì¸¡ê¸°ì— ì§„í–‰ ì½œë°± ì „ë‹¬
            if hasattr(self.ml_predictor, 'progress_callback'):
                self.ml_predictor.progress_callback = self.progress_callback

            logger.debug("ML ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘...")
            try:
                ml_val_result = self.ml_predictor.fit_predict(train_prices, len(val_prices))

                if self.progress_callback:
                    self.progress_callback('ml', 'ML ëª¨ë¸ ì „ì²´ ë°ì´í„° í•™ìŠµ ì¤‘...')

                ml_result = self.ml_predictor.fit_predict(prices, forecast_days)
                results['ml_models'] = ml_result
                predictions.append(ml_result['future_predictions'])

                # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
                ml_preds = ml_result['future_predictions']
                logger.info(f"ML ì•™ìƒë¸” ì˜ˆì¸¡: 1ì¼ì°¨={ml_preds[0]:.2f} ({(ml_preds[0]-prices[-1])/prices[-1]*100:+.2f}%), "
                           f"ìµœì¢…={ml_preds[-1]:.2f} ({(ml_preds[-1]-prices[-1])/prices[-1]*100:+.2f}%)")

                # ê²€ì¦ ì˜¤ë¥˜ ë° ì˜ˆì¸¡ê°’ ì €ì¥
                ml_val_preds = ml_val_result['future_predictions'][:len(val_prices)]
                ml_val_error = np.mean(np.abs(ml_val_preds - val_prices))
                validation_errors['ml_models'] = ml_val_error
                validation_predictions['ml_models'] = ml_val_preds
                logger.debug(f"ML ê²€ì¦ MAE: {ml_val_error:.2f}")
            except Exception as e:
                logger.warning(f"ML ëª¨ë¸ ì‹¤íŒ¨: {e}")
                validation_errors['ml_models'] = float('inf')

        # 3. ARIMA ì˜ˆì¸¡
        if self.progress_callback:
            self.progress_callback('arima', 'ARIMA ëª¨ë¸ ì‹¤í–‰ ì¤‘...')
        logger.debug("ARIMA ëª¨ë¸ í”¼íŒ… ì¤‘...")
        try:
            arima_val_result = self.arima.fit_predict(train_prices, len(val_prices))
            arima_result = self.arima.fit_predict(prices, forecast_days)
            results['arima'] = arima_result
            predictions.append(arima_result['future_predictions'])

            # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
            arima_preds = arima_result['future_predictions']
            logger.info(f"ARIMA ì˜ˆì¸¡: 1ì¼ì°¨={arima_preds[0]:.2f} ({(arima_preds[0]-prices[-1])/prices[-1]*100:+.2f}%), "
                       f"ìµœì¢…={arima_preds[-1]:.2f} ({(arima_preds[-1]-prices[-1])/prices[-1]*100:+.2f}%)")

            # ê²€ì¦ ì˜¤ë¥˜ ë° ì˜ˆì¸¡ê°’ ì €ì¥
            arima_val_preds = arima_val_result['future_predictions'][:len(val_prices)]
            arima_val_error = np.mean(np.abs(arima_val_preds - val_prices))
            validation_errors['arima'] = arima_val_error
            validation_predictions['arima'] = arima_val_preds
            logger.debug(f"ARIMA ê²€ì¦ MAE: {arima_val_error:.2f}")
        except Exception as e:
            logger.warning(f"ARIMA ì‹¤íŒ¨: {e}")
            validation_errors['arima'] = float('inf')

        # 4. LSTM ì˜ˆì¸¡ (ë”¥ëŸ¬ë‹ ëª¨ë“œ)
        if self.use_deep_learning and self.lstm and len(prices) >= 100:
            if self.progress_callback:
                self.progress_callback('lstm', 'LSTM ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì¤‘...')
            logger.debug("LSTM ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            try:
                lstm_result = self.lstm.fit_predict(prices, forecast_days)
                if 'error' not in lstm_result:
                    results['lstm'] = lstm_result
                    predictions.append(lstm_result['future_predictions'])

                    # ê²€ì¦ ì˜¤ë¥˜ (val_loss ì‚¬ìš©)
                    validation_errors['lstm'] = lstm_result.get('val_loss', float('inf'))
                    logger.debug(f"LSTM ê²€ì¦ Loss: {validation_errors['lstm']:.4f}")
            except Exception as e:
                logger.warning(f"LSTM ì‹¤íŒ¨: {e}")
                validation_errors['lstm'] = float('inf')

        # 5. Transformer ì˜ˆì¸¡ (ë”¥ëŸ¬ë‹ ëª¨ë“œ)
        if self.use_deep_learning and self.transformer and len(prices) >= 100:
            if self.progress_callback:
                self.progress_callback('transformer', 'Transformer ëª¨ë¸ í›ˆë ¨ ì¤‘...')
            logger.debug("Transformer ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            try:
                transformer_result = self.transformer.fit_predict(prices, forecast_days)
                if 'error' not in transformer_result:
                    results['transformer'] = transformer_result
                    predictions.append(transformer_result['future_predictions'])

                    # ê²€ì¦ ì˜¤ë¥˜ (val_loss ì‚¬ìš©)
                    validation_errors['transformer'] = transformer_result.get('val_loss', float('inf'))
                    logger.debug(f"Transformer ê²€ì¦ Loss: {validation_errors['transformer']:.4f}")
            except Exception as e:
                logger.warning(f"Transformer ì‹¤íŒ¨: {e}")
                validation_errors['transformer'] = float('inf')

        # 6. ê°€ì¤‘ì¹˜ ê²°ì •: ì‹œì¥ ìƒí™© ê¸°ë°˜ + ë™ì  ì¡°ì •
        if self.progress_callback:
            self.progress_callback('ensemble', 'ëª¨ë¸ ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘...')

        valid_errors = {k: v for k, v in validation_errors.items() if v != float('inf')}
        if valid_errors:
            # ì„±ëŠ¥ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ (ê²€ì¦ ì˜ˆì¸¡ê°’ í¬í•¨)
            valid_predictions = {k: v for k, v in validation_predictions.items() if k in valid_errors}
            self.update_weights_dynamically(valid_errors, valid_predictions, val_prices)

            # ğŸš€ Enhanced Weight Optimization (ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ë§Œ ì ìš©)
            if self.use_enhanced_regime and self.weight_optimizer and self.use_deep_learning:
                if 'lstm' in valid_errors and 'transformer' in valid_errors:
                    try:
                        # Brier Score ê³„ì‚° (ë”ë¯¸ - ì‹¤ì œë¡œëŠ” í™•ë¥  ì˜ˆì¸¡ í•„ìš”)
                        lstm_brier = valid_errors['lstm'] / np.mean(prices[-30:])  # ì •ê·œí™”
                        transformer_brier = valid_errors['transformer'] / np.mean(prices[-30:])

                        # ë³€ë™ì„± ê³„ì‚° (regime_featuresì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì§ì ‘ ê³„ì‚°)
                        volatility = regime_features.get('volatility', np.std(prices[-50:]) / np.mean(prices[-50:]))

                        # Enhanced ê°€ì¤‘ì¹˜ ê³„ì‚°
                        w_lstm, w_transformer = self.weight_optimizer.get_weights(
                            regime=self.current_regime,
                            volatility=volatility,
                            lstm_brier=lstm_brier,
                            transformer_brier=transformer_brier
                        )

                        # ì „ì²´ ê°€ì¤‘ì¹˜ì—ì„œ LSTM/Transformer ë¹„ìœ¨ ì¡°ì •
                        dl_total_weight = self.weights['lstm'] + self.weights['transformer']
                        if dl_total_weight > 0:
                            self.weights['lstm'] = dl_total_weight * w_lstm
                            self.weights['transformer'] = dl_total_weight * w_transformer
                            logger.info(f"âœ¨ Enhanced ê°€ì¤‘ì¹˜ ì ìš©: LSTM={w_lstm:.3f}, Transformer={w_transformer:.3f}")
                    except Exception as e:
                        logger.debug(f"Enhanced weight optimization ì‹¤íŒ¨: {e}")

            # ì‹œì¥ ìƒí™© ê°€ì¤‘ì¹˜ì™€ í˜¼í•© (70% ì„±ëŠ¥, 30% ì‹œì¥ìƒí™©)
            for model_name in self.weights.keys():
                if model_name in regime_weights:
                    self.weights[model_name] = (
                        0.7 * self.weights[model_name] +
                        0.3 * regime_weights[model_name]
                    )

            logger.info(f"ìµœì¢… ê°€ì¤‘ì¹˜ (ì‹œì¥ìƒí™©: {self.current_regime}): {self.weights}")

        # 7. ì•™ìƒë¸” ê²°í•©
        if self.progress_callback:
            self.progress_callback('ensemble', 'ìµœì¢… ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„± ì¤‘...')

        if len(predictions) > 1:
            # ê°€ì¤‘í‰ê·  ê³„ì‚°
            weighted_predictions = np.zeros(forecast_days)
            total_weight = 0

            model_order = ['kalman', 'ml_models', 'arima', 'lstm', 'transformer']
            active_models = [m for m in model_order if m in results]

            for i, model_name in enumerate(active_models):
                if self.weights.get(model_name, 0) > 0:
                    weighted_predictions += predictions[i] * self.weights[model_name]
                    total_weight += self.weights[model_name]

            ensemble_predictions = weighted_predictions / total_weight if total_weight > 0 else predictions[0]
        else:
            ensemble_predictions = predictions[0]

        # ìµœì¢… ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
        logger.info(f"âœ… ìµœì¢… ì•™ìƒë¸” ì˜ˆì¸¡: 1ì¼ì°¨={ensemble_predictions[0]:.2f} ({(ensemble_predictions[0]-prices[-1])/prices[-1]*100:+.2f}%), "
                   f"ìµœì¢…ì¼={ensemble_predictions[-1]:.2f} ({(ensemble_predictions[-1]-prices[-1])/prices[-1]*100:+.2f}%)")

        # âœ… ì‹ ë¢°ë„ ê³„ì‚°: ê²€ì¦ ì˜¤ë¥˜ ê¸°ë°˜ (ë‚®ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ)
        if len(valid_errors) > 0:
            # í‰ê·  ê²€ì¦ ì˜¤ë¥˜ë¥¼ ì‹ ë¢°ë„ë¡œ ë³€í™˜
            avg_error = np.mean(list(valid_errors.values()))
            avg_price = np.mean(prices[-30:])  # ìµœê·¼ 30ì¼ í‰ê· ê°€

            # ì˜¤ë¥˜ìœ¨ ê³„ì‚° (ì˜¤ë¥˜ / í‰ê· ê°€)
            error_rate = avg_error / avg_price if avg_price > 0 else 1.0

            # ì‹ ë¢°ë„: ì˜¤ë¥˜ìœ¨ì´ 5% ë¯¸ë§Œì´ë©´ ë†’ìŒ, 20% ì´ìƒì´ë©´ ë‚®ìŒ
            if error_rate < 0.05:
                confidence_score = 0.8 + (0.05 - error_rate) / 0.05 * 0.2  # 0.8~1.0
            elif error_rate < 0.20:
                confidence_score = 0.5 + (0.20 - error_rate) / 0.15 * 0.3  # 0.5~0.8
            else:
                confidence_score = max(0.3, 0.5 - (error_rate - 0.20) * 0.5)  # 0.3~0.5

            logger.info(f"ê²€ì¦ ì˜¤ë¥˜ìœ¨: {error_rate*100:.2f}%, ì‹ ë¢°ë„: {confidence_score*100:.1f}%")
        else:
            confidence_score = 0.5
            logger.warning("ê²€ì¦ ì˜¤ë¥˜ ì •ë³´ ì—†ìŒ, ê¸°ë³¸ ì‹ ë¢°ë„ 50% ì ìš©")

        # âœ… ë³€ë™ì„± ê¸°ë°˜ ì˜ˆì¸¡ ë²”ìœ„ ê³„ì‚° (ì‹ ë¢°êµ¬ê°„)
        # ìµœê·¼ 60ì¼ ë³€ë™ì„± (í‘œì¤€í¸ì°¨) ì‚¬ìš©
        recent_volatility = np.std(prices[-60:]) if len(prices) >= 60 else np.std(prices[-20:])

        # ëª¨ë¸ ì˜ˆì¸¡ ë¶„ì‚° (ì—¬ëŸ¬ ëª¨ë¸ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´)
        model_variance = np.std(predictions, axis=0) if len(predictions) > 1 else np.zeros(forecast_days)

        # ì¢…í•© ë¶ˆí™•ì‹¤ì„±: ì—­ì‚¬ì  ë³€ë™ì„± + ëª¨ë¸ ë¶ˆì¼ì¹˜ + ì˜ˆì¸¡ ê¸°ê°„ì— ë”°ë¥¸ ì¦ê°€
        prediction_uncertainty = np.zeros(forecast_days)
        for day in range(forecast_days):
            # ê¸°ë³¸ ë¶ˆí™•ì‹¤ì„± = ì—­ì‚¬ì  ë³€ë™ì„±
            base_uncertainty = recent_volatility

            # ëª¨ë¸ ë¶ˆì¼ì¹˜ë„
            model_disagreement = model_variance[day] if len(predictions) > 1 else recent_volatility * 0.3

            # ì˜ˆì¸¡ ê¸°ê°„ì— ë”°ë¥¸ ë¶ˆí™•ì‹¤ì„± ì¦ê°€ (ì œê³±ê·¼ìœ¼ë¡œ ì¦ê°€)
            time_factor = np.sqrt(day + 1) / np.sqrt(forecast_days)

            # ì‹ ë¢°ë„ì— ë”°ë¥¸ ì¡°ì • (ì‹ ë¢°ë„ ë‚®ìœ¼ë©´ ë¶ˆí™•ì‹¤ì„± ì¦ê°€)
            confidence_factor = 2.0 - confidence_score  # 0.5 ~ 2.0

            # ìµœì¢… ë¶ˆí™•ì‹¤ì„±
            prediction_uncertainty[day] = (base_uncertainty * 0.5 + model_disagreement * 0.5) * time_factor * confidence_factor

        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (68% = 1Ïƒ, 95% = 2Ïƒ)
        confidence_intervals = {
            '68%': {  # 1 í‘œì¤€í¸ì°¨ (ì•½ 68% ì‹ ë¢°êµ¬ê°„)
                'lower': ensemble_predictions - prediction_uncertainty,
                'upper': ensemble_predictions + prediction_uncertainty
            },
            '95%': {  # 2 í‘œì¤€í¸ì°¨ (ì•½ 95% ì‹ ë¢°êµ¬ê°„)
                'lower': ensemble_predictions - 2 * prediction_uncertainty,
                'upper': ensemble_predictions + 2 * prediction_uncertainty
            }
        }

        # ì˜ˆì¸¡ ë²”ìœ„ ë¡œê·¸ ì¶œë ¥
        logger.info(f"ğŸ“Š ì˜ˆì¸¡ ë²”ìœ„ (1ì¼ì°¨):")
        logger.info(f"   68% ì‹ ë¢°êµ¬ê°„: ${confidence_intervals['68%']['lower'][0]:.2f} ~ ${confidence_intervals['68%']['upper'][0]:.2f}")
        logger.info(f"   95% ì‹ ë¢°êµ¬ê°„: ${confidence_intervals['95%']['lower'][0]:.2f} ~ ${confidence_intervals['95%']['upper'][0]:.2f}")
        logger.info(f"   ì˜ˆì¸¡ê°’: ${ensemble_predictions[0]:.2f}")

        return {
            'ensemble_predictions': ensemble_predictions,
            'individual_results': results,
            'confidence_score': confidence_score,
            'model_weights': self.weights,
            'prediction_variance': np.var(predictions, axis=0) if len(predictions) > 1 else np.zeros(forecast_days),
            'validation_errors': validation_errors,
            'market_regime': self.current_regime,
            'training_samples': len(train_prices),
            # ìƒˆë¡œ ì¶”ê°€ëœ ì˜ˆì¸¡ ë²”ìœ„ ì •ë³´
            'confidence_intervals': confidence_intervals,
            'prediction_uncertainty': prediction_uncertainty,
            'recent_volatility': recent_volatility
        }

class StockPredictor:
    """í†µí•© ì£¼ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - ë”¥ëŸ¬ë‹ + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì§€ì›"""

    def __init__(self, use_deep_learning=False, use_optimization=False, ticker=None):
        """
        Args:
            use_deep_learning: LSTM, Transformer ì‚¬ìš© ì—¬ë¶€
            use_optimization: Bayesian Optimization ì‚¬ìš© ì—¬ë¶€
            ticker: ì£¼ì‹ í‹°ì»¤ (ëª¨ë¸ ì €ì¥/ë¡œë“œìš©)
        """
        self.ticker = ticker
        self.ensemble = EnsemblePredictor(
            use_deep_learning=use_deep_learning,
            use_optimization=use_optimization,
            ticker=ticker
        )
        self.use_deep_learning = use_deep_learning
        self.use_optimization = use_optimization
        self.progress_callback = None  # âœ… ì§„í–‰ ìƒíƒœ ì½œë°±

    def set_progress_callback(self, callback):
        """ì§„í–‰ ìƒíƒœ ì½œë°± ì„¤ì •"""
        self.progress_callback = callback
        if self.ensemble:
            self.ensemble.progress_callback = callback
    
    def get_stock_data(self, symbol, period=None, force_refresh=False):
        """
        ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° - ë™ì  ê¸°ê°„ ì„¤ì •

        Args:
            symbol: ì£¼ì‹ í‹°ì»¤
            period: ê¸°ê°„ (Noneì´ë©´ ìë™ ê²°ì •)
            force_refresh: ìºì‹œ ë¬´ì‹œí•˜ê³  ìµœì‹  ë°ì´í„° ê°•ì œ ë¡œë“œ (ëª¨ë¸ ì¬í•™ìŠµ ì‹œ ê¶Œì¥)
        """
        try:
            # ê¸°ê°„ ìë™ ê²°ì •
            if period is None:
                try:
                    from optimal_period_config import get_optimal_training_period
                    period = get_optimal_training_period(symbol)
                    logger.info(f"ğŸ“… {symbol} ìµœì  í›ˆë ¨ ê¸°ê°„: {period}")
                except ImportError:
                    period = "3y"  # ê¸°ë³¸ê°’: 3ë…„ (2y â†’ 3y ê°œì„ )
                    logger.debug(f"ê¸°ë³¸ í›ˆë ¨ ê¸°ê°„ ì‚¬ìš©: {period}")

            data = get_stock_data(symbol, period=period, force_refresh=force_refresh)
            if force_refresh:
                logger.info(f"ğŸ”„ {symbol} ìµœì‹  ë°ì´í„° ê°•ì œ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ")
            return data
        except Exception as e:
            logger.error(f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def predict_stock_price(self, symbol, forecast_days=5, show_plot=True):
        """ì¢…ëª©ì˜ ë¯¸ë˜ ì£¼ê°€ ì˜ˆì¸¡ - ì‹œì¥ ë¶„ì„ ì¶”ê°€"""
        logger.info(f"{symbol} ì£¼ê°€ ì˜ˆì¸¡ ì‹œì‘...")

        # 1. ë°ì´í„° ìˆ˜ì§‘
        if self.progress_callback:
            self.progress_callback('data', 'ë°ì´í„° ìˆ˜ì§‘ ì¤‘...')

        data = self.get_stock_data(symbol)
        if data is None or len(data) < 50:
            return {"error": "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

        prices = data['Close'].values
        dates = data.index

        logger.info(f"ë¶„ì„ ê¸°ê°„: {dates[0].strftime('%Y-%m-%d')} ~ {dates[-1].strftime('%Y-%m-%d')}")
        logger.info(f"ë°ì´í„° í¬ì¸íŠ¸: {len(prices)}ê°œ")

        # 1-1. ì‹œì¥ ì§€ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„
        if self.progress_callback:
            self.progress_callback('market_analysis', 'ì‹œì¥ ë¶„ì„ ì¤‘...')

        logger.info("ì‹œì¥ ì§€ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...")
        market_correlations = MarketCorrelationAnalyzer.calculate_correlation(symbol)

        # 1-2. ì„¹í„° ì„±ê³¼ ë¶„ì„ (ë¯¸êµ­ ì¢…ëª©ë§Œ)
        sector_info = None
        if not (symbol.endswith('.KS') or symbol.endswith('.KQ')):
            logger.info("ì„¹í„° ì„±ê³¼ ë¶„ì„ ì¤‘...")
            sector_performance = SectorAnalyzer.get_sector_performance()
            sector_info = sector_performance

        # 1-3. ì™¸êµ­ì¸/ê¸°ê´€ ë§¤ë§¤ ë™í–¥ (í•œêµ­ ì¢…ëª©ë§Œ)
        institutional_flow = None
        if InstitutionalFlowAnalyzer.is_korean_stock(symbol):
            logger.info("ì™¸êµ­ì¸/ê¸°ê´€ ë§¤ë§¤ ë™í–¥ ë¶„ì„ ì¤‘...")
            institutional_flow = InstitutionalFlowAnalyzer.fetch_institutional_data(symbol)

        # 2. ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰
        if self.progress_callback:
            self.progress_callback('ensemble', 'ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...')

        result = self.ensemble.fit_predict(prices, forecast_days)

        # ì§„í–‰ ìƒíƒœ: ê²°ê³¼ ì •ë¦¬ ì¤‘
        if self.progress_callback:
            self.progress_callback('complete', 'ê²°ê³¼ ì •ë¦¬ ì¤‘...')

        # 3. ê²°ê³¼ ì •ë¦¬
        last_price = prices[-1]
        predicted_prices = result['ensemble_predictions']

        # ì˜ˆì¸¡ ì •í™•ë„ ì¶”ì •
        confidence = result['confidence_score']
        
        # ë¯¸ë˜ ë‚ ì§œ ìƒì„± (ì˜ì—…ì¼ ê¸°ì¤€)
        future_dates = pd.bdate_range(start=dates[-1] + pd.Timedelta(days=1), 
                                     periods=forecast_days)
        
        # ìˆ˜ìµë¥  ê³„ì‚°
        returns = [(pred / last_price - 1) * 100 for pred in predicted_prices]
        
        # 4. ê²°ê³¼ ë°˜í™˜ (ì‹œì¥ ë¶„ì„ ì •ë³´ ì¶”ê°€)
        prediction_result = {
            'symbol': symbol,
            'current_price': last_price,
            'predicted_prices': predicted_prices,
            'future_dates': future_dates,
            'expected_returns': returns,
            'confidence_score': confidence,
            'model_weights': result['model_weights'],
            'recommendation': self._generate_recommendation(returns, confidence),
            'models_used': self._get_models_used(),
            # ìƒˆë¡œìš´ ì‹œì¥ ë¶„ì„ ì •ë³´
            'market_correlations': market_correlations,
            'sector_performance': sector_info,
            'institutional_flow': institutional_flow,
            'market_regime': result.get('market_regime', 'unknown'),
            # ë°ì´í„° ë° í•™ìŠµ ì •ë³´
            'data_points': len(prices),
            'training_samples': result.get('training_samples', 'N/A')
        }
        
        # 5. ê·¸ë˜í”„ í‘œì‹œ
        if show_plot:
            self._plot_predictions(dates, prices, future_dates, predicted_prices, symbol)
        
        return prediction_result
    
    def _get_models_used(self):
        """ì‚¬ìš©ëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        models = ['Kalman Filter', 'ARIMA']

        if SKLEARN_AVAILABLE:
            models.append('Random Forest')
        if XGBOOST_AVAILABLE:
            models.append('XGBoost')
        if LIGHTGBM_AVAILABLE:
            models.append('LightGBM')
        if STATSMODELS_AVAILABLE:
            models.append('ARIMA (Full)')

        if self.use_deep_learning:
            if TENSORFLOW_AVAILABLE:
                models.extend(['LSTM', 'Transformer'])

        if self.use_optimization:
            models.append('+ Bayesian Optimization')

        return models
    
    def _generate_recommendation(self, returns, confidence):
        """ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë°˜ íˆ¬ì ì¶”ì²œ"""
        avg_return = np.mean(returns)
        
        if confidence > 0.7:
            if avg_return > 5:
                return "ê°•ë ¥ ë§¤ìˆ˜ ì¶”ì²œ"
            elif avg_return > 2:
                return "ë§¤ìˆ˜ ì¶”ì²œ"
            elif avg_return < -5:
                return "ë§¤ë„ ì¶”ì²œ"
            elif avg_return < -2:
                return "ë§¤ë„ ê³ ë ¤"
            else:
                return "ë³´ìœ "
        else:
            return "ë¶ˆí™•ì‹¤ - ì‹ ì¤‘í•œ íŒë‹¨ í•„ìš”"
    
    def _plot_predictions(self, historical_dates, historical_prices, 
                         future_dates, predicted_prices, symbol):
        """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
        plt.figure(figsize=(12, 8))
        
        # ê³¼ê±° ë°ì´í„°
        plt.plot(historical_dates[-60:], historical_prices[-60:], 
                'b-', label='ì‹¤ì œ ê°€ê²©', linewidth=2)
        
        # ì˜ˆì¸¡ ë°ì´í„°
        plt.plot(future_dates, predicted_prices, 
                'r--', label='ì˜ˆì¸¡ ê°€ê²©', linewidth=2, marker='o')
        
        # ì—°ê²°ì„ 
        plt.plot([historical_dates[-1], future_dates[0]], 
                [historical_prices[-1], predicted_prices[0]], 
                'g:', linewidth=1)
        
        plt.title(f'{symbol} ì£¼ê°€ ì˜ˆì¸¡ ê²°ê³¼ (TensorFlow ì—†ìŒ)', fontsize=16)
        plt.xlabel('ë‚ ì§œ')
        plt.ylabel('ê°€ê²©')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

    def _backtest_single_point(self, args):
        """ë‹¨ì¼ ë°±í…ŒìŠ¤íŒ… í¬ì¸íŠ¸ ì‹¤í–‰ (ë³‘ë ¬/ìˆœì°¨ ì²˜ë¦¬ìš©)"""
        i, test_point, train_prices, actual_future_prices, forecast_days, test_date = args

        try:
            # ProcessPool ë³‘ë ¬ ì²˜ë¦¬: ê° í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ë…ë¦½ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            # (GIL ìš°íšŒë¡œ ì§„ì •í•œ ë³‘ë ¬ ì²˜ë¦¬, ëª¨ë“ˆì€ ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ 1íšŒ ë¡œë”©)
            ensemble = EnsemblePredictor(
                use_deep_learning=self.use_deep_learning,
                use_optimization=False  # ë°±í…ŒìŠ¤íŒ…ì—ì„œëŠ” ìµœì í™” ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
            )

            result = ensemble.fit_predict(train_prices, forecast_days)
            predicted_prices = result['ensemble_predictions']

            # ì‹¤ì œ vs ì˜ˆì¸¡ ë¹„êµ
            last_train_price = train_prices[-1]
            actual_return = (actual_future_prices[-1] - last_train_price) / last_train_price * 100
            predicted_return = (predicted_prices[-1] - last_train_price) / last_train_price * 100

            # MAE, RMSE ê³„ì‚°
            mae = np.mean(np.abs(predicted_prices - actual_future_prices))
            rmse = np.sqrt(np.mean((predicted_prices - actual_future_prices) ** 2))
            mape = np.mean(np.abs((actual_future_prices - predicted_prices) / actual_future_prices)) * 100

            # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘ (ëª¨ë¸ë³„ ì„±ëŠ¥ ë¶„ì„ìš©)
            individual_predictions = {}

            # ë””ë²„ê¹…: result í‚¤ í™•ì¸
            logger.debug(f"Result keys: {result.keys()}")

            if 'individual_results' in result:
                logger.debug(f"Individual results found: {result['individual_results'].keys()}")
                for model_name, model_result in result['individual_results'].items():
                    logger.debug(f"Processing model: {model_name}, type: {type(model_result)}")
                    if isinstance(model_result, dict) and 'future_predictions' in model_result:
                        model_pred_price = model_result['future_predictions'][-1]

                        model_pred_return = (model_pred_price - last_train_price) / last_train_price * 100
                        individual_predictions[model_name] = {
                            'predicted_return': model_pred_return,
                            'direction_match': (actual_return > 0) == (model_pred_return > 0)
                        }
                        logger.debug(f"{model_name} ì˜ˆì¸¡ ì¶”ê°€: {model_pred_return:.2f}%")
                    else:
                        logger.debug(f"{model_name} ìŠ¤í‚µ: future_predictions ì—†ìŒ")
            else:
                logger.warning("individual_results í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤!")

            return {
                'success': True,
                'index': i,
                'date': test_date,
                'actual_return': actual_return,
                'predicted_return': predicted_return,
                'mae': mae,
                'rmse': rmse,
                'mape': mape,
                'direction_match': (actual_return > 0) == (predicted_return > 0),
                'individual_predictions': individual_predictions  # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡
            }

        except Exception as e:
            logger.warning(f"ë°±í…ŒìŠ¤íŒ… {i+1} ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'index': i,
                'error': str(e)
            }

    def backtest_predictions(self, ticker, test_periods=30, forecast_days=7,
                           progress_callback=None, use_parallel=False, cancel_callback=None):
        """
        ë°±í…ŒìŠ¤íŒ…: ê³¼ê±° ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì„±ëŠ¥ ê²€ì¦

        Args:
            ticker: ì¢…ëª© ì½”ë“œ
            test_periods: í…ŒìŠ¤íŠ¸í•  ê¸°ê°„ ìˆ˜
            forecast_days: ì˜ˆì¸¡ ì¼ìˆ˜
            progress_callback: ì§„í–‰ ìƒíƒœ ì½œë°± (current, total, message)
            use_parallel: ë³‘ë ¬ ì²˜ë¦¬ ì—¬ë¶€
            cancel_callback: ì¤‘ì§€ í™•ì¸ ì½œë°±

        Returns:
            (summary, error): ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ ìš”ì•½ê³¼ ì—ëŸ¬ ë©”ì‹œì§€
        """
        try:
            # ì´ˆê¸° ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            if progress_callback:
                progress_callback(0, test_periods, f"ë°ì´í„° ë¡œë”© ì¤‘... ({ticker})")

            # ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì¶©ë¶„í•œ ê¸°ê°„)
            data = self.get_stock_data(ticker, period="2y")
            if data is None or len(data) < 100:
                return None, "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"

            prices = data['Close'].values
            dates = data.index

            # ë°±í…ŒìŠ¤íŒ… ì‘ì—… ì¤€ë¹„
            tasks = []
            for i in range(test_periods):
                test_point = len(prices) - (test_periods - i) * forecast_days - forecast_days

                if test_point < 100:
                    continue

                train_prices = prices[:test_point]
                actual_future_prices = prices[test_point:test_point + forecast_days]

                if len(actual_future_prices) < forecast_days:
                    continue

                tasks.append((
                    i,
                    test_point,
                    train_prices,
                    actual_future_prices,
                    forecast_days,
                    dates[test_point]
                ))

            if len(tasks) == 0:
                return None, "ë°±í…ŒìŠ¤íŒ…í•  ë°ì´í„° í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"

            # ì‘ì—… ì¤€ë¹„ ì™„ë£Œ ì•Œë¦¼
            if progress_callback:
                progress_callback(0, len(tasks), f"ë°±í…ŒìŠ¤íŒ… ì‹œì‘ ì¤‘... ({len(tasks)}ê°œ ì‘ì—…)")

            results = []
            errors = []
            actual_returns = []
            predicted_returns = []

            # ë³‘ë ¬ ì²˜ë¦¬ vs ìˆœì°¨ ì²˜ë¦¬
            if use_parallel:
                import time
                start_time = time.time()

                # ProcessPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰ (GIL ìš°íšŒ, ì§„ì •í•œ ë³‘ë ¬ ì²˜ë¦¬)
                max_workers = min(multiprocessing.cpu_count(), len(tasks))
                logger.info(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ: {max_workers}ê°œ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš© (CPU ì½”ì–´: {multiprocessing.cpu_count()}, ì‘ì—… ìˆ˜: {len(tasks)})")

                with ProcessPoolExecutor(max_workers=max_workers) as executor:
                    futures = {executor.submit(self._backtest_single_point, task): task for task in tasks}

                    completed = 0
                    for future in as_completed(futures):
                        # ì¤‘ì§€ í™•ì¸
                        if cancel_callback and cancel_callback():
                            executor.shutdown(wait=False, cancel_futures=True)
                            return None, "ì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë¨"

                        try:
                            result = future.result()
                        except Exception as e:
                            completed += 1
                            logger.error(f"ë°±í…ŒìŠ¤íŒ… ì‘ì—… ì‹¤íŒ¨: {e}")
                            errors.append(str(e))
                            if progress_callback:
                                progress_callback(completed, len(tasks), f"ì˜¤ë¥˜ ë°œìƒ {completed}/{len(tasks)}")
                            continue

                        completed += 1

                        # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                        if progress_callback:
                            task = futures[future]
                            progress_callback(completed, len(tasks),
                                            f"ë°±í…ŒìŠ¤íŒ… {completed}/{len(tasks)}: {task[5]:%Y-%m-%d}")

                        if result.get('success', False):
                            results.append({
                                'date': result['date'],
                                'actual_return': result['actual_return'],
                                'predicted_return': result['predicted_return'],
                                'mae': result['mae'],
                                'rmse': result['rmse'],
                                'mape': result['mape'],
                                'direction_match': result['direction_match'],
                                'individual_predictions': result.get('individual_predictions', {})
                            })
                            actual_returns.append(result['actual_return'])
                            predicted_returns.append(result['predicted_return'])
                        else:
                            errors.append(result.get('error', 'Unknown error'))

                elapsed_time = time.time() - start_time
                logger.info(f"â±ï¸ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ (í‰ê·  {elapsed_time/len(tasks):.2f}ì´ˆ/ì‘ì—…)")

            else:
                # ìˆœì°¨ ì²˜ë¦¬
                import time
                start_time = time.time()
                logger.info("ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")

                for i, task in enumerate(tasks):
                    # ì¤‘ì§€ í™•ì¸
                    if cancel_callback and cancel_callback():
                        return None, "ì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë¨"

                    # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                    if progress_callback:
                        progress_callback(i + 1, len(tasks),
                                        f"ë°±í…ŒìŠ¤íŒ… {i+1}/{len(tasks)}: {task[5]:%Y-%m-%d}")

                    result = self._backtest_single_point(task)

                    if result['success']:
                        results.append({
                            'date': result['date'],
                            'actual_return': result['actual_return'],
                            'predicted_return': result['predicted_return'],
                            'mae': result['mae'],
                            'rmse': result['rmse'],
                            'mape': result['mape'],
                            'direction_match': result['direction_match'],
                            'individual_predictions': result.get('individual_predictions', {})
                        })
                        actual_returns.append(result['actual_return'])
                        predicted_returns.append(result['predicted_return'])
                    else:
                        errors.append(result['error'])

                elapsed_time = time.time() - start_time
                logger.info(f"â±ï¸ ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ (í‰ê·  {elapsed_time/len(tasks):.2f}ì´ˆ/ì‘ì—…)")

            # ìš”ì•½ í†µê³„ ê³„ì‚°
            if len(results) == 0:
                return None, "ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"

            direction_accuracy = np.mean([r['direction_match'] for r in results]) * 100
            avg_mae = np.mean([r['mae'] for r in results])
            avg_rmse = np.mean([r['rmse'] for r in results])
            avg_mape = np.mean([r['mape'] for r in results])

            # ìƒê´€ê´€ê³„
            if len(actual_returns) > 1:
                correlation = np.corrcoef(actual_returns, predicted_returns)[0, 1]
            else:
                correlation = 0

            # ìƒì„¸ ë¶„ì„ ì¶”ê°€
            bull_correct = sum(1 for r in results if r['actual_return'] > 0 and r['direction_match'])
            bull_total = sum(1 for r in results if r['actual_return'] > 0)
            bear_correct = sum(1 for r in results if r['actual_return'] <= 0 and r['direction_match'])
            bear_total = sum(1 for r in results if r['actual_return'] <= 0)

            pred_bull = sum(1 for r in results if r['predicted_return'] > 0)
            pred_bear = len(results) - pred_bull

            # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
            logger.debug(f"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ì‹œì‘ - ì´ {len(results)}ê°œ ê²°ê³¼")
            model_performance = {}
            for idx, result in enumerate(results):
                logger.debug(f"ê²°ê³¼ {idx}: 'individual_predictions' ì¡´ì¬ = {'individual_predictions' in result}")
                if 'individual_predictions' in result:
                    logger.debug(f"ê²°ê³¼ {idx} individual_predictions: {result['individual_predictions'].keys()}")
                    for model_name, model_pred in result['individual_predictions'].items():
                        if model_name not in model_performance:
                            model_performance[model_name] = {'correct': 0, 'total': 0}

                        model_performance[model_name]['total'] += 1
                        if model_pred['direction_match']:
                            model_performance[model_name]['correct'] += 1
                else:
                    logger.warning(f"ê²°ê³¼ {idx}ì— individual_predictions ì—†ìŒ: {result.keys()}")

            logger.debug(f"ìµœì¢… model_performance: {model_performance}")

            # ëª¨ë¸ë³„ ì ì¤‘ë¥  ê³„ì‚°
            model_accuracies = {}
            for model_name, perf in model_performance.items():
                if perf['total'] > 0:
                    accuracy = (perf['correct'] / perf['total']) * 100
                    model_accuracies[model_name] = accuracy
                    logger.info(f"ğŸ“Š {model_name} ì ì¤‘ë¥ : {accuracy:.1f}% ({perf['correct']}/{perf['total']})")

            logger.debug(f"ìµœì¢… model_accuracies: {model_accuracies}")

            # âœ… ìˆ˜ìµë¥  ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€ ì¶”ê°€
            # ê°€ìƒ íŠ¸ë ˆì´ë”© ì‹œë®¬ë ˆì´ì…˜: ì˜ˆì¸¡ì„ ë¯¿ê³  ë§¤ë§¤í–ˆì„ ë•Œ ìˆ˜ìµë¥ 
            initial_capital = 10000.0  # ì´ˆê¸° ìë³¸
            current_capital = initial_capital

            # ì „ëµ 1: ì˜ˆì¸¡ ë°©í–¥ì— ë”°ë¥¸ ë‹¨ìˆœ ë§¤ë§¤
            for r in results:
                # ì˜ˆì¸¡ì´ ìƒìŠ¹ì´ë©´ ë§¤ìˆ˜, í•˜ë½ì´ë©´ ê³µë§¤ë„
                if r['predicted_return'] > 0:
                    # ë§¤ìˆ˜ ì „ëµ: ì‹¤ì œ ìˆ˜ìµë¥ ë§Œí¼ ìë³¸ ì¦ê°€
                    current_capital *= (1 + r['actual_return'] / 100)
                else:
                    # ê³µë§¤ë„ ì „ëµ: ì‹¤ì œ í•˜ë½ë¥ ë§Œí¼ ìë³¸ ì¦ê°€ (ë°˜ëŒ€ ë°©í–¥)
                    current_capital *= (1 - r['actual_return'] / 100)

            total_return_pct = ((current_capital - initial_capital) / initial_capital) * 100

            # ì „ëµ 2: ë°©í–¥ì´ ë§ì•˜ì„ ë•Œë§Œ ìˆ˜ìµ
            correct_capital = initial_capital
            for r in results:
                if r['direction_match']:
                    # ë°©í–¥ì´ ë§ì•˜ì„ ë•Œë§Œ ì‹¤ì œ ìˆ˜ìµë¥ ë§Œí¼ ì¦ê°€
                    correct_capital *= (1 + abs(r['actual_return']) / 100)

            correct_only_return_pct = ((correct_capital - initial_capital) / initial_capital) * 100

            # ìµœëŒ€ ë‚™í­ (Maximum Drawdown) ê³„ì‚°
            capital_history = [initial_capital]
            current_capital_track = initial_capital
            for r in results:
                if r['predicted_return'] > 0:
                    current_capital_track *= (1 + r['actual_return'] / 100)
                else:
                    current_capital_track *= (1 - r['actual_return'] / 100)
                capital_history.append(current_capital_track)

            running_max = initial_capital
            max_drawdown = 0
            for cap in capital_history:
                if cap > running_max:
                    running_max = cap
                drawdown = ((running_max - cap) / running_max) * 100
                if drawdown > max_drawdown:
                    max_drawdown = drawdown

            # Sharpe Ratio (ìœ„í—˜ ëŒ€ë¹„ ìˆ˜ìµ)
            # ìˆ˜ìµë¥ ì˜ í‰ê·  / í‘œì¤€í¸ì°¨ (ê°„ë‹¨í•œ ë²„ì „)
            returns_array = np.array([r['actual_return'] for r in results])
            if np.std(returns_array) > 0:
                sharpe_ratio = np.mean(returns_array) / np.std(returns_array)
            else:
                sharpe_ratio = 0

            logger.info(f"ğŸ’° ìˆ˜ìµë¥  ì‹œë®¬ë ˆì´ì…˜:")
            logger.info(f"   - ì˜ˆì¸¡ ê¸°ë°˜ ë§¤ë§¤: {total_return_pct:+.2f}%")
            logger.info(f"   - ì •ë‹µë§Œ ë§¤ë§¤: {correct_only_return_pct:+.2f}%")
            logger.info(f"   - ìµœëŒ€ ë‚™í­: {max_drawdown:.2f}%")
            logger.info(f"   - Sharpe Ratio: {sharpe_ratio:.3f}")

            summary = {
                'ticker': ticker,
                'test_periods': len(results),
                'forecast_days': forecast_days,
                'direction_accuracy': direction_accuracy,
                'avg_mae': avg_mae,
                'avg_rmse': avg_rmse,
                'avg_mape': avg_mape,
                'correlation': correlation,
                'actual_returns': actual_returns,
                'predicted_returns': predicted_returns,
                'results': results,
                'errors': errors,
                # ìƒì„¸ ë¶„ì„
                'bull_accuracy': (bull_correct / bull_total * 100) if bull_total > 0 else 0,
                'bear_accuracy': (bear_correct / bear_total * 100) if bear_total > 0 else 0,
                'bull_total': bull_total,
                'bear_total': bear_total,
                'pred_bull': pred_bull,
                'pred_bear': pred_bear,
                # ëª¨ë¸ë³„ ì„±ëŠ¥
                'model_accuracies': model_accuracies,
                # ìˆ˜ìµë¥  ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€
                'trading_return': total_return_pct,
                'correct_only_return': correct_only_return_pct,
                'max_drawdown': max_drawdown,
                'sharpe_ratio': sharpe_ratio,
                'capital_history': capital_history
            }

            return summary, None

        except Exception as e:
            logger.error(f"ë°±í…ŒìŠ¤íŒ… ì˜¤ë¥˜: {e}")
            return None, str(e)

# ì‚¬ìš© ì˜ˆì œ
def example_usage():
    """ì‚¬ìš© ì˜ˆì œ"""
    # ì˜µì…˜ ì„¤ì •:
    # use_deep_learning=True: LSTM, Transformer ì‚¬ìš© (TensorFlow í•„ìš”)
    # use_optimization=True: Bayesian Optimization ì‚¬ìš© (scikit-optimize í•„ìš”)
    predictor = StockPredictor(
        use_deep_learning=True,      # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‚¬ìš©
        use_optimization=False        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    )

    logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:")
    logger.info(f"- scikit-learn: {'ì‚¬ìš© ê°€ëŠ¥' if SKLEARN_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"- XGBoost: {'ì‚¬ìš© ê°€ëŠ¥' if XGBOOST_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"- LightGBM: {'ì‚¬ìš© ê°€ëŠ¥' if LIGHTGBM_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"- statsmodels: {'ì‚¬ìš© ê°€ëŠ¥' if STATSMODELS_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"- TensorFlow: {'ì‚¬ìš© ê°€ëŠ¥' if TENSORFLOW_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"- Bayesian Opt: {'ì‚¬ìš© ê°€ëŠ¥' if HYPEROPT_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")

    # ì˜ˆì œ 1: ì• í”Œ ì£¼ì‹ ì˜ˆì¸¡
    logger.info("=" * 50)
    logger.info("APPLE (AAPL) ì£¼ê°€ ì˜ˆì¸¡")
    logger.info("=" * 50)

    result = predictor.predict_stock_price('AAPL', forecast_days=7, show_plot=False)

    if 'error' not in result:
        logger.info(f"í˜„ì¬ê°€: ${result['current_price']:.2f}")
        logger.info(f"ì˜ˆì¸¡ ê°€ê²©: {[f'${p:.2f}' for p in result['predicted_prices']]}")
        logger.info(f"ì˜ˆìƒ ìˆ˜ìµë¥ : {[f'{r:.1f}%' for r in result['expected_returns']]}")
        logger.info(f"ì‹ ë¢°ë„: {result['confidence_score']:.1%}")
        logger.info(f"ì¶”ì²œ: {result['recommendation']}")
        logger.info(f"ì‚¬ìš© ëª¨ë¸: {', '.join(result['models_used'])}")

if __name__ == "__main__":
    example_usage()
