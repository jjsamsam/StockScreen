#!/usr/bin/env python3
"""
stock_prediction.py
TensorFlow ì—†ì´ scikit-learn, XGBoost, LightGBM, statsmodelsë¥¼ ì‚¬ìš©í•œ ì£¼ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- Kalman Filter (ìˆœìˆ˜ NumPy êµ¬í˜„)
- XGBoost Regressor
- LightGBM Regressor  
- Random Forest
- ARIMA
- Ensemble Method
"""

import numpy as np
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

# ë¡œê¹… ì„¤ì •
from logger_config import get_logger
logger = get_logger(__name__)

# ìµœì í™” ëª¨ë“ˆ
from cache_manager import get_stock_data

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from sklearn.linear_model import LinearRegression
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
    from sklearn.metrics import mean_squared_error, mean_absolute_error
    from sklearn.model_selection import train_test_split, TimeSeriesSplit
    SKLEARN_AVAILABLE = True
    logger.info("scikit-learn ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.error("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install scikit-learn")
    SKLEARN_AVAILABLE = False

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
    logger.info(f"XGBoost ì‚¬ìš© ê°€ëŠ¥ (v{xgb.__version__})")
except ImportError:
    logger.error("XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install xgboost")
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
    logger.info(f"LightGBM ì‚¬ìš© ê°€ëŠ¥ (v{lgb.__version__})")
except ImportError:
    logger.error("LightGBMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install lightgbm")
    LIGHTGBM_AVAILABLE = False

try:
    from statsmodels.tsa.arima.model import ARIMA
    import statsmodels.api as sm
    STATSMODELS_AVAILABLE = True
    logger.info("statsmodels ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.error("statsmodelsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install statsmodels")
    STATSMODELS_AVAILABLE = False

# TensorFlow Lazy Import (ì‹¤ì œ ì‚¬ìš© ì‹œì ì— import)
TENSORFLOW_AVAILABLE = None  # None = ì•„ì§ í™•ì¸ ì•ˆ í•¨, True/False = í™•ì¸ ì™„ë£Œ
_tensorflow_modules = {}  # ìºì‹±ìš©

def _lazy_import_tensorflow():
    """TensorFlowë¥¼ í•„ìš”í•  ë•Œë§Œ import (Lazy Loading)"""
    global TENSORFLOW_AVAILABLE, _tensorflow_modules

    if TENSORFLOW_AVAILABLE is not None:
        return TENSORFLOW_AVAILABLE

    try:
        logger.info("TensorFlow ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒ, ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        import tensorflow as tf
        from tensorflow import keras
        from tensorflow.keras import layers, Model
        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

        _tensorflow_modules['tf'] = tf
        _tensorflow_modules['keras'] = keras
        _tensorflow_modules['layers'] = layers
        _tensorflow_modules['Model'] = Model
        _tensorflow_modules['EarlyStopping'] = EarlyStopping
        _tensorflow_modules['ReduceLROnPlateau'] = ReduceLROnPlateau

        TENSORFLOW_AVAILABLE = True
        logger.info(f"âœ… TensorFlow ë¡œë”© ì™„ë£Œ (v{tf.__version__})")
        return True
    except ImportError as e:
        logger.warning(f"TensorFlowë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        logger.info("LSTM/Transformer ëª¨ë¸ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤ (XGBoost/LightGBM ë“± ë‹¤ë¥¸ ëª¨ë¸ì€ ì •ìƒ ì‘ë™)")
        TENSORFLOW_AVAILABLE = False
        return False

try:
    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
    from skopt import BayesSearchCV
    from skopt.space import Real, Integer, Categorical
    HYPEROPT_AVAILABLE = True
    logger.info("Hyperparameter ìµœì í™” ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.warning("scikit-optimize ì„¤ì¹˜ ê¶Œì¥: pip install scikit-optimize")
    HYPEROPT_AVAILABLE = False

class KalmanFilterPredictor:
    """
    ìˆœìˆ˜ NumPyë¡œ êµ¬í˜„í•œ Kalman Filterë¥¼ ì‚¬ìš©í•œ ì£¼ê°€ ì˜ˆì¸¡
    
    ì›ë¦¬:
    - ìƒíƒœ ê³µê°„ ëª¨ë¸ë¡œ ì£¼ê°€ì˜ ìˆ¨ê²¨ì§„ ìƒíƒœ(ì¶”ì„¸, ë…¸ì´ì¦ˆ)ë¥¼ ì¶”ì 
    - ì´ì „ ìƒíƒœ + ê´€ì¸¡ê°’ì„ ê²°í•©í•˜ì—¬ ìµœì  ì¶”ì •
    - ë‹¨ê¸° ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ì§„ì§œ ì¶”ì„¸ íŒŒì•…
    """
    
    def __init__(self, process_variance=1e-5, measurement_variance=1e-1):
        """
        Args:
            process_variance: í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ (ì‘ì„ìˆ˜ë¡ ì•ˆì •ì )
            measurement_variance: ì¸¡ì • ë…¸ì´ì¦ˆ (í´ìˆ˜ë¡ ê´€ì¸¡ê°’ ëœ ì‹ ë¢°)
        """
        self.process_variance = process_variance
        self.measurement_variance = measurement_variance
        self.reset()
    
    def reset(self):
        """ì¹¼ë§Œ í•„í„° ìƒíƒœ ì´ˆê¸°í™”"""
        self.x = 0  # ìƒíƒœ ì¶”ì •ê°’ (ê°€ê²©)
        self.P = 1  # ì˜¤ì°¨ ê³µë¶„ì‚°
        self.Q = self.process_variance  # í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ
        self.R = self.measurement_variance  # ì¸¡ì • ë…¸ì´ì¦ˆ
        
        self.predictions = []
        self.states = []
    
    def predict_and_update(self, measurement):
        """ì¹¼ë§Œ í•„í„°ì˜ ì˜ˆì¸¡-ì—…ë°ì´íŠ¸ ë‹¨ê³„"""
        # 1. ì˜ˆì¸¡ ë‹¨ê³„
        x_pred = self.x
        P_pred = self.P + self.Q
        
        # 2. ì—…ë°ì´íŠ¸ ë‹¨ê³„
        K = P_pred / (P_pred + self.R)
        self.x = x_pred + K * (measurement - x_pred)
        self.P = (1 - K) * P_pred
        
        # ê²°ê³¼ ì €ì¥
        self.predictions.append(self.x)
        self.states.append({'x': self.x, 'P': self.P, 'K': K})
        
        return self.x
    
    def fit_predict(self, prices, forecast_days=5):
        """ê°€ê²© ì‹œê³„ì—´ì— ì¹¼ë§Œ í•„í„° ì ìš© ë° ë¯¸ë˜ ì˜ˆì¸¡"""
        self.reset()
        
        # ëª¨ë“  ê´€ì¸¡ê°’ì— ëŒ€í•´ ì¹¼ë§Œ í•„í„° ì ìš©
        filtered_prices = []
        for price in prices:
            filtered_price = self.predict_and_update(price)
            filtered_prices.append(filtered_price)
        
        # âœ… Kalman Filter ë¯¸ë˜ ì˜ˆì¸¡
        # í˜„ì¬ê°€ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ (í•„í„°ë§ ê°’ê³¼ì˜ ì°¨ì´ ë³´ì •)
        current_price = prices[-1]
        last_filtered = filtered_prices[-1]

        # í•„í„°ë§ëœ ì¶”ì„¸ ê³„ì‚°
        if len(filtered_prices) >= 10:
            trend = (filtered_prices[-1] - filtered_prices[-10]) / 10
        else:
            trend = filtered_prices[-1] - filtered_prices[-2] if len(filtered_prices) >= 2 else 0

        # í˜„ì¬ê°€ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ (í•„í„°ë§ ì˜¤ì°¨ ë³´ì •)
        future_predictions = []
        for i in range(forecast_days):
            # í˜„ì¬ê°€ + ì¶”ì„¸
            future_pred = current_price + trend * (i + 1)
            future_predictions.append(future_pred)
        
        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
        confidence_interval = 2 * np.sqrt(self.P)
        
        return {
            'filtered_prices': np.array(filtered_prices),
            'future_predictions': np.array(future_predictions),
            'confidence_interval': confidence_interval,
            'last_state': self.x,
            'uncertainty': self.P
        }

class HyperparameterOptimizer:
    """Bayesian Optimizationì„ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""

    @staticmethod
    def optimize_xgboost(X_train, y_train, n_iter=20):
        """XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        if not HYPEROPT_AVAILABLE or not XGBOOST_AVAILABLE:
            logger.warning("Bayesian Optimization ë¶ˆê°€ - ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
            return None

        search_spaces = {
            'n_estimators': Integer(100, 500),
            'max_depth': Integer(3, 15),
            'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
            'min_child_weight': Integer(1, 10),
            'subsample': Real(0.6, 1.0),
            'colsample_bytree': Real(0.6, 1.0),
            'gamma': Real(0.0, 0.5),
            'reg_alpha': Real(0.0, 1.0),
            'reg_lambda': Real(0.0, 2.0)
        }

        bayes_cv = BayesSearchCV(
            xgb.XGBRegressor(random_state=42, verbosity=0),
            search_spaces,
            n_iter=n_iter,
            cv=3,
            n_jobs=-1,
            scoring='neg_mean_squared_error',
            verbose=0
        )

        bayes_cv.fit(X_train, y_train)
        logger.info(f"XGBoost ìµœì  íŒŒë¼ë¯¸í„°: {bayes_cv.best_params_}")

        return bayes_cv.best_estimator_

    @staticmethod
    def optimize_lightgbm(X_train, y_train, n_iter=20):
        """LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        if not HYPEROPT_AVAILABLE or not LIGHTGBM_AVAILABLE:
            logger.warning("Bayesian Optimization ë¶ˆê°€ - ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
            return None

        search_spaces = {
            'n_estimators': Integer(100, 500),
            'max_depth': Integer(3, 15),
            'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
            'num_leaves': Integer(20, 100),
            'min_child_samples': Integer(10, 50),
            'subsample': Real(0.6, 1.0),
            'colsample_bytree': Real(0.6, 1.0),
            'reg_alpha': Real(0.0, 1.0),
            'reg_lambda': Real(0.0, 2.0)
        }

        bayes_cv = BayesSearchCV(
            lgb.LGBMRegressor(random_state=42, verbosity=-1),
            search_spaces,
            n_iter=n_iter,
            cv=3,
            n_jobs=-1,
            scoring='neg_mean_squared_error',
            verbose=0
        )

        bayes_cv.fit(X_train, y_train)
        logger.info(f"LightGBM ìµœì  íŒŒë¼ë¯¸í„°: {bayes_cv.best_params_}")

        return bayes_cv.best_estimator_

    @staticmethod
    def optimize_random_forest(X_train, y_train, n_iter=20):
        """Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        if not HYPEROPT_AVAILABLE or not SKLEARN_AVAILABLE:
            logger.warning("Bayesian Optimization ë¶ˆê°€ - ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
            return None

        search_spaces = {
            'n_estimators': Integer(100, 500),
            'max_depth': Integer(5, 30),
            'min_samples_split': Integer(2, 20),
            'min_samples_leaf': Integer(1, 10),
            'max_features': Categorical(['sqrt', 'log2', None])
        }

        bayes_cv = BayesSearchCV(
            RandomForestRegressor(random_state=42, n_jobs=-1),
            search_spaces,
            n_iter=n_iter,
            cv=3,
            n_jobs=1,  # RF ìì²´ê°€ ë³‘ë ¬ì²˜ë¦¬
            scoring='neg_mean_squared_error',
            verbose=0
        )

        bayes_cv.fit(X_train, y_train)
        logger.info(f"Random Forest ìµœì  íŒŒë¼ë¯¸í„°: {bayes_cv.best_params_}")

        return bayes_cv.best_estimator_


class AdvancedMLPredictor:
    """
    XGBoost, LightGBM, Random Forestë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ê¸°
    """

    def __init__(self, sequence_length=30, use_optimization=False, ticker=None, auto_load=True):
        self.sequence_length = sequence_length
        self.scaler = RobustScaler()  # StandardScaler -> RobustScaler (ì´ìƒì¹˜ì— ê°•í•¨)
        self.models = {}
        self.use_optimization = use_optimization
        self.progress_callback = None  # ì§„í–‰ ì½œë°± (ì™¸ë¶€ì—ì„œ ì„¤ì •)
        self.ticker = ticker
        self.persistence = None

        # ëª¨ë¸ ì €ì¥/ë¡œë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            from model_persistence import get_model_persistence
            self.persistence = get_model_persistence()

            # ìë™ ë¡œë“œ
            if auto_load and ticker:
                self._try_load_models()
        except ImportError:
            logger.debug("model_persistence ëª¨ë“ˆ ì—†ìŒ, ì €ì¥/ë¡œë“œ ë¹„í™œì„±í™”")

    def _try_load_models(self):
        """ì €ì¥ëœ ML ëª¨ë¸ë“¤ ìë™ ë¡œë“œ ì‹œë„"""
        if not self.persistence or not self.ticker:
            return

        loaded_count = 0
        for model_type in ['random_forest', 'xgboost', 'lightgbm']:
            try:
                model, metadata, scaler = self.persistence.load_sklearn_model(self.ticker, model_type)
                if model is not None:
                    self.models[model_type] = model
                    if scaler is not None and loaded_count == 0:  # ì²« ë²ˆì§¸ ëª¨ë¸ì˜ scaler ì‚¬ìš©
                        self.scaler = scaler
                    loaded_count += 1
                    logger.info(f"âœ… ì €ì¥ëœ {model_type} ëª¨ë¸ ë¡œë“œ: {self.ticker}")
            except Exception as e:
                logger.debug(f"{model_type} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        if loaded_count > 0:
            logger.info(f"âœ… ì´ {loaded_count}ê°œ ML ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True

        return False
        
    def create_features(self, data):
        """ê¸°ìˆ ì  ì§€í‘œë¥¼ í¬í•¨í•œ í”¼ì²˜ ìƒì„± - ê³ ê¸‰ ì§€í‘œ ì¶”ê°€"""
        df = pd.DataFrame()

        # ê¸°ë³¸ ê°€ê²© ì •ë³´
        df['close'] = data
        df['high'] = data  # ê°„ë‹¨í™”ë¥¼ ìœ„í•´ closeì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        df['low'] = data
        df['volume'] = 1000000  # ë”ë¯¸ ê±°ë˜ëŸ‰

        # === ê¸°ì¡´ ì§€í‘œ ===
        # ì´ë™í‰ê· 
        df['ma5'] = df['close'].rolling(5).mean()
        df['ma10'] = df['close'].rolling(10).mean()
        df['ma20'] = df['close'].rolling(20).mean()
        df['ma50'] = df['close'].rolling(50).mean()
        df['ma200'] = df['close'].rolling(200).mean()

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # MACD
        exp1 = df['close'].ewm(span=12).mean()
        exp2 = df['close'].ewm(span=26).mean()
        df['macd'] = exp1 - exp2
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']

        # ë³¼ë¦°ì € ë°´ë“œ
        df['bb_middle'] = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']

        # ë³€í™”ìœ¨
        df['pct_change_1'] = df['close'].pct_change()
        df['pct_change_5'] = df['close'].pct_change(5)
        df['pct_change_10'] = df['close'].pct_change(10)

        # ë³€ë™ì„±
        df['volatility'] = df['close'].rolling(20).std()

        # === ìƒˆë¡œìš´ ê³ ê¸‰ ì§€í‘œ ===
        # ATR (Average True Range) - ë³€ë™ì„±
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        df['atr'] = true_range.rolling(14).mean()

        # ADX (Average Directional Index) - ì¶”ì„¸ ê°•ë„
        plus_dm = df['high'].diff()
        minus_dm = -df['low'].diff()
        plus_dm[plus_dm < 0] = 0
        minus_dm[minus_dm < 0] = 0

        tr14 = true_range.rolling(14).sum()
        plus_di = 100 * (plus_dm.rolling(14).sum() / tr14)
        minus_di = 100 * (minus_dm.rolling(14).sum() / tr14)

        dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di)
        df['adx'] = dx.rolling(14).mean()
        df['plus_di'] = plus_di
        df['minus_di'] = minus_di

        # Stochastic Oscillator - ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„
        low_14 = df['low'].rolling(14).min()
        high_14 = df['high'].rolling(14).max()
        df['stoch_k'] = 100 * (df['close'] - low_14) / (high_14 - low_14)
        df['stoch_d'] = df['stoch_k'].rolling(3).mean()

        # OBV (On-Balance Volume) - ê±°ë˜ëŸ‰ ê¸°ë°˜
        obv = [0]
        for i in range(1, len(df)):
            if df['close'].iloc[i] > df['close'].iloc[i-1]:
                obv.append(obv[-1] + df['volume'].iloc[i])
            elif df['close'].iloc[i] < df['close'].iloc[i-1]:
                obv.append(obv[-1] - df['volume'].iloc[i])
            else:
                obv.append(obv[-1])
        df['obv'] = obv
        df['obv_ma'] = df['obv'].rolling(20).mean()

        # Ichimoku Cloud
        nine_period_high = df['high'].rolling(9).max()
        nine_period_low = df['low'].rolling(9).min()
        df['tenkan_sen'] = (nine_period_high + nine_period_low) / 2

        period26_high = df['high'].rolling(26).max()
        period26_low = df['low'].rolling(26).min()
        df['kijun_sen'] = (period26_high + period26_low) / 2

        df['senkou_span_a'] = ((df['tenkan_sen'] + df['kijun_sen']) / 2).shift(26)

        period52_high = df['high'].rolling(52).max()
        period52_low = df['low'].rolling(52).min()
        df['senkou_span_b'] = ((period52_high + period52_low) / 2).shift(26)

        # Williams %R
        df['williams_r'] = -100 * (high_14 - df['close']) / (high_14 - low_14)

        # CCI (Commodity Channel Index)
        tp = (df['high'] + df['low'] + df['close']) / 3
        df['cci'] = (tp - tp.rolling(20).mean()) / (0.015 * tp.rolling(20).std())

        # ROC (Rate of Change)
        df['roc'] = ((df['close'] - df['close'].shift(10)) / df['close'].shift(10)) * 100

        # MFI (Money Flow Index)
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        money_flow = typical_price * df['volume']

        positive_flow = []
        negative_flow = []
        for i in range(1, len(df)):
            if typical_price.iloc[i] > typical_price.iloc[i-1]:
                positive_flow.append(money_flow.iloc[i])
                negative_flow.append(0)
            elif typical_price.iloc[i] < typical_price.iloc[i-1]:
                positive_flow.append(0)
                negative_flow.append(money_flow.iloc[i])
            else:
                positive_flow.append(0)
                negative_flow.append(0)

        positive_flow = [0] + positive_flow
        negative_flow = [0] + negative_flow

        positive_mf = pd.Series(positive_flow).rolling(14).sum()
        negative_mf = pd.Series(negative_flow).rolling(14).sum()
        mfi = 100 - (100 / (1 + positive_mf / negative_mf))
        df['mfi'] = mfi

        return df
    
    def prepare_data(self, prices):
        """ML ëª¨ë¸ìš© ë°ì´í„° ì „ì²˜ë¦¬ - ì´ìƒì¹˜ ì œê±° ì¶”ê°€"""
        # í”¼ì²˜ ìƒì„±
        df = self.create_features(prices)

        # NaN ì œê±°
        df = df.dropna()

        # ì´ìƒì¹˜ ì œê±° (Z-score > 3)
        from scipy import stats
        z_scores = np.abs(stats.zscore(df['close']))
        df = df[(z_scores < 3)]

        if len(df) == 0:
            raise ValueError("ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

        logger.debug(f"ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„° í¬ì¸íŠ¸: {len(df)}ê°œ")
        
        if len(df) < self.sequence_length + 1:
            raise ValueError(f"ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ {self.sequence_length + 1}ê°œ í•„ìš”")
        
        # í”¼ì²˜ ì„ íƒ - ê³ ê¸‰ ì§€í‘œ í¬í•¨
        feature_columns = [
            # ê¸°ì¡´ ì§€í‘œ
            'ma5', 'ma10', 'ma20', 'ma50', 'ma200',
            'rsi', 'macd', 'macd_signal', 'macd_hist',
            'bb_upper', 'bb_lower', 'bb_width',
            'pct_change_1', 'pct_change_5', 'pct_change_10',
            'volatility',
            # ìƒˆë¡œìš´ ê³ ê¸‰ ì§€í‘œ
            'atr', 'adx', 'plus_di', 'minus_di',
            'stoch_k', 'stoch_d',
            'obv_ma', 'williams_r', 'cci', 'roc', 'mfi',
            'tenkan_sen', 'kijun_sen'
        ]
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        # âœ… ìˆ˜ì •: ë‹¤ìŒë‚  ë³€í™”ìœ¨(%)ì„ ì˜ˆì¸¡í•˜ë„ë¡ ë³€ê²½
        X, y = [], []
        for i in range(self.sequence_length, len(df) - 1):  # -1: ë‹¤ìŒë‚  ë°ì´í„°ê°€ ìˆì–´ì•¼ í•¨
            # ê³¼ê±° sequence_lengthê°œì˜ í”¼ì²˜ë“¤
            sequence_features = []
            for j in range(i - self.sequence_length, i):
                row_features = df[feature_columns].iloc[j].values
                sequence_features.extend(row_features)

            X.append(sequence_features)

            # âœ… íƒ€ê²Ÿ: ë‹¤ìŒë‚  ë³€í™”ìœ¨ (%)
            current_price = df['close'].iloc[i]
            next_price = df['close'].iloc[i + 1]
            price_change_pct = (next_price - current_price) / current_price * 100
            y.append(price_change_pct)

        return np.array(X), np.array(y)
    
    def train_models_with_cv(self, X, y):
        """Time Series Cross-Validationì„ ì‚¬ìš©í•œ ëª¨ë¸ í›ˆë ¨"""
        if not SKLEARN_AVAILABLE:
            return

        # Time Series Split ì„¤ì •
        tscv = TimeSeriesSplit(n_splits=5)

        # 1. Random Forest with CV
        if self.progress_callback:
            self.progress_callback('ml', 'Random Forest í•™ìŠµ ì¤‘ (1/3)...')

        rf_scores = []
        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            rf_model = RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1
            )
            rf_model.fit(X_train, y_train)
            rf_pred = rf_model.predict(X_val)
            rf_scores.append(np.sqrt(mean_squared_error(y_val, rf_pred)))

        # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í›ˆë ¨
        rf_final = RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1
        )
        rf_final.fit(X, y)
        self.models['random_forest'] = rf_final
        logger.info(f"Random Forest CV RMSE: {np.mean(rf_scores):.2f} (Â±{np.std(rf_scores):.2f})")

        # ëª¨ë¸ ì €ì¥
        if self.persistence and self.ticker:
            try:
                metadata = {'cv_rmse_mean': np.mean(rf_scores), 'cv_rmse_std': np.std(rf_scores)}
                self.persistence.save_sklearn_model(rf_final, self.ticker, 'random_forest', metadata, self.scaler)
                # ì €ì¥ ì§í›„, ì˜¤ë˜ëœ ë²„ì „ ì •ë¦¬(ìµœì‹  5ê°œë§Œ ìœ ì§€)
                self.persistence.delete_old_models(self.ticker, keep_latest=5)                
            except Exception as e:
                logger.warning(f"Random Forest ì €ì¥ ì‹¤íŒ¨: {e}")

        # 2. XGBoost with CV
        if XGBOOST_AVAILABLE:
            if self.progress_callback:
                self.progress_callback('ml', 'XGBoost í•™ìŠµ ì¤‘ (2/3)...')

            xgb_scores = []
            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                xgb_model = xgb.XGBRegressor(
                    n_estimators=150,  # 300 â†’ 150 (ì†ë„ 2ë°° í–¥ìƒ)
                    max_depth=8,
                    learning_rate=0.05,  # 0.03 â†’ 0.05 (learning rate ë†’ì—¬ì„œ ë³´ì™„)
                    min_child_weight=3,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    gamma=0.1,
                    reg_alpha=0.1,
                    reg_lambda=1.0,
                    random_state=42,
                    verbosity=0
                )
                xgb_model.fit(X_train, y_train)
                xgb_pred = xgb_model.predict(X_val)
                xgb_scores.append(np.sqrt(mean_squared_error(y_val, xgb_pred)))

            xgb_final = xgb.XGBRegressor(
                n_estimators=150,  # 300 â†’ 150
                max_depth=8,
                learning_rate=0.05,  # 0.03 â†’ 0.05
                min_child_weight=3,
                subsample=0.8,
                colsample_bytree=0.8,
                gamma=0.1,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=42,
                verbosity=0
            )
            xgb_final.fit(X, y)
            self.models['xgboost'] = xgb_final
            logger.info(f"XGBoost CV RMSE: {np.mean(xgb_scores):.2f} (Â±{np.std(xgb_scores):.2f})")

            # ëª¨ë¸ ì €ì¥
            if self.persistence and self.ticker:
                try:
                    metadata = {'cv_rmse_mean': np.mean(xgb_scores), 'cv_rmse_std': np.std(xgb_scores)}
                    self.persistence.save_sklearn_model(xgb_final, self.ticker, 'xgboost', metadata, self.scaler)
                    # ì €ì¥ ì§í›„, ì˜¤ë˜ëœ ë²„ì „ ì •ë¦¬(ìµœì‹  5ê°œë§Œ ìœ ì§€)
                    self.persistence.delete_old_models(self.ticker, keep_latest=5)
                except Exception as e:
                    logger.warning(f"XGBoost ì €ì¥ ì‹¤íŒ¨: {e}")

        # 3. LightGBM with CV
        if LIGHTGBM_AVAILABLE:
            if self.progress_callback:
                self.progress_callback('ml', 'LightGBM í•™ìŠµ ì¤‘ (3/3)...')

            lgb_scores = []
            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                lgb_model = lgb.LGBMRegressor(
                    n_estimators=300,
                    max_depth=8,
                    learning_rate=0.03,
                    num_leaves=31,
                    min_child_samples=20,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=0.1,
                    reg_lambda=1.0,
                    random_state=42,
                    verbosity=-1
                )
                lgb_model.fit(X_train, y_train)
                lgb_pred = lgb_model.predict(X_val)
                lgb_scores.append(np.sqrt(mean_squared_error(y_val, lgb_pred)))

            lgb_final = lgb.LGBMRegressor(
                n_estimators=300,
                max_depth=8,
                learning_rate=0.03,
                num_leaves=31,
                min_child_samples=20,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=42,
                verbosity=-1
            )
            lgb_final.fit(X, y)
            self.models['lightgbm'] = lgb_final
            logger.info(f"LightGBM CV RMSE: {np.mean(lgb_scores):.2f} (Â±{np.std(lgb_scores):.2f})")

            # ëª¨ë¸ ì €ì¥
            if self.persistence and self.ticker:
                try:
                    metadata = {'cv_rmse_mean': np.mean(lgb_scores), 'cv_rmse_std': np.std(lgb_scores)}
                    self.persistence.save_sklearn_model(lgb_final, self.ticker, 'lightgbm', metadata, self.scaler)
                    # ì €ì¥ ì§í›„, ì˜¤ë˜ëœ ë²„ì „ ì •ë¦¬(ìµœì‹  5ê°œë§Œ ìœ ì§€)
                    self.persistence.delete_old_models(self.ticker, keep_latest=5)
                except Exception as e:
                    logger.warning(f"LightGBM ì €ì¥ ì‹¤íŒ¨: {e}")

    def train_models(self, X_train, y_train, X_val, y_val):
        """ì—¬ëŸ¬ ML ëª¨ë¸ í›ˆë ¨ - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜µì…˜"""

        # 1. Random Forest
        if SKLEARN_AVAILABLE:
            if self.use_optimization:
                logger.info("Random Forest Bayesian Optimization ì‹¤í–‰...")
                rf_model = HyperparameterOptimizer.optimize_random_forest(X_train, y_train, n_iter=15)

            if not self.use_optimization or rf_model is None:
                rf_model = RandomForestRegressor(
                    n_estimators=200,
                    max_depth=15,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    max_features='sqrt',
                    random_state=42,
                    n_jobs=-1
                )
                rf_model.fit(X_train, y_train)

            self.models['random_forest'] = rf_model
            rf_pred = rf_model.predict(X_val)
            rf_score = np.sqrt(mean_squared_error(y_val, rf_pred))
            logger.info(f"Random Forest RMSE: {rf_score:.2f}")

        # 2. XGBoost
        if XGBOOST_AVAILABLE:
            if self.use_optimization:
                logger.info("XGBoost Bayesian Optimization ì‹¤í–‰...")
                xgb_model = HyperparameterOptimizer.optimize_xgboost(X_train, y_train, n_iter=15)

            if not self.use_optimization or xgb_model is None:
                xgb_model = xgb.XGBRegressor(
                    n_estimators=300,
                    max_depth=8,
                    learning_rate=0.03,
                    min_child_weight=3,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    gamma=0.1,
                    reg_alpha=0.1,
                    reg_lambda=1.0,
                    random_state=42,
                    verbosity=0
                )
                xgb_model.fit(X_train, y_train)

            self.models['xgboost'] = xgb_model
            xgb_pred = xgb_model.predict(X_val)
            xgb_score = np.sqrt(mean_squared_error(y_val, xgb_pred))
            logger.info(f"XGBoost RMSE: {xgb_score:.2f}")

        # 3. LightGBM
        if LIGHTGBM_AVAILABLE:
            if self.use_optimization:
                logger.info("LightGBM Bayesian Optimization ì‹¤í–‰...")
                lgb_model = HyperparameterOptimizer.optimize_lightgbm(X_train, y_train, n_iter=15)

            if not self.use_optimization or lgb_model is None:
                lgb_model = lgb.LGBMRegressor(
                    n_estimators=300,
                    max_depth=8,
                    learning_rate=0.03,
                    num_leaves=31,
                    min_child_samples=20,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=0.1,
                    reg_lambda=1.0,
                    random_state=42,
                    verbosity=-1
                )
                lgb_model.fit(X_train, y_train)

            self.models['lightgbm'] = lgb_model
            lgb_pred = lgb_model.predict(X_val)
            lgb_score = np.sqrt(mean_squared_error(y_val, lgb_pred))
            logger.info(f"LightGBM RMSE: {lgb_score:.2f}")
    
    def fit_predict(self, prices, forecast_days=5, use_cv=True):
        """ML ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡ - CV ì˜µì…˜ ì¶”ê°€"""
        if len(prices) < self.sequence_length + 20:
            raise ValueError(f"ìµœì†Œ {self.sequence_length + 20}ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤")

        # ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_data(prices)

        # Time Series Cross-Validation ì‚¬ìš© ì—¬ë¶€
        if use_cv and len(X) > 100:
            logger.info("Time Series Cross-Validation ì‚¬ìš©")
            self.train_models_with_cv(X, y)
        else:
            # í›ˆë ¨/ê²€ì¦ ë¶„í• 
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42, shuffle=False
            )

            # ëª¨ë¸ í›ˆë ¨
            self.train_models(X_train, y_train, X_val, y_val)
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        if not self.models:
            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ML ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ìµœê·¼ ì‹œí€€ìŠ¤ë¡œ ë¯¸ë˜ ì˜ˆì¸¡
        last_sequence = X[-1].reshape(1, -1)
        
        # âœ… ëª¨ë¸ ì˜ˆì¸¡: ë³€í™”ìœ¨(%)ì„ ì˜ˆì¸¡
        predictions_pct = {}
        for model_name, model in self.models.items():
            pred_pct = model.predict(last_sequence)[0]  # ë³€í™”ìœ¨(%)
            predictions_pct[model_name] = pred_pct
            logger.info(f"{model_name} ì˜ˆì¸¡ ë³€í™”ìœ¨: {pred_pct:+.2f}%")

        # ì•™ìƒë¸” (í‰ê·  ë³€í™”ìœ¨)
        ensemble_pct = np.mean(list(predictions_pct.values()))
        current_price = prices[-1]

        logger.info(f"í˜„ì¬ê°€: {current_price:.2f}, ì•™ìƒë¸” ì˜ˆì¸¡ ë³€í™”ìœ¨: {ensemble_pct:+.2f}%")

        # âœ… ë³€í™”ìœ¨ì„ ì ˆëŒ€ ê°€ê²©ìœ¼ë¡œ ë³€í™˜
        day1_pred = current_price * (1 + ensemble_pct / 100)

        # âœ… ì¬ê·€ì  ì˜ˆì¸¡: ë§¤ì¼ ê°™ì€ ë³€í™”ìœ¨ ì ìš© (ë‹¨ìˆœí™”)
        future_predictions = []
        future_predictions.append(day1_pred)

        # 2ì¼ì°¨ ì´í›„: ë³€í™”ìœ¨ì„ ë°˜ë³µ ì ìš© (ê°ì‡  ì ìš©)
        for i in range(1, forecast_days):
            # ë³€í™”ìœ¨ ê°ì‡ : ë©€ì–´ì§ˆìˆ˜ë¡ 0%ì— ìˆ˜ë ´
            decay = 0.9 ** i
            adjusted_pct = ensemble_pct * decay
            next_pred = current_price * (1 + adjusted_pct * (i + 1) / 100)
            future_predictions.append(next_pred)

        logger.info(f"ML ì˜ˆì¸¡ ì™„ë£Œ: 1ì¼ì°¨={future_predictions[0]:.2f} ({ensemble_pct:+.2f}%), "
                   f"ìµœì¢…={future_predictions[-1]:.2f} ({(future_predictions[-1]-current_price)/current_price*100:+.2f}%)")
        
        return {
            'future_predictions': np.array(future_predictions),
            'individual_predictions': predictions_pct,  # ë³€í™”ìœ¨ë¡œ ì €ì¥
            'ensemble_prediction_pct': ensemble_pct,  # ë³€í™”ìœ¨(%)
            'model_performance': {
                'models_used': list(self.models.keys()),
                'validation_score': 'calculated_above'
            }
        }

class LSTMPredictor:
    """LSTM ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì£¼ê°€ ì˜ˆì¸¡"""

    def __init__(self, sequence_length=60, units=128, ticker=None, auto_load=True):
        self.sequence_length = sequence_length
        self.units = units
        self.model = None
        self.scaler = MinMaxScaler()
        self.ticker = ticker
        self.persistence = None

        # ëª¨ë¸ ì €ì¥/ë¡œë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            from model_persistence import get_model_persistence
            self.persistence = get_model_persistence()

            # ìë™ ë¡œë“œ (í‹°ì»¤ê°€ ìˆê³ , auto_load=Trueì¸ ê²½ìš°)
            if auto_load and ticker:
                self._try_load_model()
        except ImportError:
            logger.debug("model_persistence ëª¨ë“ˆ ì—†ìŒ, ì €ì¥/ë¡œë“œ ë¹„í™œì„±í™”")

    def _try_load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œë„"""
        if not self.persistence or not self.ticker:
            return

        try:
            model, metadata, scaler = self.persistence.load_keras_model(self.ticker, 'lstm')
            if model is not None:
                self.model = model
                if scaler is not None:
                    self.scaler = scaler
                logger.info(f"âœ… ì €ì¥ëœ LSTM ëª¨ë¸ ë¡œë“œ: {self.ticker} (ë²„ì „: {metadata.get('version', 'unknown')})")
                return True
        except Exception as e:
            logger.debug(f"LSTM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ìƒˆë¡œ í›ˆë ¨): {e}")

        return False

    def build_model(self, input_shape):
        """LSTM ëª¨ë¸ êµ¬ì¶•"""
        # Lazy Import TensorFlow
        if not _lazy_import_tensorflow():
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤")

        keras = _tensorflow_modules['keras']
        layers = _tensorflow_modules['layers']

        model = keras.Sequential([
            layers.LSTM(self.units, return_sequences=True, input_shape=input_shape),
            layers.Dropout(0.2),
            layers.LSTM(self.units // 2, return_sequences=True),
            layers.Dropout(0.2),
            layers.LSTM(self.units // 4),
            layers.Dropout(0.2),
            layers.Dense(32, activation='relu'),
            layers.Dense(1)
        ])

        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )

        return model

    def prepare_sequences(self, data):
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ì¤€ë¹„"""
        scaled_data = self.scaler.fit_transform(data.reshape(-1, 1))

        X, y = [], []
        for i in range(self.sequence_length, len(scaled_data)):
            X.append(scaled_data[i-self.sequence_length:i, 0])
            y.append(scaled_data[i, 0])

        return np.array(X), np.array(y)

    def fit_predict(self, prices, forecast_days=5, force_retrain=False):
        """LSTM ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"""
        if not TENSORFLOW_AVAILABLE:
            logger.warning("TensorFlow ì—†ìŒ - LSTM ê±´ë„ˆëœ€")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback'}

        try:
            # ì €ì¥ëœ ëª¨ë¸ì´ ìˆê³  ì¬í›ˆë ¨ ê°•ì œê°€ ì•„ë‹ˆë©´ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰
            if self.model is not None and not force_retrain:
                logger.info("âœ… ê¸°ì¡´ LSTM ëª¨ë¸ ì‚¬ìš© (ì¬í›ˆë ¨ ì—†ìŒ)")
                return self._predict_only(prices, forecast_days)

            X, y = self.prepare_sequences(prices)

            if len(X) < 50:
                raise ValueError("LSTM í•™ìŠµì— ì¶©ë¶„í•œ ë°ì´í„° ì—†ìŒ")

            # í•™ìŠµ/ê²€ì¦ ë¶„í• 
            split = int(len(X) * 0.8)
            X_train, X_val = X[:split], X[split:]
            y_train, y_val = y[:split], y[split:]

            # 3D í˜•íƒœë¡œ reshape
            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))

            # ëª¨ë¸ êµ¬ì¶•
            self.model = self.build_model((X_train.shape[1], 1))

            # ì½œë°± ì„¤ì •
            EarlyStopping = _tensorflow_modules['EarlyStopping']
            ReduceLROnPlateau = _tensorflow_modules['ReduceLROnPlateau']
            early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  # patience 10 â†’ 15
            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)

            # í•™ìŠµ
            logger.info("ğŸ”„ LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=150,  # 100 â†’ 150
                batch_size=32,
                callbacks=[early_stop, reduce_lr],
                verbose=0
            )

            # ë¯¸ë˜ ì˜ˆì¸¡
            last_sequence = X[-1].reshape((1, self.sequence_length, 1))
            predictions = []

            for _ in range(forecast_days):
                pred = self.model.predict(last_sequence, verbose=0)[0, 0]
                predictions.append(pred)

                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                last_sequence = np.roll(last_sequence, -1, axis=1)
                last_sequence[0, -1, 0] = pred

            # ì—­ìŠ¤ì¼€ì¼ë§
            predictions = self.scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()

            # ëª¨ë¸ ì €ì¥
            if self.persistence and self.ticker:
                try:
                    metadata = {
                        'train_loss': history.history['loss'][-1],
                        'val_loss': history.history['val_loss'][-1],
                        'epochs_trained': len(history.history['loss']),
                        'sequence_length': self.sequence_length,
                        'units': self.units,
                        'data_size': len(prices)
                    }
                    self.persistence.save_keras_model(self.model, self.ticker, 'lstm', metadata, self.scaler)
                    # ì €ì¥ ì§í›„, ì˜¤ë˜ëœ ë²„ì „ ì •ë¦¬(ìµœì‹  5ê°œë§Œ ìœ ì§€)
                    self.persistence.delete_old_models(self.ticker, keep_latest=5)
                except Exception as e:
                    logger.warning(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

            return {
                'future_predictions': predictions,
                'model_type': 'LSTM',
                'train_loss': history.history['loss'][-1],
                'val_loss': history.history['val_loss'][-1]
            }

        except Exception as e:
            logger.error(f"LSTM ì‹¤íŒ¨: {e}")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback', 'error': str(e)}

    def _predict_only(self, prices, forecast_days):
        """ì €ì¥ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰ (ì¬í›ˆë ¨ ì—†ìŒ)"""
        try:
            X, y = self.prepare_sequences(prices)
            X = X.reshape((X.shape[0], X.shape[1], 1))

            # ë¯¸ë˜ ì˜ˆì¸¡
            last_sequence = X[-1].reshape((1, self.sequence_length, 1))
            predictions = []

            for _ in range(forecast_days):
                pred = self.model.predict(last_sequence, verbose=0)[0, 0]
                predictions.append(pred)

                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                last_sequence = np.roll(last_sequence, -1, axis=1)
                last_sequence[0, -1, 0] = pred

            # ì—­ìŠ¤ì¼€ì¼ë§
            predictions = self.scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()

            return {
                'future_predictions': predictions,
                'model_type': 'LSTM',
                'using_cached_model': True
            }

        except Exception as e:
            logger.error(f"LSTM ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback', 'error': str(e)}


class TransformerPredictor:
    """Transformer ëª¨ë¸ì„ ì‚¬ìš©í•œ ì£¼ê°€ ì˜ˆì¸¡"""

    def __init__(self, sequence_length=60, d_model=64, num_heads=4, num_layers=2, ticker=None, auto_load=True):
        self.sequence_length = sequence_length
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.model = None
        self.scaler = MinMaxScaler()
        self.ticker = ticker
        self.persistence = None

        # ëª¨ë¸ ì €ì¥/ë¡œë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            from model_persistence import get_model_persistence
            self.persistence = get_model_persistence()

            # ìë™ ë¡œë“œ
            if auto_load and ticker:
                self._try_load_model()
        except ImportError:
            logger.debug("model_persistence ëª¨ë“ˆ ì—†ìŒ, ì €ì¥/ë¡œë“œ ë¹„í™œì„±í™”")

    def _try_load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œë„"""
        if not self.persistence or not self.ticker:
            return

        try:
            model, metadata, scaler = self.persistence.load_keras_model(self.ticker, 'transformer')
            if model is not None:
                self.model = model
                if scaler is not None:
                    self.scaler = scaler
                logger.info(f"âœ… ì €ì¥ëœ Transformer ëª¨ë¸ ë¡œë“œ: {self.ticker} (ë²„ì „: {metadata.get('version', 'unknown')})")
                return True
        except Exception as e:
            logger.debug(f"Transformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ìƒˆë¡œ í›ˆë ¨): {e}")

        return False

    def transformer_encoder(self, inputs, head_size, num_heads, ff_dim, dropout=0.1):
        """Transformer Encoder Block"""
        layers = _tensorflow_modules['layers']

        # Multi-Head Attention
        x = layers.MultiHeadAttention(
            key_dim=head_size, num_heads=num_heads, dropout=dropout
        )(inputs, inputs)
        x = layers.Dropout(dropout)(x)
        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)

        # Feed Forward
        ff = layers.Dense(ff_dim, activation="relu")(x)
        ff = layers.Dropout(dropout)(ff)
        ff = layers.Dense(inputs.shape[-1])(ff)
        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)

        return x

    def build_model(self, input_shape):
        """Transformer ëª¨ë¸ êµ¬ì¶•"""
        # Lazy Import TensorFlow
        if not _lazy_import_tensorflow():
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤")

        keras = _tensorflow_modules['keras']
        layers = _tensorflow_modules['layers']

        inputs = keras.Input(shape=input_shape)

        # Positional Encoding
        x = layers.Dense(self.d_model)(inputs)

        # Transformer Encoder Blocks
        for _ in range(self.num_layers):
            x = self.transformer_encoder(
                x,
                head_size=self.d_model // self.num_heads,
                num_heads=self.num_heads,
                ff_dim=self.d_model * 4,
                dropout=0.1
            )

        # Output layers
        x = layers.GlobalAveragePooling1D()(x)
        x = layers.Dropout(0.1)(x)
        x = layers.Dense(64, activation="relu")(x)
        x = layers.Dropout(0.1)(x)
        outputs = layers.Dense(1)(x)

        model = keras.Model(inputs=inputs, outputs=outputs)
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )

        return model

    def prepare_sequences(self, data):
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ì¤€ë¹„"""
        scaled_data = self.scaler.fit_transform(data.reshape(-1, 1))

        X, y = [], []
        for i in range(self.sequence_length, len(scaled_data)):
            X.append(scaled_data[i-self.sequence_length:i, 0])
            y.append(scaled_data[i, 0])

        return np.array(X), np.array(y)

    def fit_predict(self, prices, forecast_days=5, force_retrain=False):
        """Transformer ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"""
        if not TENSORFLOW_AVAILABLE:
            logger.warning("TensorFlow ì—†ìŒ - Transformer ê±´ë„ˆëœ€")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback'}

        try:
            # ì €ì¥ëœ ëª¨ë¸ì´ ìˆê³  ì¬í›ˆë ¨ ê°•ì œê°€ ì•„ë‹ˆë©´ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰
            if self.model is not None and not force_retrain:
                logger.info("âœ… ê¸°ì¡´ Transformer ëª¨ë¸ ì‚¬ìš© (ì¬í›ˆë ¨ ì—†ìŒ)")
                return self._predict_only(prices, forecast_days)

            X, y = self.prepare_sequences(prices)

            if len(X) < 50:
                raise ValueError("Transformer í•™ìŠµì— ì¶©ë¶„í•œ ë°ì´í„° ì—†ìŒ")

            # í•™ìŠµ/ê²€ì¦ ë¶„í• 
            split = int(len(X) * 0.8)
            X_train, X_val = X[:split], X[split:]
            y_train, y_val = y[:split], y[split:]

            # 3D í˜•íƒœë¡œ reshape
            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))

            # ëª¨ë¸ êµ¬ì¶•
            self.model = self.build_model((X_train.shape[1], 1))

            # ì½œë°± ì„¤ì •
            EarlyStopping = _tensorflow_modules['EarlyStopping']
            ReduceLROnPlateau = _tensorflow_modules['ReduceLROnPlateau']
            early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)  # patience 15 â†’ 20
            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)

            # í•™ìŠµ
            logger.info("ğŸ”„ Transformer ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=150,  # 100 â†’ 150
                batch_size=32,
                callbacks=[early_stop, reduce_lr],
                verbose=0
            )

            # ë¯¸ë˜ ì˜ˆì¸¡
            last_sequence = X[-1].reshape((1, self.sequence_length, 1))
            predictions = []

            for _ in range(forecast_days):
                pred = self.model.predict(last_sequence, verbose=0)[0, 0]
                predictions.append(pred)

                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                last_sequence = np.roll(last_sequence, -1, axis=1)
                last_sequence[0, -1, 0] = pred

            # ì—­ìŠ¤ì¼€ì¼ë§
            predictions = self.scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()

            # ëª¨ë¸ ì €ì¥
            if self.persistence and self.ticker:
                try:
                    metadata = {
                        'train_loss': history.history['loss'][-1],
                        'val_loss': history.history['val_loss'][-1],
                        'epochs_trained': len(history.history['loss']),
                        'sequence_length': self.sequence_length,
                        'd_model': self.d_model,
                        'num_heads': self.num_heads,
                        'num_layers': self.num_layers,
                        'data_size': len(prices)
                    }
                    self.persistence.save_keras_model(self.model, self.ticker, 'transformer', metadata, self.scaler)
                    # ì €ì¥ ì§í›„, ì˜¤ë˜ëœ ë²„ì „ ì •ë¦¬(ìµœì‹  5ê°œë§Œ ìœ ì§€)
                    self.persistence.delete_old_models(self.ticker, keep_latest=5)
                except Exception as e:
                    logger.warning(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

            return {
                'future_predictions': predictions,
                'model_type': 'Transformer',
                'train_loss': history.history['loss'][-1],
                'val_loss': history.history['val_loss'][-1]
            }

        except Exception as e:
            logger.error(f"Transformer ì‹¤íŒ¨: {e}")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback', 'error': str(e)}

    def _predict_only(self, prices, forecast_days):
        """ì €ì¥ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰ (ì¬í›ˆë ¨ ì—†ìŒ)"""
        try:
            X, y = self.prepare_sequences(prices)
            X = X.reshape((X.shape[0], X.shape[1], 1))

            # ë¯¸ë˜ ì˜ˆì¸¡
            last_sequence = X[-1].reshape((1, self.sequence_length, 1))
            predictions = []

            for _ in range(forecast_days):
                pred = self.model.predict(last_sequence, verbose=0)[0, 0]
                predictions.append(pred)

                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                last_sequence = np.roll(last_sequence, -1, axis=1)
                last_sequence[0, -1, 0] = pred

            # ì—­ìŠ¤ì¼€ì¼ë§
            predictions = self.scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()

            return {
                'future_predictions': predictions,
                'model_type': 'Transformer',
                'using_cached_model': True
            }

        except Exception as e:
            logger.error(f"Transformer ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {'future_predictions': np.full(forecast_days, prices[-1]), 'method': 'fallback', 'error': str(e)}


class ARIMAPredictor:
    """ARIMA ëª¨ë¸ì„ ì‚¬ìš©í•œ ì£¼ê°€ ì˜ˆì¸¡"""
    
    def __init__(self, order=(1,1,1)):
        self.order = order
        self.model = None
    
    def fit_predict(self, prices, forecast_days=5):
        """ARIMA ëª¨ë¸ í”¼íŒ… ë° ì˜ˆì¸¡"""
        if not STATSMODELS_AVAILABLE:
            # âœ… ARIMA ëŒ€ì²´: ì´ë™í‰ê·  ê¸°ë°˜ ì¶”ì„¸
            ma_window = min(10, len(prices) // 2)
            if ma_window < 2:
                ma_window = 2
            trend = np.mean(np.diff(prices[-ma_window:]))
            last_price = prices[-1]

            future_predictions = []
            for i in range(forecast_days):
                future_pred = last_price + trend * (i + 1)
                future_predictions.append(future_pred)

            return {
                'future_predictions': np.array(future_predictions),
                'method': 'moving_average_fallback',
                'trend': trend
            }
        
        try:
            # ARIMA ëª¨ë¸ í”¼íŒ…
            model = ARIMA(prices, order=self.order)
            self.model = model.fit()
            
            # ì˜ˆì¸¡
            forecast_result = self.model.forecast(steps=forecast_days)
            confidence_intervals = self.model.get_forecast(steps=forecast_days).conf_int()
            
            return {
                'future_predictions': forecast_result.values if hasattr(forecast_result, 'values') else forecast_result,
                'confidence_intervals': confidence_intervals.values if hasattr(confidence_intervals, 'values') else confidence_intervals,
                'aic': self.model.aic,
                'bic': self.model.bic
            }
        
        except Exception as e:
            logger.error(f"ARIMA ëª¨ë¸ ì˜¤ë¥˜: {e}")
            # âœ… ARIMA ì‹¤íŒ¨ì‹œ ì„ í˜• ì¶”ì„¸ ì™¸ì‚½
            trend = np.mean(np.diff(prices[-10:]))
            last_price = prices[-1]

            future_predictions = []
            for i in range(forecast_days):
                future_pred = last_price + trend * (i + 1)
                future_predictions.append(future_pred)

            return {
                'future_predictions': np.array(future_predictions),
                'method': 'linear_trend_fallback',
                'error': str(e)
            }

class MarketCorrelationAnalyzer:
    """ì‹œì¥ ì§€ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„"""

    @staticmethod
    def get_market_indices(ticker):
        """ì¢…ëª©ì— ë§ëŠ” ì‹œì¥ ì§€ìˆ˜ ì„ íƒ"""
        if ticker.endswith('.KS') or ticker.endswith('.KQ'):
            # í•œêµ­ ì¢…ëª©
            return {
                'KOSPI': '^KS11',
                'KOSDAQ': '^KQ11'
            }
        else:
            # ë¯¸êµ­ ì¢…ëª©
            return {
                'S&P500': '^GSPC',
                'Nasdaq': '^IXIC',
                'Dow': '^DJI'
            }

    @staticmethod
    def calculate_correlation(stock_symbol, period='1y'):
        """ì‹œì¥ ì§€ìˆ˜ì™€ ìƒê´€ê³„ìˆ˜ ê³„ì‚°"""
        try:
            # ì¢…ëª© ë°ì´í„°
            stock_data = get_stock_data(stock_symbol, period=period)
            if stock_data is None or len(stock_data) < 30:
                return {}

            # ì§€ìˆ˜ ë°ì´í„°
            indices = MarketCorrelationAnalyzer.get_market_indices(stock_symbol)
            correlations = {}

            for index_name, index_symbol in indices.items():
                try:
                    index_data = get_stock_data(index_symbol, period=period)
                    if index_data is not None and len(index_data) >= 30:
                        # ë‚ ì§œ ë§ì¶”ê¸°
                        merged = stock_data['Close'].to_frame().join(
                            index_data['Close'].to_frame(),
                            how='inner',
                            rsuffix='_index'
                        )

                        if len(merged) >= 30:
                            corr = merged.iloc[:, 0].corr(merged.iloc[:, 1])
                            correlations[index_name] = corr
                except Exception as e:
                    logger.warning(f"{index_name} ìƒê´€ê´€ê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")

            return correlations

        except Exception as e:
            logger.error(f"ì‹œì¥ ìƒê´€ê´€ê³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}


class SectorAnalyzer:
    """ì„¹í„°/ì‚°ì—… ë™í–¥ ë¶„ì„"""

    SECTOR_ETFS = {
        'Technology': 'XLK',
        'Healthcare': 'XLV',
        'Financial': 'XLF',
        'Energy': 'XLE',
        'Consumer Discretionary': 'XLY',
        'Consumer Staples': 'XLP',
        'Industrials': 'XLI',
        'Materials': 'XLB',
        'Real Estate': 'XLRE',
        'Utilities': 'XLU',
        'Communication': 'XLC'
    }

    @staticmethod
    def get_sector_performance(period='1mo'):
        """ì„¹í„°ë³„ ì„±ê³¼ ë¶„ì„"""
        sector_performance = {}

        for sector, etf in SectorAnalyzer.SECTOR_ETFS.items():
            try:
                data = get_stock_data(etf, period=period)
                if data is not None and len(data) >= 2:
                    start_price = data['Close'].iloc[0]
                    end_price = data['Close'].iloc[-1]
                    performance = ((end_price - start_price) / start_price) * 100
                    sector_performance[sector] = performance
            except Exception as e:
                logger.warning(f"{sector} ì„±ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")

        return sector_performance

    @staticmethod
    def compare_with_sector(stock_symbol, sector_etf, period='1y'):
        """ì¢…ëª©ê³¼ ì„¹í„° ETF ë¹„êµ"""
        try:
            stock_data = get_stock_data(stock_symbol, period=period)
            sector_data = get_stock_data(sector_etf, period=period)

            if stock_data is None or sector_data is None:
                return None

            # ìƒëŒ€ ì„±ê³¼
            stock_return = ((stock_data['Close'].iloc[-1] - stock_data['Close'].iloc[0])
                           / stock_data['Close'].iloc[0]) * 100
            sector_return = ((sector_data['Close'].iloc[-1] - sector_data['Close'].iloc[0])
                            / sector_data['Close'].iloc[0]) * 100

            return {
                'stock_return': stock_return,
                'sector_return': sector_return,
                'outperformance': stock_return - sector_return
            }
        except Exception as e:
            logger.error(f"ì„¹í„° ë¹„êµ ì˜¤ë¥˜: {e}")
            return None


class InstitutionalFlowAnalyzer:
    """ì™¸êµ­ì¸/ê¸°ê´€ ë§¤ë§¤ ë™í–¥ ë¶„ì„ (í•œêµ­ ì¢…ëª©)"""

    @staticmethod
    def is_korean_stock(ticker):
        """í•œêµ­ ì¢…ëª© ì—¬ë¶€ í™•ì¸"""
        return ticker.endswith('.KS') or ticker.endswith('.KQ')

    @staticmethod
    def fetch_institutional_data(ticker):
        """ì™¸êµ­ì¸/ê¸°ê´€ ë§¤ë§¤ ë°ì´í„° ìˆ˜ì§‘"""
        if not InstitutionalFlowAnalyzer.is_korean_stock(ticker):
            return None

        try:
            # FinanceDataReader ì‚¬ìš© (ì„¤ì¹˜ í•„ìš”: pip install finance-datareader)
            import FinanceDataReader as fdr
            from datetime import datetime, timedelta

            # ìµœê·¼ 60ì¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì—¬ìœ ìˆê²Œ 30ì¼ ì´ìƒ í™•ë³´)
            end_date = datetime.now()
            start_date = end_date - timedelta(days=60)

            df = fdr.DataReader(ticker, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))

            if df is not None and len(df) > 0 and 'ForeignBuy' in df.columns and 'InstitutionBuy' in df.columns:
                # ìµœê·¼ 30ì˜ì—…ì¼ ë°ì´í„° ì‚¬ìš©
                recent_30d = df.tail(30)

                return {
                    'foreign_net_buy': recent_30d['ForeignBuy'].sum() - recent_30d['ForeignSell'].sum(),
                    'institution_net_buy': recent_30d['InstitutionBuy'].sum() - recent_30d['InstitutionSell'].sum(),
                    'foreign_ownership': recent_30d['ForeignOwnership'].iloc[-1] if 'ForeignOwnership' in df.columns else None
                }
            else:
                return None

        except ImportError:
            logger.warning("FinanceDataReader ì„¤ì¹˜ í•„ìš”: pip install finance-datareader")
            return None
        except Exception as e:
            logger.error(f"ì™¸êµ­ì¸/ê¸°ê´€ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return None


class MarketRegimeDetector:
    """ì‹œì¥ ìƒí™©(ìƒìŠ¹ì¥/í•˜ë½ì¥/íš¡ë³´ì¥) ê°ì§€ ì‹œìŠ¤í…œ"""

    @staticmethod
    def detect_regime(prices, window=50):
        """
        ì‹œì¥ ìƒí™© ê°ì§€
        Returns: 'bull' (ìƒìŠ¹ì¥), 'bear' (í•˜ë½ì¥), 'sideways' (íš¡ë³´ì¥)
        """
        if len(prices) < window:
            return 'sideways'

        recent_prices = prices[-window:]

        # ì¶”ì„¸ ë¶„ì„
        trend = np.polyfit(range(len(recent_prices)), recent_prices, 1)[0]
        avg_price = np.mean(recent_prices)
        trend_pct = (trend / avg_price) * 100

        # ë³€ë™ì„± ë¶„ì„
        volatility = np.std(recent_prices) / np.mean(recent_prices)

        # ìƒìŠ¹/í•˜ë½ ì¼ìˆ˜ ë¹„ìœ¨
        price_changes = np.diff(recent_prices)
        up_days = np.sum(price_changes > 0)
        down_days = np.sum(price_changes < 0)
        up_ratio = up_days / len(price_changes) if len(price_changes) > 0 else 0.5

        # ì‹œì¥ ìƒí™© íŒë‹¨
        if trend_pct > 0.5 and up_ratio > 0.55:
            regime = 'bull'  # ìƒìŠ¹ì¥
        elif trend_pct < -0.5 and up_ratio < 0.45:
            regime = 'bear'  # í•˜ë½ì¥
        else:
            regime = 'sideways'  # íš¡ë³´ì¥

        logger.info(f"ì‹œì¥ ìƒí™©: {regime} (ì¶”ì„¸: {trend_pct:.2f}%, ë³€ë™ì„±: {volatility:.2%}, ìƒìŠ¹ë¹„ìœ¨: {up_ratio:.1%})")

        return regime

    @staticmethod
    def get_regime_weights(regime):
        """ì‹œì¥ ìƒí™©ë³„ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë°˜í™˜"""
        weights = {
            'bull': {  # ìƒìŠ¹ì¥: íŠ¸ë Œë“œ ì¶”ì¢… ëª¨ë¸ ê°•í™”
                'kalman': 0.20,
                'ml_models': 0.40,
                'arima': 0.15,
                'lstm': 0.15,
                'transformer': 0.10
            },
            'bear': {  # í•˜ë½ì¥: ì•ˆì •ì ì¸ ëª¨ë¸ ê°•í™”
                'kalman': 0.30,
                'ml_models': 0.25,
                'arima': 0.25,
                'lstm': 0.10,
                'transformer': 0.10
            },
            'sideways': {  # íš¡ë³´ì¥: ê· í˜•ì¡íŒ ê°€ì¤‘ì¹˜
                'kalman': 0.20,
                'ml_models': 0.25,
                'arima': 0.25,
                'lstm': 0.15,
                'transformer': 0.10
            }
        }

        return weights.get(regime, weights['sideways'])


class EnsemblePredictor:
    """ì—¬ëŸ¬ ëª¨ë¸ì„ ê²°í•©í•œ ì•™ìƒë¸” ì˜ˆì¸¡ê¸° - ë™ì  ê°€ì¤‘ì¹˜ + ì‹œì¥ ìƒí™© ì¸ì‹"""

    def __init__(self, use_deep_learning=False, use_optimization=False, ticker=None):
        self.ticker = ticker
        self.kalman = KalmanFilterPredictor()
        self.ml_predictor = AdvancedMLPredictor(use_optimization=use_optimization, ticker=ticker) if (SKLEARN_AVAILABLE or XGBOOST_AVAILABLE or LIGHTGBM_AVAILABLE) else None
        self.arima = ARIMAPredictor()

        # ë”¥ëŸ¬ë‹ ëª¨ë¸ (ì˜µì…˜)
        self.use_deep_learning = use_deep_learning and TENSORFLOW_AVAILABLE
        if self.use_deep_learning:
            self.lstm = LSTMPredictor(ticker=ticker)
            self.transformer = TransformerPredictor(ticker=ticker)
        else:
            self.lstm = None
            self.transformer = None

        # ì´ˆê¸° ê°€ì¤‘ì¹˜ (ë™ì ìœ¼ë¡œ ì¡°ì •ë¨)
        self.weights = {
            'kalman': 0.25,
            'ml_models': 0.40 if self.ml_predictor else 0,
            'arima': 0.25 if self.ml_predictor else 0.75,
            'lstm': 0.05 if self.use_deep_learning else 0,
            'transformer': 0.05 if self.use_deep_learning else 0
        }

        # ëª¨ë¸ ì„±ëŠ¥ ì¶”ì 
        self.performance_history = {
            'kalman': [],
            'ml_models': [],
            'arima': [],
            'lstm': [],
            'transformer': []
        }

        # âœ… ì§„í–‰ ìƒíƒœ ì½œë°±
        self.progress_callback = None

        # ì‹œì¥ ìƒí™©
        self.current_regime = 'sideways'

    def update_weights_dynamically(self, validation_errors):
        """ê²€ì¦ ì˜¤ë¥˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •"""
        # ì—­ì˜¤ë¥˜ ê°€ì¤‘ì¹˜: ì˜¤ë¥˜ê°€ ì‘ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
        inverse_errors = {}
        total_inverse = 0

        for model_name, error in validation_errors.items():
            if error > 0:
                inverse_errors[model_name] = 1.0 / error
                total_inverse += inverse_errors[model_name]

        # ì •ê·œí™”í•˜ì—¬ ê°€ì¤‘ì¹˜ ê³„ì‚°
        if total_inverse > 0:
            for model_name in self.weights.keys():
                if model_name in inverse_errors:
                    self.weights[model_name] = inverse_errors[model_name] / total_inverse
                else:
                    self.weights[model_name] = 0

        logger.info(f"ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸: {self.weights}")
    
    def fit_predict(self, prices, forecast_days=5):
        """ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰ - ë™ì  ê°€ì¤‘ì¹˜ + ì‹œì¥ ìƒí™© ì¸ì‹"""
        results = {}
        predictions = []
        validation_errors = {}

        logger.info("ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘...")

        # ì‹œì¥ ìƒí™© ê°ì§€
        self.current_regime = MarketRegimeDetector.detect_regime(prices)
        regime_weights = MarketRegimeDetector.get_regime_weights(self.current_regime)

        # ê²€ì¦ ì„¸íŠ¸ ë¶„ë¦¬ (ë§ˆì§€ë§‰ 10% ì‚¬ìš©)
        split_point = int(len(prices) * 0.9)
        train_prices = prices[:split_point]
        val_prices = prices[split_point:]

        # 1. Kalman Filter ì˜ˆì¸¡
        if self.progress_callback:
            self.progress_callback('kalman', 'Kalman Filter ì˜ˆì¸¡ ì¤‘...')
        logger.debug("Kalman Filter ì‹¤í–‰ ì¤‘...")
        try:
            kalman_result = self.kalman.fit_predict(train_prices, len(val_prices))
            results['kalman'] = self.kalman.fit_predict(prices, forecast_days)
            predictions.append(results['kalman']['future_predictions'])

            # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
            kalman_preds = results['kalman']['future_predictions']
            logger.info(f"Kalman ì˜ˆì¸¡: 1ì¼ì°¨={kalman_preds[0]:.2f} ({(kalman_preds[0]-prices[-1])/prices[-1]*100:+.2f}%), "
                       f"ìµœì¢…={kalman_preds[-1]:.2f} ({(kalman_preds[-1]-prices[-1])/prices[-1]*100:+.2f}%)")

            # ê²€ì¦ ì˜¤ë¥˜ ê³„ì‚°
            kalman_val_error = np.mean(np.abs(kalman_result['future_predictions'][:len(val_prices)] - val_prices))
            validation_errors['kalman'] = kalman_val_error
            logger.debug(f"Kalman ê²€ì¦ MAE: {kalman_val_error:.2f}")
        except Exception as e:
            logger.warning(f"Kalman ì‹¤íŒ¨: {e}")
            validation_errors['kalman'] = float('inf')

        # 2. ML ëª¨ë¸ ì˜ˆì¸¡ (XGBoost, LightGBM, Random Forest)
        if self.ml_predictor and len(prices) >= 50:
            if self.progress_callback:
                self.progress_callback('ml', 'ML ëª¨ë¸ ê²€ì¦ ë°ì´í„° í•™ìŠµ ì¤‘...')

            # ML ì˜ˆì¸¡ê¸°ì— ì§„í–‰ ì½œë°± ì „ë‹¬
            if hasattr(self.ml_predictor, 'progress_callback'):
                self.ml_predictor.progress_callback = self.progress_callback

            logger.debug("ML ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘...")
            try:
                ml_val_result = self.ml_predictor.fit_predict(train_prices, len(val_prices))

                if self.progress_callback:
                    self.progress_callback('ml', 'ML ëª¨ë¸ ì „ì²´ ë°ì´í„° í•™ìŠµ ì¤‘...')

                ml_result = self.ml_predictor.fit_predict(prices, forecast_days)
                results['ml_models'] = ml_result
                predictions.append(ml_result['future_predictions'])

                # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
                ml_preds = ml_result['future_predictions']
                logger.info(f"ML ì•™ìƒë¸” ì˜ˆì¸¡: 1ì¼ì°¨={ml_preds[0]:.2f} ({(ml_preds[0]-prices[-1])/prices[-1]*100:+.2f}%), "
                           f"ìµœì¢…={ml_preds[-1]:.2f} ({(ml_preds[-1]-prices[-1])/prices[-1]*100:+.2f}%)")

                # ê²€ì¦ ì˜¤ë¥˜ ê³„ì‚°
                ml_val_error = np.mean(np.abs(ml_val_result['future_predictions'][:len(val_prices)] - val_prices))
                validation_errors['ml_models'] = ml_val_error
                logger.debug(f"ML ê²€ì¦ MAE: {ml_val_error:.2f}")
            except Exception as e:
                logger.warning(f"ML ëª¨ë¸ ì‹¤íŒ¨: {e}")
                validation_errors['ml_models'] = float('inf')

        # 3. ARIMA ì˜ˆì¸¡
        if self.progress_callback:
            self.progress_callback('arima', 'ARIMA ëª¨ë¸ ì‹¤í–‰ ì¤‘...')
        logger.debug("ARIMA ëª¨ë¸ í”¼íŒ… ì¤‘...")
        try:
            arima_val_result = self.arima.fit_predict(train_prices, len(val_prices))
            arima_result = self.arima.fit_predict(prices, forecast_days)
            results['arima'] = arima_result
            predictions.append(arima_result['future_predictions'])

            # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
            arima_preds = arima_result['future_predictions']
            logger.info(f"ARIMA ì˜ˆì¸¡: 1ì¼ì°¨={arima_preds[0]:.2f} ({(arima_preds[0]-prices[-1])/prices[-1]*100:+.2f}%), "
                       f"ìµœì¢…={arima_preds[-1]:.2f} ({(arima_preds[-1]-prices[-1])/prices[-1]*100:+.2f}%)")

            # ê²€ì¦ ì˜¤ë¥˜ ê³„ì‚°
            arima_val_error = np.mean(np.abs(arima_val_result['future_predictions'][:len(val_prices)] - val_prices))
            validation_errors['arima'] = arima_val_error
            logger.debug(f"ARIMA ê²€ì¦ MAE: {arima_val_error:.2f}")
        except Exception as e:
            logger.warning(f"ARIMA ì‹¤íŒ¨: {e}")
            validation_errors['arima'] = float('inf')

        # 4. LSTM ì˜ˆì¸¡ (ë”¥ëŸ¬ë‹ ëª¨ë“œ)
        if self.use_deep_learning and self.lstm and len(prices) >= 100:
            if self.progress_callback:
                self.progress_callback('lstm', 'LSTM ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì¤‘...')
            logger.debug("LSTM ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            try:
                lstm_result = self.lstm.fit_predict(prices, forecast_days)
                if 'error' not in lstm_result:
                    results['lstm'] = lstm_result
                    predictions.append(lstm_result['future_predictions'])

                    # ê²€ì¦ ì˜¤ë¥˜ (val_loss ì‚¬ìš©)
                    validation_errors['lstm'] = lstm_result.get('val_loss', float('inf'))
                    logger.debug(f"LSTM ê²€ì¦ Loss: {validation_errors['lstm']:.4f}")
            except Exception as e:
                logger.warning(f"LSTM ì‹¤íŒ¨: {e}")
                validation_errors['lstm'] = float('inf')

        # 5. Transformer ì˜ˆì¸¡ (ë”¥ëŸ¬ë‹ ëª¨ë“œ)
        if self.use_deep_learning and self.transformer and len(prices) >= 100:
            if self.progress_callback:
                self.progress_callback('transformer', 'Transformer ëª¨ë¸ í›ˆë ¨ ì¤‘...')
            logger.debug("Transformer ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            try:
                transformer_result = self.transformer.fit_predict(prices, forecast_days)
                if 'error' not in transformer_result:
                    results['transformer'] = transformer_result
                    predictions.append(transformer_result['future_predictions'])

                    # ê²€ì¦ ì˜¤ë¥˜ (val_loss ì‚¬ìš©)
                    validation_errors['transformer'] = transformer_result.get('val_loss', float('inf'))
                    logger.debug(f"Transformer ê²€ì¦ Loss: {validation_errors['transformer']:.4f}")
            except Exception as e:
                logger.warning(f"Transformer ì‹¤íŒ¨: {e}")
                validation_errors['transformer'] = float('inf')

        # 6. ê°€ì¤‘ì¹˜ ê²°ì •: ì‹œì¥ ìƒí™© ê¸°ë°˜ + ë™ì  ì¡°ì •
        if self.progress_callback:
            self.progress_callback('ensemble', 'ëª¨ë¸ ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘...')

        valid_errors = {k: v for k, v in validation_errors.items() if v != float('inf')}
        if valid_errors:
            # ì„±ëŠ¥ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜
            self.update_weights_dynamically(valid_errors)

            # ì‹œì¥ ìƒí™© ê°€ì¤‘ì¹˜ì™€ í˜¼í•© (70% ì„±ëŠ¥, 30% ì‹œì¥ìƒí™©)
            for model_name in self.weights.keys():
                if model_name in regime_weights:
                    self.weights[model_name] = (
                        0.7 * self.weights[model_name] +
                        0.3 * regime_weights[model_name]
                    )

            logger.info(f"ìµœì¢… ê°€ì¤‘ì¹˜ (ì‹œì¥ìƒí™©: {self.current_regime}): {self.weights}")

        # 7. ì•™ìƒë¸” ê²°í•©
        if self.progress_callback:
            self.progress_callback('ensemble', 'ìµœì¢… ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„± ì¤‘...')

        if len(predictions) > 1:
            # ê°€ì¤‘í‰ê·  ê³„ì‚°
            weighted_predictions = np.zeros(forecast_days)
            total_weight = 0

            model_order = ['kalman', 'ml_models', 'arima', 'lstm', 'transformer']
            active_models = [m for m in model_order if m in results]

            for i, model_name in enumerate(active_models):
                if self.weights.get(model_name, 0) > 0:
                    weighted_predictions += predictions[i] * self.weights[model_name]
                    total_weight += self.weights[model_name]

            ensemble_predictions = weighted_predictions / total_weight if total_weight > 0 else predictions[0]
        else:
            ensemble_predictions = predictions[0]

        # ìµœì¢… ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
        logger.info(f"âœ… ìµœì¢… ì•™ìƒë¸” ì˜ˆì¸¡: 1ì¼ì°¨={ensemble_predictions[0]:.2f} ({(ensemble_predictions[0]-prices[-1])/prices[-1]*100:+.2f}%), "
                   f"ìµœì¢…ì¼={ensemble_predictions[-1]:.2f} ({(ensemble_predictions[-1]-prices[-1])/prices[-1]*100:+.2f}%)")

        # âœ… ì‹ ë¢°ë„ ê³„ì‚°: ê²€ì¦ ì˜¤ë¥˜ ê¸°ë°˜ (ë‚®ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ)
        if len(valid_errors) > 0:
            # í‰ê·  ê²€ì¦ ì˜¤ë¥˜ë¥¼ ì‹ ë¢°ë„ë¡œ ë³€í™˜
            avg_error = np.mean(list(valid_errors.values()))
            avg_price = np.mean(prices[-30:])  # ìµœê·¼ 30ì¼ í‰ê· ê°€

            # ì˜¤ë¥˜ìœ¨ ê³„ì‚° (ì˜¤ë¥˜ / í‰ê· ê°€)
            error_rate = avg_error / avg_price if avg_price > 0 else 1.0

            # ì‹ ë¢°ë„: ì˜¤ë¥˜ìœ¨ì´ 5% ë¯¸ë§Œì´ë©´ ë†’ìŒ, 20% ì´ìƒì´ë©´ ë‚®ìŒ
            if error_rate < 0.05:
                confidence_score = 0.8 + (0.05 - error_rate) / 0.05 * 0.2  # 0.8~1.0
            elif error_rate < 0.20:
                confidence_score = 0.5 + (0.20 - error_rate) / 0.15 * 0.3  # 0.5~0.8
            else:
                confidence_score = max(0.3, 0.5 - (error_rate - 0.20) * 0.5)  # 0.3~0.5

            logger.info(f"ê²€ì¦ ì˜¤ë¥˜ìœ¨: {error_rate*100:.2f}%, ì‹ ë¢°ë„: {confidence_score*100:.1f}%")
        else:
            confidence_score = 0.5
            logger.warning("ê²€ì¦ ì˜¤ë¥˜ ì •ë³´ ì—†ìŒ, ê¸°ë³¸ ì‹ ë¢°ë„ 50% ì ìš©")

        return {
            'ensemble_predictions': ensemble_predictions,
            'individual_results': results,
            'confidence_score': confidence_score,
            'model_weights': self.weights,
            'prediction_variance': np.var(predictions, axis=0) if len(predictions) > 1 else np.zeros(forecast_days),
            'validation_errors': validation_errors,
            'market_regime': self.current_regime
        }

class StockPredictor:
    """í†µí•© ì£¼ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - ë”¥ëŸ¬ë‹ + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì§€ì›"""

    def __init__(self, use_deep_learning=False, use_optimization=False, ticker=None):
        """
        Args:
            use_deep_learning: LSTM, Transformer ì‚¬ìš© ì—¬ë¶€
            use_optimization: Bayesian Optimization ì‚¬ìš© ì—¬ë¶€
            ticker: ì£¼ì‹ í‹°ì»¤ (ëª¨ë¸ ì €ì¥/ë¡œë“œìš©)
        """
        self.ticker = ticker
        self.ensemble = EnsemblePredictor(
            use_deep_learning=use_deep_learning,
            use_optimization=use_optimization,
            ticker=ticker
        )
        self.use_deep_learning = use_deep_learning
        self.use_optimization = use_optimization
        self.progress_callback = None  # âœ… ì§„í–‰ ìƒíƒœ ì½œë°±

    def set_progress_callback(self, callback):
        """ì§„í–‰ ìƒíƒœ ì½œë°± ì„¤ì •"""
        self.progress_callback = callback
        if self.ensemble:
            self.ensemble.progress_callback = callback
    
    def get_stock_data(self, symbol, period=None):
        """
        ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° - ë™ì  ê¸°ê°„ ì„¤ì •

        Args:
            symbol: ì£¼ì‹ í‹°ì»¤
            period: ê¸°ê°„ (Noneì´ë©´ ìë™ ê²°ì •)
        """
        try:
            # ê¸°ê°„ ìë™ ê²°ì •
            if period is None:
                try:
                    from optimal_period_config import get_optimal_training_period
                    period = get_optimal_training_period(symbol)
                    logger.info(f"ğŸ“… {symbol} ìµœì  í›ˆë ¨ ê¸°ê°„: {period}")
                except ImportError:
                    period = "3y"  # ê¸°ë³¸ê°’: 3ë…„ (2y â†’ 3y ê°œì„ )
                    logger.debug(f"ê¸°ë³¸ í›ˆë ¨ ê¸°ê°„ ì‚¬ìš©: {period}")

            data = get_stock_data(symbol, period=period)
            return data
        except Exception as e:
            logger.error(f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def predict_stock_price(self, symbol, forecast_days=5, show_plot=True):
        """ì¢…ëª©ì˜ ë¯¸ë˜ ì£¼ê°€ ì˜ˆì¸¡ - ì‹œì¥ ë¶„ì„ ì¶”ê°€"""
        logger.info(f"{symbol} ì£¼ê°€ ì˜ˆì¸¡ ì‹œì‘...")

        # 1. ë°ì´í„° ìˆ˜ì§‘
        if self.progress_callback:
            self.progress_callback('data', 'ë°ì´í„° ìˆ˜ì§‘ ì¤‘...')

        data = self.get_stock_data(symbol)
        if data is None or len(data) < 50:
            return {"error": "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

        prices = data['Close'].values
        dates = data.index

        # === ì¦ë¶„ í•™ìŠµ: ìµœì‹  ë°ì´í„°ë¡œ XGBoost/LightGBM ë¯¸ì„¸ ì—…ë°ì´íŠ¸ ===
        try:
            mlp = self.ensemble.ml_predictor if hasattr(self, 'ensemble') else None
            if mlp and mlp.persistence and self.ticker:
                # ìµœì‹  ì‹œí€€ìŠ¤ ì¼ë¶€ë§Œ ì‚¬ìš©í•´ ë¹ ë¥¸ ì¦ë¶„ ì—…ë°ì´íŠ¸
                X_all, y_all = mlp.prepare_data(prices)
                tail = min(200, len(y_all))  # ìµœê·¼ 200ê°œ ìƒ˜í”Œ ì‚¬ìš© (ë°ì´í„° ì ìœ¼ë©´ ê°€ëŠ¥í•œ ë²”ìœ„)
                if tail > 0:
                    X_new, y_new = X_all[-tail:], y_all[-tail:]

                    if mlp.persistence.supports_incremental_learning('xgboost'):
                        mlp.persistence.incremental_train_xgboost(self.ticker, X_new, y_new, n_estimators_add=50)

                    if mlp.persistence.supports_incremental_learning('lightgbm'):
                        mlp.persistence.incremental_train_lightgbm(self.ticker, X_new, y_new, n_estimators_add=50)
        except Exception as e:
            logger.warning(f"ì¦ë¶„ í•™ìŠµ ê±´ë„ˆëœ€: {e}")

        logger.info(f"ë¶„ì„ ê¸°ê°„: {dates[0].strftime('%Y-%m-%d')} ~ {dates[-1].strftime('%Y-%m-%d')}")
        logger.info(f"ë°ì´í„° í¬ì¸íŠ¸: {len(prices)}ê°œ")

        # 1-1. ì‹œì¥ ì§€ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„
        if self.progress_callback:
            self.progress_callback('market_analysis', 'ì‹œì¥ ë¶„ì„ ì¤‘...')

        logger.info("ì‹œì¥ ì§€ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...")
        market_correlations = MarketCorrelationAnalyzer.calculate_correlation(symbol)

        # 1-2. ì„¹í„° ì„±ê³¼ ë¶„ì„ (ë¯¸êµ­ ì¢…ëª©ë§Œ)
        sector_info = None
        if not (symbol.endswith('.KS') or symbol.endswith('.KQ')):
            logger.info("ì„¹í„° ì„±ê³¼ ë¶„ì„ ì¤‘...")
            sector_performance = SectorAnalyzer.get_sector_performance()
            sector_info = sector_performance

        # 1-3. ì™¸êµ­ì¸/ê¸°ê´€ ë§¤ë§¤ ë™í–¥ (í•œêµ­ ì¢…ëª©ë§Œ)
        institutional_flow = None
        if InstitutionalFlowAnalyzer.is_korean_stock(symbol):
            logger.info("ì™¸êµ­ì¸/ê¸°ê´€ ë§¤ë§¤ ë™í–¥ ë¶„ì„ ì¤‘...")
            institutional_flow = InstitutionalFlowAnalyzer.fetch_institutional_data(symbol)

        # 2. ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰
        if self.progress_callback:
            self.progress_callback('ensemble', 'ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...')

        result = self.ensemble.fit_predict(prices, forecast_days)

        # ì§„í–‰ ìƒíƒœ: ê²°ê³¼ ì •ë¦¬ ì¤‘
        if self.progress_callback:
            self.progress_callback('complete', 'ê²°ê³¼ ì •ë¦¬ ì¤‘...')

        # 3. ê²°ê³¼ ì •ë¦¬
        last_price = prices[-1]
        predicted_prices = result['ensemble_predictions']

        # ì˜ˆì¸¡ ì •í™•ë„ ì¶”ì •
        confidence = result['confidence_score']
        
        # ë¯¸ë˜ ë‚ ì§œ ìƒì„± (ì˜ì—…ì¼ ê¸°ì¤€)
        future_dates = pd.bdate_range(start=dates[-1] + pd.Timedelta(days=1), 
                                     periods=forecast_days)
        
        # ìˆ˜ìµë¥  ê³„ì‚°
        returns = [(pred / last_price - 1) * 100 for pred in predicted_prices]
        
        # 4. ê²°ê³¼ ë°˜í™˜ (ì‹œì¥ ë¶„ì„ ì •ë³´ ì¶”ê°€)
        prediction_result = {
            'symbol': symbol,
            'current_price': last_price,
            'predicted_prices': predicted_prices,
            'future_dates': future_dates,
            'expected_returns': returns,
            'confidence_score': confidence,
            'model_weights': result['model_weights'],
            'recommendation': self._generate_recommendation(returns, confidence),
            'models_used': self._get_models_used(),
            # ìƒˆë¡œìš´ ì‹œì¥ ë¶„ì„ ì •ë³´
            'market_correlations': market_correlations,
            'sector_performance': sector_info,
            'institutional_flow': institutional_flow,
            'market_regime': result.get('market_regime', 'unknown')
        }
        
        # 5. ê·¸ë˜í”„ í‘œì‹œ
        if show_plot:
            self._plot_predictions(dates, prices, future_dates, predicted_prices, symbol)
        
        return prediction_result
    
    def _get_models_used(self):
        """ì‚¬ìš©ëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        models = ['Kalman Filter', 'ARIMA']

        if SKLEARN_AVAILABLE:
            models.append('Random Forest')
        if XGBOOST_AVAILABLE:
            models.append('XGBoost')
        if LIGHTGBM_AVAILABLE:
            models.append('LightGBM')
        if STATSMODELS_AVAILABLE:
            models.append('ARIMA (Full)')

        if self.use_deep_learning:
            if TENSORFLOW_AVAILABLE:
                models.extend(['LSTM', 'Transformer'])

        if self.use_optimization:
            models.append('+ Bayesian Optimization')

        return models
    
    def _generate_recommendation(self, returns, confidence):
        """ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë°˜ íˆ¬ì ì¶”ì²œ"""
        avg_return = np.mean(returns)
        
        if confidence > 0.7:
            if avg_return > 5:
                return "ê°•ë ¥ ë§¤ìˆ˜ ì¶”ì²œ"
            elif avg_return > 2:
                return "ë§¤ìˆ˜ ì¶”ì²œ"
            elif avg_return < -5:
                return "ë§¤ë„ ì¶”ì²œ"
            elif avg_return < -2:
                return "ë§¤ë„ ê³ ë ¤"
            else:
                return "ë³´ìœ "
        else:
            return "ë¶ˆí™•ì‹¤ - ì‹ ì¤‘í•œ íŒë‹¨ í•„ìš”"
    
    def _plot_predictions(self, historical_dates, historical_prices, 
                         future_dates, predicted_prices, symbol):
        """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
        plt.figure(figsize=(12, 8))
        
        # ê³¼ê±° ë°ì´í„°
        plt.plot(historical_dates[-60:], historical_prices[-60:], 
                'b-', label='ì‹¤ì œ ê°€ê²©', linewidth=2)
        
        # ì˜ˆì¸¡ ë°ì´í„°
        plt.plot(future_dates, predicted_prices, 
                'r--', label='ì˜ˆì¸¡ ê°€ê²©', linewidth=2, marker='o')
        
        # ì—°ê²°ì„ 
        plt.plot([historical_dates[-1], future_dates[0]], 
                [historical_prices[-1], predicted_prices[0]], 
                'g:', linewidth=1)
        
        plt.title(f'{symbol} ì£¼ê°€ ì˜ˆì¸¡ ê²°ê³¼ (TensorFlow ì—†ìŒ)', fontsize=16)
        plt.xlabel('ë‚ ì§œ')
        plt.ylabel('ê°€ê²©')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

    def _backtest_single_point(self, args):
        """ë‹¨ì¼ ë°±í…ŒìŠ¤íŒ… í¬ì¸íŠ¸ ì‹¤í–‰ (ë³‘ë ¬/ìˆœì°¨ ì²˜ë¦¬ìš©)"""
        i, test_point, train_prices, actual_future_prices, forecast_days, test_date = args

        try:
            # ProcessPool ë³‘ë ¬ ì²˜ë¦¬: ê° í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ë…ë¦½ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            # (GIL ìš°íšŒë¡œ ì§„ì •í•œ ë³‘ë ¬ ì²˜ë¦¬, ëª¨ë“ˆì€ ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ 1íšŒ ë¡œë”©)
            ensemble = EnsemblePredictor(
                use_deep_learning=self.use_deep_learning,
                use_optimization=False  # ë°±í…ŒìŠ¤íŒ…ì—ì„œëŠ” ìµœì í™” ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
            )

            result = ensemble.fit_predict(train_prices, forecast_days)
            predicted_prices = result['ensemble_predictions']

            # ì‹¤ì œ vs ì˜ˆì¸¡ ë¹„êµ
            last_train_price = train_prices[-1]
            actual_return = (actual_future_prices[-1] - last_train_price) / last_train_price * 100
            predicted_return = (predicted_prices[-1] - last_train_price) / last_train_price * 100

            # MAE, RMSE ê³„ì‚°
            mae = np.mean(np.abs(predicted_prices - actual_future_prices))
            rmse = np.sqrt(np.mean((predicted_prices - actual_future_prices) ** 2))
            mape = np.mean(np.abs((actual_future_prices - predicted_prices) / actual_future_prices)) * 100

            # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘ (ëª¨ë¸ë³„ ì„±ëŠ¥ ë¶„ì„ìš©)
            individual_predictions = {}

            # ë””ë²„ê¹…: result í‚¤ í™•ì¸
            logger.debug(f"Result keys: {result.keys()}")

            if 'individual_results' in result:
                logger.debug(f"Individual results found: {result['individual_results'].keys()}")
                for model_name, model_result in result['individual_results'].items():
                    logger.debug(f"Processing model: {model_name}, type: {type(model_result)}")
                    if isinstance(model_result, dict) and 'future_predictions' in model_result:
                        model_pred_price = model_result['future_predictions'][-1]

                        model_pred_return = (model_pred_price - last_train_price) / last_train_price * 100
                        individual_predictions[model_name] = {
                            'predicted_return': model_pred_return,
                            'direction_match': (actual_return > 0) == (model_pred_return > 0)
                        }
                        logger.debug(f"{model_name} ì˜ˆì¸¡ ì¶”ê°€: {model_pred_return:.2f}%")
                    else:
                        logger.debug(f"{model_name} ìŠ¤í‚µ: future_predictions ì—†ìŒ")
            else:
                logger.warning("individual_results í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤!")

            return {
                'success': True,
                'index': i,
                'date': test_date,
                'actual_return': actual_return,
                'predicted_return': predicted_return,
                'mae': mae,
                'rmse': rmse,
                'mape': mape,
                'direction_match': (actual_return > 0) == (predicted_return > 0),
                'individual_predictions': individual_predictions  # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡
            }

        except Exception as e:
            logger.warning(f"ë°±í…ŒìŠ¤íŒ… {i+1} ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'index': i,
                'error': str(e)
            }

    def backtest_predictions(self, ticker, test_periods=30, forecast_days=7,
                           progress_callback=None, use_parallel=False, cancel_callback=None):
        """
        ë°±í…ŒìŠ¤íŒ…: ê³¼ê±° ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì„±ëŠ¥ ê²€ì¦

        Args:
            ticker: ì¢…ëª© ì½”ë“œ
            test_periods: í…ŒìŠ¤íŠ¸í•  ê¸°ê°„ ìˆ˜
            forecast_days: ì˜ˆì¸¡ ì¼ìˆ˜
            progress_callback: ì§„í–‰ ìƒíƒœ ì½œë°± (current, total, message)
            use_parallel: ë³‘ë ¬ ì²˜ë¦¬ ì—¬ë¶€
            cancel_callback: ì¤‘ì§€ í™•ì¸ ì½œë°±

        Returns:
            (summary, error): ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ ìš”ì•½ê³¼ ì—ëŸ¬ ë©”ì‹œì§€
        """
        try:
            # ì´ˆê¸° ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            if progress_callback:
                progress_callback(0, test_periods, f"ë°ì´í„° ë¡œë”© ì¤‘... ({ticker})")

            # ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì¶©ë¶„í•œ ê¸°ê°„)
            data = self.get_stock_data(ticker, period="2y")
            if data is None or len(data) < 100:
                return None, "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"

            prices = data['Close'].values
            dates = data.index

            # ë°±í…ŒìŠ¤íŒ… ì‘ì—… ì¤€ë¹„
            tasks = []
            for i in range(test_periods):
                test_point = len(prices) - (test_periods - i) * forecast_days - forecast_days

                if test_point < 100:
                    continue

                train_prices = prices[:test_point]
                actual_future_prices = prices[test_point:test_point + forecast_days]

                if len(actual_future_prices) < forecast_days:
                    continue

                tasks.append((
                    i,
                    test_point,
                    train_prices,
                    actual_future_prices,
                    forecast_days,
                    dates[test_point]
                ))

            if len(tasks) == 0:
                return None, "ë°±í…ŒìŠ¤íŒ…í•  ë°ì´í„° í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"

            # ì‘ì—… ì¤€ë¹„ ì™„ë£Œ ì•Œë¦¼
            if progress_callback:
                progress_callback(0, len(tasks), f"ë°±í…ŒìŠ¤íŒ… ì‹œì‘ ì¤‘... ({len(tasks)}ê°œ ì‘ì—…)")

            results = []
            errors = []
            actual_returns = []
            predicted_returns = []

            # ë³‘ë ¬ ì²˜ë¦¬ vs ìˆœì°¨ ì²˜ë¦¬
            if use_parallel:
                import time
                start_time = time.time()

                # ProcessPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰ (GIL ìš°íšŒ, ì§„ì •í•œ ë³‘ë ¬ ì²˜ë¦¬)
                max_workers = min(multiprocessing.cpu_count(), len(tasks))
                logger.info(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ: {max_workers}ê°œ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš© (CPU ì½”ì–´: {multiprocessing.cpu_count()}, ì‘ì—… ìˆ˜: {len(tasks)})")

                with ProcessPoolExecutor(max_workers=max_workers) as executor:
                    futures = {executor.submit(self._backtest_single_point, task): task for task in tasks}

                    completed = 0
                    for future in as_completed(futures):
                        # ì¤‘ì§€ í™•ì¸
                        if cancel_callback and cancel_callback():
                            executor.shutdown(wait=False, cancel_futures=True)
                            return None, "ì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë¨"

                        try:
                            result = future.result()
                        except Exception as e:
                            completed += 1
                            logger.error(f"ë°±í…ŒìŠ¤íŒ… ì‘ì—… ì‹¤íŒ¨: {e}")
                            errors.append(str(e))
                            if progress_callback:
                                progress_callback(completed, len(tasks), f"ì˜¤ë¥˜ ë°œìƒ {completed}/{len(tasks)}")
                            continue

                        completed += 1

                        # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                        if progress_callback:
                            task = futures[future]
                            progress_callback(completed, len(tasks),
                                            f"ë°±í…ŒìŠ¤íŒ… {completed}/{len(tasks)}: {task[5]:%Y-%m-%d}")

                        if result.get('success', False):
                            results.append({
                                'date': result['date'],
                                'actual_return': result['actual_return'],
                                'predicted_return': result['predicted_return'],
                                'mae': result['mae'],
                                'rmse': result['rmse'],
                                'mape': result['mape'],
                                'direction_match': result['direction_match'],
                                'individual_predictions': result.get('individual_predictions', {})
                            })
                            actual_returns.append(result['actual_return'])
                            predicted_returns.append(result['predicted_return'])
                        else:
                            errors.append(result.get('error', 'Unknown error'))

                elapsed_time = time.time() - start_time
                logger.info(f"â±ï¸ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ (í‰ê·  {elapsed_time/len(tasks):.2f}ì´ˆ/ì‘ì—…)")

            else:
                # ìˆœì°¨ ì²˜ë¦¬
                import time
                start_time = time.time()
                logger.info("ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")

                for i, task in enumerate(tasks):
                    # ì¤‘ì§€ í™•ì¸
                    if cancel_callback and cancel_callback():
                        return None, "ì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë¨"

                    # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                    if progress_callback:
                        progress_callback(i + 1, len(tasks),
                                        f"ë°±í…ŒìŠ¤íŒ… {i+1}/{len(tasks)}: {task[5]:%Y-%m-%d}")

                    result = self._backtest_single_point(task)

                    if result['success']:
                        results.append({
                            'date': result['date'],
                            'actual_return': result['actual_return'],
                            'predicted_return': result['predicted_return'],
                            'mae': result['mae'],
                            'rmse': result['rmse'],
                            'mape': result['mape'],
                            'direction_match': result['direction_match'],
                            'individual_predictions': result.get('individual_predictions', {})
                        })
                        actual_returns.append(result['actual_return'])
                        predicted_returns.append(result['predicted_return'])
                    else:
                        errors.append(result['error'])

                elapsed_time = time.time() - start_time
                logger.info(f"â±ï¸ ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ (í‰ê·  {elapsed_time/len(tasks):.2f}ì´ˆ/ì‘ì—…)")

            # ìš”ì•½ í†µê³„ ê³„ì‚°
            if len(results) == 0:
                return None, "ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"

            direction_accuracy = np.mean([r['direction_match'] for r in results]) * 100
            avg_mae = np.mean([r['mae'] for r in results])
            avg_rmse = np.mean([r['rmse'] for r in results])
            avg_mape = np.mean([r['mape'] for r in results])

            # ìƒê´€ê´€ê³„
            if len(actual_returns) > 1:
                correlation = np.corrcoef(actual_returns, predicted_returns)[0, 1]
            else:
                correlation = 0

            # ìƒì„¸ ë¶„ì„ ì¶”ê°€
            bull_correct = sum(1 for r in results if r['actual_return'] > 0 and r['direction_match'])
            bull_total = sum(1 for r in results if r['actual_return'] > 0)
            bear_correct = sum(1 for r in results if r['actual_return'] <= 0 and r['direction_match'])
            bear_total = sum(1 for r in results if r['actual_return'] <= 0)

            pred_bull = sum(1 for r in results if r['predicted_return'] > 0)
            pred_bear = len(results) - pred_bull

            # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
            logger.debug(f"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ì‹œì‘ - ì´ {len(results)}ê°œ ê²°ê³¼")
            model_performance = {}
            for idx, result in enumerate(results):
                logger.debug(f"ê²°ê³¼ {idx}: 'individual_predictions' ì¡´ì¬ = {'individual_predictions' in result}")
                if 'individual_predictions' in result:
                    logger.debug(f"ê²°ê³¼ {idx} individual_predictions: {result['individual_predictions'].keys()}")
                    for model_name, model_pred in result['individual_predictions'].items():
                        if model_name not in model_performance:
                            model_performance[model_name] = {'correct': 0, 'total': 0}

                        model_performance[model_name]['total'] += 1
                        if model_pred['direction_match']:
                            model_performance[model_name]['correct'] += 1
                else:
                    logger.warning(f"ê²°ê³¼ {idx}ì— individual_predictions ì—†ìŒ: {result.keys()}")

            logger.debug(f"ìµœì¢… model_performance: {model_performance}")

            # ëª¨ë¸ë³„ ì ì¤‘ë¥  ê³„ì‚°
            model_accuracies = {}
            for model_name, perf in model_performance.items():
                if perf['total'] > 0:
                    accuracy = (perf['correct'] / perf['total']) * 100
                    model_accuracies[model_name] = accuracy
                    logger.info(f"ğŸ“Š {model_name} ì ì¤‘ë¥ : {accuracy:.1f}% ({perf['correct']}/{perf['total']})")

            logger.debug(f"ìµœì¢… model_accuracies: {model_accuracies}")

            summary = {
                'ticker': ticker,
                'test_periods': len(results),
                'forecast_days': forecast_days,
                'direction_accuracy': direction_accuracy,
                'avg_mae': avg_mae,
                'avg_rmse': avg_rmse,
                'avg_mape': avg_mape,
                'correlation': correlation,
                'actual_returns': actual_returns,
                'predicted_returns': predicted_returns,
                'results': results,
                'errors': errors,
                # ìƒì„¸ ë¶„ì„
                'bull_accuracy': (bull_correct / bull_total * 100) if bull_total > 0 else 0,
                'bear_accuracy': (bear_correct / bear_total * 100) if bear_total > 0 else 0,
                'bull_total': bull_total,
                'bear_total': bear_total,
                'pred_bull': pred_bull,
                'pred_bear': pred_bear,
                # ëª¨ë¸ë³„ ì„±ëŠ¥
                'model_accuracies': model_accuracies
            }

            return summary, None

        except Exception as e:
            logger.error(f"ë°±í…ŒìŠ¤íŒ… ì˜¤ë¥˜: {e}")
            return None, str(e)

# ì‚¬ìš© ì˜ˆì œ
def example_usage():
    """ì‚¬ìš© ì˜ˆì œ"""
    # ì˜µì…˜ ì„¤ì •:
    # use_deep_learning=True: LSTM, Transformer ì‚¬ìš© (TensorFlow í•„ìš”)
    # use_optimization=True: Bayesian Optimization ì‚¬ìš© (scikit-optimize í•„ìš”)
    predictor = StockPredictor(
        use_deep_learning=True,      # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‚¬ìš©
        use_optimization=False        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    )

    logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:")
    logger.info(f"- scikit-learn: {'ì‚¬ìš© ê°€ëŠ¥' if SKLEARN_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"- XGBoost: {'ì‚¬ìš© ê°€ëŠ¥' if XGBOOST_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"- LightGBM: {'ì‚¬ìš© ê°€ëŠ¥' if LIGHTGBM_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"- statsmodels: {'ì‚¬ìš© ê°€ëŠ¥' if STATSMODELS_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"- TensorFlow: {'ì‚¬ìš© ê°€ëŠ¥' if TENSORFLOW_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"- Bayesian Opt: {'ì‚¬ìš© ê°€ëŠ¥' if HYPEROPT_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")

    # ì˜ˆì œ 1: ì• í”Œ ì£¼ì‹ ì˜ˆì¸¡
    logger.info("=" * 50)
    logger.info("APPLE (AAPL) ì£¼ê°€ ì˜ˆì¸¡")
    logger.info("=" * 50)

    result = predictor.predict_stock_price('AAPL', forecast_days=7, show_plot=False)

    if 'error' not in result:
        logger.info(f"í˜„ì¬ê°€: ${result['current_price']:.2f}")
        logger.info(f"ì˜ˆì¸¡ ê°€ê²©: {[f'${p:.2f}' for p in result['predicted_prices']]}")
        logger.info(f"ì˜ˆìƒ ìˆ˜ìµë¥ : {[f'{r:.1f}%' for r in result['expected_returns']]}")
        logger.info(f"ì‹ ë¢°ë„: {result['confidence_score']:.1%}")
        logger.info(f"ì¶”ì²œ: {result['recommendation']}")
        logger.info(f"ì‚¬ìš© ëª¨ë¸: {', '.join(result['models_used'])}")

if __name__ == "__main__":
    example_usage()